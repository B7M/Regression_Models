```{r setup, include = FALSE}
ottrpal::set_knitr_image_path()
knitr::opts_chunk$set(echo = FALSE)
```

# Week 03

## Multi-variable regression

We now extend linear regression so that our models can contain more variables. A natural first approach is to assume additive effects, basically extending our line to a plane, or generalized version of a plane as we add more variables. Multi-variable regression represents one of the most widely used and successful methods in statistics.  

If you're utilizing predictor X to forecast a response Y and discover a meaningful relationship, there's a potential issue if the predictor hasn't been randomly assigned to the subjects or units being observed. In such cases, there's always a concern that there might be another variable, whether known or unknown, that could account for the observed relationship. For example, imagine if you had a friend who downloaded some data, where they had all sorts of health information from people
and also their dietary information. This person claims to have found an interesting relationship: breath mint usage has a significant regression relationship with forced expiratory volume(FEV), a measure of lung function. You would be skeptical there's very little basis for a biological relationship there. Breath mints are just sugar! But maybe, but what you've really be thinking is what other variables might explain this relationship? You might have two hypotheses: this person dug through lots and lots of variables and just found the one that was significant, and it's just a chance of association, which is the problem of multiplicity. In addition it is likely, you would think the real problem is smokers tend to use more breath mints, and smoking has this relationship with lung function. It's well-established that chronic exposure to a smoker, even second-hand smoke has negative impacts on lung function. So it's probably smoking it probably has nothing to do with the breath mints, it's a indirect effect of breath mints through smoking, not a direct effect of breath mints on lung function. This would be the hypothesis. To establish that there's a breath mint effect beyond smoking we could consider smokers by themselves, and see whether their lung function differs by their breath mint usage, and consider non-smokers by themselves, and see whether their lung function differs by breath mint usage, where we conditioned on smoking status. This way we would compare like with like. Multivariable regression is sort of automated way to do that in a linear fashion. It makes fair enough assumptions, in automated way. In this section we will explain how it works and we will also talk a little bit about its limitations. 

Multivariable regression is trying to look at the relationship of a predictor and a response, while having, at some level, accounted for other variables. Moreover, multivariable regression is actually a good prediction model.
For example, a Kaggle competition wanted to predict the number of days a person would be in the hospital in subsequent years given their claims history and number of days they were in the hospital in previous years. The insurance companies seek to harness an extensive dataset derived from claims, aiming to predict a singular numerical outcome. However, the conventional approach of simple linear regression would be insufficient when confronted with multiple predictors. How can we extend the scope of simple linear regression to accommodate a multitude of regressors for predictive purposes? The procedure is similar to simple linear regression where there's more predictor terms,X values. For example, $X_1$ might be the number of insurance claims in the previous year, and $X_2$ might be whether or not the person had a particular cardiac problem, and so on. The first variable is typically just a constant one, so there's an intercept that's included, a term that's just $\beta_0$ by itself. Interestrigly in this competition, we found that multivariable regression could get people very close to the winning entry, while other machine learning methods like random forest, and boosting only improved the results minorly on top of multivariable regression.

Note: in case of breath mint study, one of the predictors, $X_1$ might be breath mint usage (a binary variable), and $X_2$ might be how much a person smoked.

* The general linear model extends simple linear regression (SLR) by adding terms linearly into the model.
$$
Y_i =  \beta_0 X_{0i} + \beta_1 X_{1i} + \ldots +
\beta_{p} X_{pi} + \epsilon_{i} 
= \sum_{k=0}^p X_{ik} \beta_j + \epsilon_{i}
$$
* Where $X_{1i}=1$ typically, the $\beta_j$ are the coefficients of the model.


* Least squares (and hence ML estimates under iid Gaussianity of the errors) minimizes
$$
\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2
$$
Note, the important linearity is linearity in the coefficients. Thus
$$
Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +
\beta_{p} X_{pi}^2 + \epsilon_{i} 
$$
is still a linear model. (We've just squared the elements of the predictor variables.)

### How to get the coefficients, derivation of formulas

Here we will go through the derivation of formulas to show how the least squares estimates are obtained. This derivation is not required for the course, but it may be helpful for those who are interested in understanding how the estimates are obtained.

Just to review, if you have regression to the origin, you want a line that's forced to the origin that has no intercepts. You have the single predictor $X$ and a single predictor of $Y$ and you want no intercept, $E[Y_i]=X_{1i}\beta_1$. The slope estimate was $\sum X_i Y_i / \sum X_i^2$. Now lets try to derive the least squares estimate when we have two regressors, which can be generalized to models with more variables. In $E[Y_i] = X_{1i}\beta_1 + X_{2i}\beta_2 = \mu_i$, Least squares tries to minimize:
$$
\sum_{i=1}^n (Y_i - X_{1i} \beta_1 - X_{2i} \beta_2)^2
$$
Here we try to give a development that is more intuitive than what you would get with something like linear algebra. 
$$\Sum(y_i - X_{0i} \beta_0 - X_{1i} \beta_1$$
Imagine we knew $\beta_1$ or fix $\beta_1$, then we can write $\tilde y_i = y_i - x_{0i} \beta_0$ and subsequently $\Sum(\tilde y_i - X_{1i} \beta_1$. This is exactly regression through the origin with just the single regressor. So we can write $\beta_1 = \sum \tilde y_i X_{1i} / \sum X_{1i}^2$. Now we can plug this back into the original equation and we get:
$$
\sum_{i=1}^n (Y_i - X_{1i} \beta_1 - X_{2i} \sum \tilde y_i X_{1i} / \sum X_{1i}^2)^2
$$
This is an equation that only involves $\beta_0$ and a regression through the origin for $\beta_0$. What it works out to be, and this is the interesting part, is that the regression slope for $\beta_0$, is exactly what you would obtain if you took the residual of $X_1$ out of $X_0$, and $X_1$ out of $Y$ and
then just did regression to the origin. 

Multivariable regression calculates the coefficient for $X_0$, $\beta_0$, as if you had removed the effect of $X_1$ from both $Y$ and $X_0$. Similarly, the regression coefficient for $X_1$, $\beta_1$, is what you would get if you were to remove the effect of $X_0$ from both $Y$ and $X_1$. This is why multivariable regression is thought of as having adjusted for the other variables.
A coefficient from a multivariable regression is the coefficient where the linear effect of all the other variables on that predictor and response has been removed.

### Results

In $E[Y_i] = X_{0i}\beta_0 + X_{1i}\beta_1$, we have two covariants, $X_1 , X_2$. 
$$\hat \beta_0 = \frac{\sum_{i=1}^n e_{i, Y | X_1} e_{i, X_0 | X_1}}{\sum_{i=1}^n e_{i, X_0 | X_1}^2}$$

$\beta_0$ is what you would get with regression through the origin if you removed the second coefficient $X_1$. Similarly, the same thing could be said about the coefficient for $X_1 \beta_1$. $\hat \beta_1$ is the linear regression where linear effect of $X_0$ out of both the response $Y$, and the second predictor, $X_1$. This is why multivariable regression relationships are considered as having been adjusted for all the other variables. 

### Example with two variables, simple linear regression

$Y_{i} = \beta_0 X_{0i} + \beta_1 X_{1i}$ where  $X_{0i} = 1$ is an intercept term. Notice the fitted coefficient of $X_{1i}$ on $Y_{i}$ is $\bar Y$. The residuals are $e_{i, Y | X_1} = Y_i - \bar Y$. Thus the fitted coefficient of $X_{1i}$ on $X_{0i}$ is $\bar X_1$, which is the residuals $e_{i, X_0 | X_1}= X_{0i} - \bar X_0$. We can write:
$$
\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_0} e_{i, X_1 | X_0}}{\sum_{i=1}^n e_{i, X_1 | X_0}^2} = \frac{\sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2}
= Cor(X, Y) \frac{Sd(Y)}{Sd(X)}
$$

### The general case

More generally, multivariate regression estimates are exactly those having removed the linear relationship of the other variables from both the regressor and response. Least squares solutions have to minimize$$\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2$$. The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals. In this sense, multivariate regression "adjusts" a coefficient for the linear impact of the other variables. 

### Examples with multiple-variables

In the following simulation we have 100 observations and want to generate three predictors, `x, x2, x3`, where they are all just standard normals. When we write `y = 1 + x + x2 + x3`, all my coefficients are 1, meaning the population model used for simulation, they're all 1. Next we add some random noise, that's the error term. 

```{r, echo = T, eval = F}

n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
sum(ey * ex) / sum(ex ^ 2)
coef(lm(ey ~ ex - 1))
coef(lm(y ~ x + x2 + x3)) 

```

Here we want to point out, `coef(lm(ey ~ ex - 1))` is the same coefficient as if we regress y on x, x2 and x3, and an intercept`coef(lm(y ~ x + x2 + x3))`. You see the x term here is exactly the same as the regression through the origin estimate with the residuals. 

### Interpretation of coefficients

The regression predictor, given the collection of covariants take a specific value, $x_1$ to $x_p$, is just the sum of the $x_k\beta_k$. $$E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k$$

If one of the predictors, say $X_1$, is incremented by 1 i.e. $X_1$ instead of $x_1$ takes $x_1+1$, then the regression coefficient $\beta_1$ is the expected change in the response.
$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k
$$

If we subtract the two terms the expected value of the response from the responce where the first co-efficient takes the value of $x_1 +1$ works out to be $\beta_1$. 
$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p]$$
$$= (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k + \sum_{k=1}^p x_{k} \beta_k = \beta_1 $$

Notice all the other $x_2$ to $x_p$ were held fixed, the interpretation of a multivariate regression coefficient is the expected change in the response per unit change in the regressor, holding all of the other regressors fixed. 



The basic components of the linear models are exactly the same as in simple linear regression.

* Model $Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}$ where $\epsilon_i \sim N(0, \sigma^2)$
* Fitted responses $\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}$
* Residuals $e_i = Y_i - \hat Y_i$
* Variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2$ (note the $n-p$ degrees of freedom)
* To get predicted responses at new values, $x_1, \ldots, x_p$, simply plug them into the linear model $\sum_{k=1}^p x_{k} \hat \beta_{k}$
* Coefficients have standard errors, $\hat \sigma_{\hat \beta_k}$, and
$\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$
follows a $T$ distribution with $n-p$ degrees of freedom.
* Predicted responses have standard errors and we can calculate predicted and expected response intervals.

These should all be pretty familiar because they're basically the same as what we did for linear aggression, the difference is we have more terms now. Remember in linear aggression we had two terms, we had an intercept and a covariant now we're just adding more covariants potentially.

One point to note is that the variance estimate is not quite the same as the average squared residuals. In linear regression we divided by $n-2$, now we divide by $n-p$. That's kind of a technical point because if you know $n-p$ of the residuals you implicity know the last $p$ of them due to some linear constraints. That's a minor point you can think of the residuals variants estimate is nothing other than the average square residuals for the most part with $N-p$ part not withstanding.


In a sense all the things we knew about from linear regression carryover to multi-variable regression. 


To end this section, we want to emphasize how important linear models are to the data scientist. Before you do any machine learning or any complex algorithm, linear models should be your first attempt. They offer parsimonious and well understood easily describe relationships between predictors and response. There are some modern
machine learning algorithms that can beat some of the propetries of linear models, like the imposed linearity. Nonetheless, linear models should always be your starting point. There's some amazing things you can do with linear models that you may not think that would be possible. For example, you can take a time series like a music sound or something like that, and decompose it into its harmonics. This is so-called discrete Fourier transform can be thought of the as the fit from a linear model. You can flexibly fit rather complicated functions and curves and things like that using linear models. You can fit factor variables as predictors. ANOVA and ANCOVA are special cases of linear models. You can uncover complex multivariate relationships within a response and you can build fairly accurate prediction models.

## Multi-variable regression tips and tricks

Let's start this discussion with the famous Swiss Fertility Data. Using the following command you can load the data and see the documentation.

```{r, echo = T, eval = F}
require(datasets); 
data(swiss); 
?swiss
```

The data shows standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. A data frame with 47 observations on 6 variables, each of which is in percent, i.e., in [0, 100]. The varibles are:

* [,1]   Fertility          a common standardized fertility measure
* [,2]   Agriculture        % of males involved in agriculture as occupation
* [,3]	 Examination        % draftees receiving highest mark on army examination
* [,4]	 Education          % education beyond primary school for draftees
* [,5]	 Catholic           % catholic (as opposed to protestant)
* [,6]	 Infant.Mortality   live births who live less than 1 year

All variables but Fertility give proportions of the population.

Visualizing some of the basic scatter plots is always a good practice. 

```{r, echo = T, eval = T}
g <- ggpairs(
  swiss,
  lower = list(continuous = "smooth"),
  wrap = function(...) {
    ggally_smooth(..., method = "loess")
  }
)
g
```

In this plot you see fertility is on the x-axis for all the plots in the first column, agriculture is on the x-axis for all of the plots in the second column. Agriculture is also on the y-axis for the first graph. In addition, the corresponding upper triangular part of the matrix gives the correlation between the two variables. For example, fertility and agriculture the relation turns out to be fairly linear with confidence prediction band around it. The correlation between the two is 0.35. 

Let's investigate the relationship where agriculture, the percent of the province that works in the agricultural industry, with fertility. 

```{r, echo = T, eval = T}
summary(lm(Fertility ~ . , data = swiss))$coefficients
```

Tilde period in `lm` function is a shorthand for all the other variables in the data frame. The output of the `summary` function gives the coefficients of the model. The first column gives the estimated coefficients, the second column gives the standard errors of the coefficients, the third column gives the t-statistics, and the fourth column gives the p-values. The p-values are the probability of observing a t-statistic as extreme as the one observed, if the true coefficient were 0.

The number $-0.17$ in the `Agriculture` variable row is interpreted as: we expect a 0.17 decrease, in standardized fertility for every 1% increase in the percentage
of males involved in agriculture, holding the other variables constant. Meaning we hold examination and education, percent Catholic and infant mortality constant. 

The next column, the standard error 0.07, talks about how precise that coefficient is. It talks about the statistical variability of that coefficient. If we wanted to perform a hypothesis test, we would take the estimate, subtract off the hypothesized value, which in this case is zero, and divide it by the standard error of the estimate. Which is the definition of T-statistic. R conveniently provides it to us, -2.448. 


We can calculate the probability of getting a t-statistic as extreme as that. As small as negative 2.448 or smaller, and because we're doing a two-sided test,
we would double that p-value. The degrees of freedom are $n - #coefficients$, including the intercept. But again, R does that on our behalf, and that works out to be 0.018. By standard thresholding rules, type one error rate of say 5%, that would be statistically significant. 

In the following section we will see how the process of model selection changes the estimates. We start by contrasting the model with a model that just has agriculture as predictor, the previous model had all the other variables in this predictor.

```{r, echo = T, eval = T}
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
```

The agriculture variable is about the same magnitude, 0.19 instead of 0.17 but with changed signs. Instead of agriculture having a negative effect on fertility, it has a positive effect on fertility. Adjusting for the other variables changes the actual direction of the effect of agriculture on fertility. This is the impact of something so-called Simpson's Paradox. Notice in both cases the agriculture coefficient is strongly statistically significant. 
We would like to create (via simulation) an example where an effect can reverse itself. It can help us understand Simpson's paradox could happen. 
Keep in mind, regression is a dynamic process, where you have to think about what variables to include and why. If there hasn't been randomization to
protect you from confounding, you have to go through a scientific dynamic process of putting confounders in and out and thinking about what they're
doing to your effective interest in order to evaluate it. 

```{r, echo = T, eval =T}
n <- 100; x2 <- 1 : n; x1 <- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
plot(x1)
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
```

The second regressor, $x_2$, is the values $1-n$, $x_1$ is a variable that depends on $x_2$ and random noise. Think of $x_2$ as something we might measure regularly, like days, and $x_1$ as something like a saving account where the balance goes up with time and random fluctuations. The random fluctuations impact the spending, so the money doesn't necessarily always just go up. It goes up and down sporadically, but the linear trend is going up. Let's assume y is happiness with a measure like `y = -x1 + x2 + noise`. The true generating model `y` is negatively associated with `-x1` suggesting happiness is negatively associated with money and positively associated with `x2`, so it goes up with time and down with `x1` with some random normal noise. We know from the model `y = -x1 + x2 + noise` the outcome depends negatively on `x1` with a coefficient of minus 1, and depends positively on `x2` with a coefficient of plus 1. If fit `x1` by itself we get an enormous coefficient, 95, which is clearly wrong. It's nothing near to the negative 1 that it's supposed to be or that we would hope it would be. It is picking up the residual effect of `x2` that's a big driver of y, but when we fit the correct model, `x1` and `x2`, together we will get the correct coefficients, about minus 1 for `x1`, and about plus 1 for `x2`. You can imagine why this would happen by answering: what is regression doing? It's taking `x1` and removing the linear effect of `x2`.

Let's do some plots to highlight this, just to show us how it works a little bit.

```{r, echo = T, eval = T}
dat = data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))
library(ggplot2)
g = ggplot(dat, aes(y = y, x = x1, colour = x2))
g = g + geom_point(colour="grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") 
g = g + geom_point(size = 4) 
g
```

There is a clear positive linear relationship, between the `x1` and `y`. However, with x2, which is the color, there's also clear positive gradient. As y goes up, so does x2. And also you can see as x1 goes up, so does x2. So you can see the confounding that's happening here.

```{r, echo = T, eval = T}
g2 = ggplot(dat, aes(y = ey, x = ex1, colour = x2))  
g2 = g2 + geom_point(colour="grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") + geom_point(size = 4) 
g2
```

If we plot the residuals you can see that for the residual y and the residual `x1`, there's a clear negative linear relationship, and if you stare at it enough, you realize that the slope of this line should be around negative 1. You can also see that the `x2` variable is clearly not related to the residual `x1` variable. 

It is important to remember the above explanation doesn't mean that throwing every variable into your regression model is the right thing to do. There's consequences to throwing in unnecessary variables. It can make your model less interpretable, it can make your model less stable, and it can make your model less generalizable. It's important to think about what variables you're putting in and why.

In the earlier example about Swiss data the agriculture effect reversed itself after we included the other variables in the model. You will find that this happens quite a bit when education and examination are included. Educational attainment is negatively correlated with the percent working in agriculture, a correlation of -0.64. In addition, education and examination are kind of measuring the same thing. Their correlation, those two variables is 0.7. The percent of males in the province working in agriculture is negatively related to educational attainment (correlation of `r cor(swiss$Agriculture, swiss$Education)`) and Education and Examination (correlation of `r cor(swiss$Education, swiss$Examination)`) are obviously measuring similar things. The question is: is the positive marginal an artifact for not having accounted for, say, Education level? (Education does have a stronger effect, by the way.)
At the minimum, anyone claiming that provinces that are more agricultural have higher fertility rates would immediately be open to criticism.

*Notice*
What if we include an unnecessary variable?
Here we introduce `z` which adds no new linear information, since it's a linear combination of variables already included. R just drops terms that are linear combinations of other terms.

```{r, echo = TRUE}
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
```

### Dummy variables are smart
You might be surprised to find out how flexible linear regression models are. For example, you can fit factor variables as regressors and come up with things like
analysis of variance as a special case of linear models. 

Consider the linear model $Y_i = \beta_0 + X_{i1} \beta_1 + \epsilon_{i}$ where each $X_{i1}$ is binary so that it is a 1 if measurement $i$ is in a group and 0 otherwise. (Treated versus not in a clinical trial, for example.) The estimated mean for the treated group is the mean of the people who are treated. Then for people in the treated group we can write $E[Y_i] = \beta_0 + \beta_1$. $\beta_1$ is interpreted as the increase, or decrease if it's negative, in the mean response for those that were treated.
Similarly for people without treatment we have $E[Y_i] = \beta_0$.

The LS fits work out to be $\hat \beta_0 + \hat \beta_1$ is the mean for those in the group and $\hat \beta_0$ is the mean for those not in the group.
You see that linear regression provides the fitted values and tell you about the means for both of the groups, in addition it gives you an inference for comparing the two groups automatically.


**Note** including a binary variable that is 1 for those not in the group would be redundant. It would create three parameters to describe two means.


We can generalize this to more than two groups. If we have a three-level variable, we can create two binary variables, one for each level, and then we can compare the means of the three groups. For example, imagine you have some outcome but you want to compare it to U.S. political party affiliation. In this case, let's say you were only considering those who were Democrats, Republicans, or registered Independents. Well, you can do that by having a variable X1, that's one for Republicans and zero for otherwise, a variable X2 that's one for Democrats and zero for otherwise, we omit the X3 for Independents because of redundancy. If we know that you're not Republican and not a Democrat, then you must be an Independent in our data set the way we've set things up and having a third variable wouldn't have any new information. 
$$Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \epsilon_i$$ 
The mean for the three groups are:
* If $i$ is Republican $E[Y_i] = \beta_0 +\beta_1$
* If $i$ is Democrat $E[Y_i] = \beta_0 + \beta_2$.
* If $i$ is Independent $E[Y_i] = \beta_0$.

If we compare the means like $\beta_0$ the mean for the Independents versus $\beta_0 +\beta_1$ the mean for the Republicans, i.e. subtract those two, we get $\beta_1$. Which means $\beta_1$ compares Republicans to Independents, and similarly $\beta_2$ compares Democrats to Independents and $\beta_1 - \beta_2$ compares Republicans to Democrats. By omitting the regression variable for the Independents, the intercept became the value for the Independents, and all of the other coefficients have become interpreted relative to Independents and shows choice of reference category changes the interpretation. If we had included the regressor for
Independents and excluded the one for Republicans, then the intercept would be for Republicans, and the coefficient in front of the Democratic would be Democrats versus Republicans. The coefficient in front of the Independent would be Independent versus Republican. To illustrate how this works we move to R.
In this example we look at a factor variable and see how R is treating it the datset is `InsectSprays` and we're going to fit a linear model to it. 

```{r, echo = T, eval = T}
require(datasets);data(InsectSprays); require(stats); require(ggplot2)
g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill  = spray))
g = g + geom_violin(colour = "black", size = 2)
g = g + xlab("Type of spray") + ylab("Insect count")
g
```

So, presumably,
number left after applying the spray, and the spray factor is the type of spray,
okay? And then it gives some examples
of working with this data, but we don't need that cuz we're
gonna build our own examples. So let's first plot some of the data. So I want to do a ggplot and
I've already loaded ggplot2, but just to remind you, in case you're
restarting your R session from earlier, you want to make sure
that you acquire ggplot2. There, it's loaded. And then I have my ggplot and
then my data is InsectSprays. And then for my aesthetic,
my Y is the count, the number of insects. My X is the spray. They don't give you too much
information about the sprays, but there's a couple of different
sprays that they use. And then I want to fill the objects I'm
creating with the factor variable spray. So there I've created my ggplot. And then I wanna do a violin plot. A violin plot is kind of like a histogram
but sort of tilted on its side. And then they repeat it on both sides so
it looks a little like a violin. Well, it looks like a violin
if you're data cooperates. Otherwise, it looks like a blob. Okay, there's our violin plot. And then I wanna set my labels. And then if you wanna actually see
the plot, you gotta bring it up. Okay, so, here's my violin plot. So you see there's sprays eight
labeled spray A, B, C, D, E, and F? Okay.
And you can see the insect counts, so I presume they applied the spray
to numerous batches of insects and they,
>> It's unfortunate they're not telling me whether or not the count is the count of
the number of alive or the number dead. So we don't know if this is
a better spray, a better, let's say it's a mosquito spray or something like
that, cuz no one likes mosquitoes. We don't know if this is a better spray or
a worse spray. But let's talk about how we can
test the difference between different factor levels in
this case using linear models. And then, at the end I'll talk about some
shortcomings of the approach that I'm proposing here. But here's a violin plot. And let me just do head Insect sprays to just show you the data, what it looks
like, to see we have a bunch of counts. And then,
the spray label's a very simple data set. And so, let's look at what happens when we
include insect spray as a linear model and y as an outcome. So, let's fit our model. And now, what we're fitting is, our outcome is the count,
the number of insects. Our predictor is the spray,
which spray was used as a factor variable. It's already a factor variable. And then, I give it the data-set. And then, here, I just want
the summary of the output from lm. Again, normally you wanna
assign your lm to a variable so you can keep it for later. And then, I just wanna, for, to keep
the printing a little bit self-contained, I'm grabbing the coefficient table. And so, there you see, the Intercept,
spray B, spray C, spray D, spray E, and spray F. And spray A is conspicuously missing. And the idea is that everything here
is now in comparison with spray A. So, this 0.833 is the change in the mean between spray B and spray A. In this case, 14.5,
the intercept is the mean for spray A. And if you look over here at our plot,
that seems about right. Look at our violin plot. 14.5 seems about right for spray A. And spray B, it seems reasonable
that it would be off by, it would be changed just by
a little bit from spray A. Now, spray C looks like it
has a much lower count, okay? And look, it's coefficient is minus 12. Okay?
And that looks like about right. So this one's at 14.5. And somewhere around two seems
about right for this one, spray C. And so, that's exactly what
this coefficient is saying. This negative 12 here is the different
between spray C minus spray A. Now, if we wanted to compare,
for example, spray B and spray C, we would have to subtract this,
0.833, and this negative12. Now, we wouldn't have a standard error for
that comparison immediately. However, that would give us the estimate. If I were to take the average count for
the sprays, for those with spray A, I would get 14.5. If I were to take the average count for spray B, I would get 14.5 plus 0.833. So, I'd like now to show you how I
can hard code the same model and not rely on r to actually
pick the reference level. So, remember what I did last time
is I did count was my outcome and my factor variable spray was my predictor. And what r does is it picks the spray
level that's the lowest alphanumerically. So, in this case, spray level A,
to set as the reference level. So let me show how you can hard
code that myself manually. So, here count is my outcome. And then, I'm gonna create a variable
using the I function which in lm actually performs the operation inside the
regression, inside the model statement. So, here I just wanna look at the
instances where the spray is equal to B. And then, I multiply that times 1 to
change it from Boolean to numeric. And then, here's a variable that's
1 when spray is C, and 0 otherwise. And here's a variable that's 1 when
the spray is D, and 0 otherwise. And here's one for E,
and here's one for F. So, I've included all of them except A. So, I've forced A to
be my reference level. And I'm going to run this model. And it should give me the same
result as with r did. It's just now I've shown you exactly how
r is creating the regression variables. So, let me just remind ourselves
what r gives us when we run, and let it handle the factor
variable by itself. And then, let me do the same thing where
I've created my own factor variables. And then, you can see 14.5, 14.5, 0.833, 0.833, you can see that it's identical. So this is what r is
doing behind the scenes. And let's keep exploring this because
this is kind of an important point. If you mess this up with factor variables,
you get very incorrect conclusions. Now, let me show you what happens
if I do include spray A here. So, I've done the same model I did before
where I include all the variables, but now I'm also including
an extra variable for spray A. And let me just do the lm part. And notice it gives an NA in
front of the spray A coefficient. And the reason for
that is because it's redundant. We have six means, right? For six sprays. And we have seven parameters in intercept. And then, now I've tried to put
in six regression parameters. I have six means to fit seven parameters. It can't do that, so
it's gonna drop one of them. Now, what if I do want my coefficients, instead of being interpreted as
levels referenced to a control level, what if I want my coefficients to
be the mean for each of the groups? Well, you can do that, but
you have to remove the intercept. So, watch what happen when I say count is
my outcome, and spray is my predictor, but I remove the intercept. Then notice what happens is that now I get a different set of
coefficients, one for each spray level. So, it includes A, B, C, D, E and F. It hasn't dropped any levels. And it can do that now because it has six
parameters, and six means to work with. And these are exactly equal to
the means for each spray in the data. So, if I were to just go ahead and
calculate the means for each spray, right? It works out to be the same numbers. 14.5, 15.3, 2.08 in both, and so on. Now, I want to emphasize this model is no different than my model
that included an intercept. Why don't I go back to my model with
my intercept just to illustrate this. So, now it's just that the coefficients
have a different interpretation. Now, the intercept from the model,
when I fit count as spray but included the intercept,
my intercept now is interpreted, 14.5, as the mean for spray A. And you can see that it's
exactly the empirical mean for spray A when I calculate the mean. It works out that way. And then, spray B,
we talked about earlier, was the comparison between the reference
level spray A and spray B. Okay?
So, if I add these together, 14.5 and 0.833,
I should get the mean for spray B. Okay, and that's what you see, 14.5 plus
0.833, that gets me 15.33, and so on. So, If I add 14.5 and
minus 12, I'm gonna get 2.08. If I add 14.5 and negative 9,
I get 4.9, and so on. So, this model,
where I've included an intercept, has all the same information as
the model where I omitted the intercept, the only difference is how
the coefficients are interpreted. In the model with the intercept now
the intercept is interpreted as the sprayA mean and
all the coefficients are interpreted as relative to sprayA
differences from sprayA. And then if I would have fit it without
the intercept then I get the mean for each spray. And if I want differences then I
have to subtract the coefficients. And you usually want one of them to
be a reference level because then you can do test. So now my p values are testing whether or
not, for the t test whether or not A is different from B, and A is
different from C, and A is different from D, and so on, whereas the p values from
this test are just testing whether or not those means are different from 0,
which is a very different test. Did sprayA kill any insects is what
this is testing, where in this one, the sprayB row is testing whether
sprayA is different from sprayB. So, what I'm trying to illustrate is that,
how you play around with factor variables in LM is very important
in terms of how you interpret it. It's not just a conceptual or
theoretical thing to worry about it. It's a very practical
thing to worry about. What your intercept means changes
dramatically depending on what your reference level is or whether or
not you include an intercept. Let me do one last thing. Where now I show you how
you can re-level and in this case sprayA was my reference
level you can very easily re-level it. So say sprayC is your reference level. So now here I just use
the re level command so now inspect spray,
the reference level is sprayC but now I've just created a new
variable where that spray2. And now I'm gonna do my linear
model where my outcome is my count. And spray2 is my predictor
now instead of spray. And this is the one that has
C as the reference level. And then R knows not to do the one that
has the lowest alphanumeric letter, but instead has the reference
level that I set. And there when I do it
notice sprayA is present, sprayC is gone cuz now
it's the reference level. 


## Adjustment


## Residuals again



## Model selection


## Practical R Exercises in swirl
During this week of the course you should complete the following lessons in the Regression Models swirl course:

1. MultiVar Examples2
2. MultiVar Examples3
3. Residuals Diagnostics and Variation

## Week 3 Quiz


## (OPTIONAL) Practice exercise in regression modeling


