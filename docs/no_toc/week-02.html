<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Week 02 | Course Name</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Week 02 | Course Name" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Week 02 | Course Name" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="week-01.html"/>
<link rel="next" href="week-03.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#available-course-formats"><i class="fa fa-check"></i><b>0.1</b> Available course formats</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="week-01.html"><a href="week-01.html"><i class="fa fa-check"></i><b>1</b> Week 01</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-01.html"><a href="week-01.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="week-01.html"><a href="week-01.html#welcome-to-regression-models"><i class="fa fa-check"></i><b>1.1.1</b> Welcome to Regression Models</a></li>
<li class="chapter" data-level="1.1.2" data-path="week-01.html"><a href="week-01.html#some-basics"><i class="fa fa-check"></i><b>1.1.2</b> Some Basics</a></li>
<li class="chapter" data-level="1.1.3" data-path="week-01.html"><a href="week-01.html#syllabus-xxx"><i class="fa fa-check"></i><b>1.1.3</b> Syllabus (xxx)</a></li>
<li class="chapter" data-level="1.1.4" data-path="week-01.html"><a href="week-01.html#data-science-specialization-community-site"><i class="fa fa-check"></i><b>1.1.4</b> Data Science Specialization Community Site</a></li>
<li class="chapter" data-level="1.1.5" data-path="week-01.html"><a href="week-01.html#where-to-get-more-advanced-material"><i class="fa fa-check"></i><b>1.1.5</b> Where to get more advanced material</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="week-01.html"><a href="week-01.html#introduction-to-regression-and-least-squares"><i class="fa fa-check"></i><b>1.2</b> Introduction to regression and least squares</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="week-01.html"><a href="week-01.html#introduction-to-regression"><i class="fa fa-check"></i><b>1.2.1</b> Introduction to Regression</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="week-01.html"><a href="week-01.html#linear-least-squares"><i class="fa fa-check"></i><b>1.3</b> Linear least squares</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="week-01.html"><a href="week-01.html#notations-and-background"><i class="fa fa-check"></i><b>1.3.1</b> Notations and background</a></li>
<li class="chapter" data-level="1.3.2" data-path="week-01.html"><a href="week-01.html#linear-least-squares-1"><i class="fa fa-check"></i><b>1.3.2</b> Linear Least Squares</a></li>
<li class="chapter" data-level="1.3.3" data-path="week-01.html"><a href="week-01.html#linear-least-squares-coding-example"><i class="fa fa-check"></i><b>1.3.3</b> Linear Least Squares Coding Example</a></li>
<li class="chapter" data-level="1.3.4" data-path="week-01.html"><a href="week-01.html#mathematical-details-optional-xxx"><i class="fa fa-check"></i><b>1.3.4</b> Mathematical Details (Optional) XXX</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="week-01.html"><a href="week-01.html#regression-to-the-mean"><i class="fa fa-check"></i><b>1.4</b> Regression to the Mean</a></li>
<li class="chapter" data-level="1.5" data-path="week-01.html"><a href="week-01.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>1.5</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="1.6" data-path="week-01.html"><a href="week-01.html#week-1-quiz"><i class="fa fa-check"></i><b>1.6</b> Week 1 Quiz</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-02.html"><a href="week-02.html"><i class="fa fa-check"></i><b>2</b> Week 02</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-02.html"><a href="week-02.html#statistical-linear-regression-models"><i class="fa fa-check"></i><b>2.1</b> Statistical linear regression models</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="week-02.html"><a href="week-02.html#statistical-linear-regression-models-1"><i class="fa fa-check"></i><b>2.1.1</b> Statistical Linear Regression Models</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="week-02.html"><a href="week-02.html#residuals"><i class="fa fa-check"></i><b>2.2</b> Residuals</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="week-02.html"><a href="week-02.html#optional-reading-how-to-derive-r-squared"><i class="fa fa-check"></i><b>2.2.1</b> Optional reading How to derive R squared:</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="week-02.html"><a href="week-02.html#inference-in-regression"><i class="fa fa-check"></i><b>2.3</b> Inference in regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="week-02.html"><a href="week-02.html#prediction"><i class="fa fa-check"></i><b>2.3.1</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="week-02.html"><a href="week-02.html#for-the-project"><i class="fa fa-check"></i><b>2.4</b> For the project</a></li>
<li class="chapter" data-level="2.5" data-path="week-02.html"><a href="week-02.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>2.5</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="2.6" data-path="week-02.html"><a href="week-02.html#week-2-quiz"><i class="fa fa-check"></i><b>2.6</b> Week 2 Quiz</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-03.html"><a href="week-03.html"><i class="fa fa-check"></i><b>3</b> Week 03</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-03.html"><a href="week-03.html#multi-variable-regression"><i class="fa fa-check"></i><b>3.1</b> Multi-variable regression</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="week-03.html"><a href="week-03.html#how-to-get-the-coefficients-derivation-of-formulas"><i class="fa fa-check"></i><b>3.1.1</b> How to get the coefficients, derivation of formulas</a></li>
<li class="chapter" data-level="3.1.2" data-path="week-03.html"><a href="week-03.html#results"><i class="fa fa-check"></i><b>3.1.2</b> Results</a></li>
<li class="chapter" data-level="3.1.3" data-path="week-03.html"><a href="week-03.html#example-with-two-variables-simple-linear-regression"><i class="fa fa-check"></i><b>3.1.3</b> Example with two variables, simple linear regression</a></li>
<li class="chapter" data-level="3.1.4" data-path="week-03.html"><a href="week-03.html#the-general-case"><i class="fa fa-check"></i><b>3.1.4</b> The general case</a></li>
<li class="chapter" data-level="3.1.5" data-path="week-03.html"><a href="week-03.html#examples-with-multiple-variables"><i class="fa fa-check"></i><b>3.1.5</b> Examples with multiple-variables</a></li>
<li class="chapter" data-level="3.1.6" data-path="week-03.html"><a href="week-03.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>3.1.6</b> Interpretation of coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="week-03.html"><a href="week-03.html#multi-variable-regression-tips-and-tricks"><i class="fa fa-check"></i><b>3.2</b> Multi-variable regression tips and tricks</a></li>
<li class="chapter" data-level="3.3" data-path="week-03.html"><a href="week-03.html#adjustment"><i class="fa fa-check"></i><b>3.3</b> Adjustment</a></li>
<li class="chapter" data-level="3.4" data-path="week-03.html"><a href="week-03.html#residuals-again"><i class="fa fa-check"></i><b>3.4</b> Residuals again</a></li>
<li class="chapter" data-level="3.5" data-path="week-03.html"><a href="week-03.html#model-selection"><i class="fa fa-check"></i><b>3.5</b> Model selection</a></li>
<li class="chapter" data-level="3.6" data-path="week-03.html"><a href="week-03.html#practical-r-exercises-in-swirl-2"><i class="fa fa-check"></i><b>3.6</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="3.7" data-path="week-03.html"><a href="week-03.html#week-3-quiz"><i class="fa fa-check"></i><b>3.7</b> Week 3 Quiz</a></li>
<li class="chapter" data-level="3.8" data-path="week-03.html"><a href="week-03.html#optional-practice-exercise-in-regression-modeling"><i class="fa fa-check"></i><b>3.8</b> (OPTIONAL) Practice exercise in regression modeling</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-04.html"><a href="week-04.html"><i class="fa fa-check"></i><b>4</b> Week 04</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-04.html"><a href="week-04.html#glm"><i class="fa fa-check"></i><b>4.1</b> GLM</a></li>
<li class="chapter" data-level="4.2" data-path="week-04.html"><a href="week-04.html#logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.3" data-path="week-04.html"><a href="week-04.html#poisson-regression"><i class="fa fa-check"></i><b>4.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="4.4" data-path="week-04.html"><a href="week-04.html#hodgepodge"><i class="fa fa-check"></i><b>4.4</b> Hodgepodge</a></li>
<li class="chapter" data-level="4.5" data-path="week-04.html"><a href="week-04.html#practical-r-exercises-in-swirl-3"><i class="fa fa-check"></i><b>4.5</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="4.6" data-path="week-04.html"><a href="week-04.html#week-4-quiz"><i class="fa fa-check"></i><b>4.6</b> Week 4 Quiz</a></li>
<li class="chapter" data-level="4.7" data-path="week-04.html"><a href="week-04.html#course-project-1"><i class="fa fa-check"></i><b>4.7</b> Course Project</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Name</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="week-02" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Week 02</h1>
<div id="statistical-linear-regression-models" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Statistical linear regression models</h2>
<p>Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know how to extend our estimates to a population. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity.</p>
<div id="statistical-linear-regression-models-1" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Statistical Linear Regression Models</h3>
<p>Finding a good regression line using least squares is a mathematical procedure. However, we’d like to do statistics. We’d like to draw emphasis based on our data. In other words we’d like to generalize from our data to a population using statistical models. Consider the probabilistic model for linear regression
<span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span>
The values of <span class="math inline">\(\beta_0, \beta_1\)</span> are the population parameters that we would like to estimate. <span class="math inline">\(X_i\)</span> is a collection of explanatory variables that we do know, and <span class="math inline">\(\epsilon_i\)</span> is iid Gaussian errors.</p>
<ul>
<li>Here the <span class="math inline">\(\epsilon_{i}\)</span> are assumed iid <span class="math inline">\(N(0, \sigma^2)\)</span>.</li>
</ul>
<p>Understanding independent errors in regression can be approached in various ways. One relatively straightforward interpretation is to consider them as the cumulative effect of unmodeled variables that might collectively influence the response. These unmodeled variables act on the response in a manner that can be statistically modeled as independent and identically distributed Gaussian errors.</p>
<p>Setting aside the complexities of interpretation, let’s focus on the mechanics of working with statistical inference for regression. It’s important to note that the expected value of the response given a specific value of the regressor is simply the line at that regressor, represented as <span class="math inline">\(β_0 + β₁x_i\)</span>. Additionally, the variance of the response at any given value of the regressor is denoted as σ². It’s crucial to clarify that this variance pertains to the variation around the regression line and not the overall response variance. Conditioning on X reduces the variation, making it lower than the unconditional response variance.</p>
<ul>
<li>Note, <span class="math inline">\(E[Y_i ~|~ X_i = x_i] = \mu_i = \beta_0 + \beta_1 x_i\)</span></li>
<li>Note, <span class="math inline">\(Var(Y_i ~|~ X_i = x_i) = \sigma^2\)</span>.</li>
</ul>
<p>Both the expected value and variance mentioned here are population quantities. Although there are sample analogs that estimate these values, it’s essential to recognize that, at this point, we are referring to population quantities—these are the estimands that we ideally want to know.</p>
<p>Now that we have a formal statistical framework, we can interpret our regression coefficients with respect to that framework. Take for example, the intercept. It is the expected value <span class="math inline">\(Y\)</span> given that the regressor is 0. <span class="math display">\[E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0\]</span></p>
<p>Note that the regressor being equal to zero is often not of interest in the study. For example, if the regression variable is blood pressure, probably you’re not interested in the response for among people with blood pressure of zero. However, there is an easy fix for this. Consider just shifting our regression variable by a constant <span class="math inline">\(a\)</span>.
<span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \epsilon_i
= \tilde \beta_0 + \beta_1 (X_i - a) + \epsilon_i
\]</span>
We see a new regression line with a new intercept and the same slope. So, shifting your <span class="math inline">\(X\)</span> values by value <span class="math inline">\(a\)</span> changes the intercept, but not the slope.</p>
<ul>
<li>Often <span class="math inline">\(a\)</span> is set to <span class="math inline">\(\bar X\)</span> so that the intercept is interpretted as the expected response at the average <span class="math inline">\(X\)</span> value.</li>
</ul>
<p>For slope, we can interpret it as the expected change in response for a 1 unit change in the predictor.
<span class="math display">\[
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
\]</span>
* Consider the impact of changing the units of <span class="math inline">\(X\)</span>.
<span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
= \beta_0 + \frac{\beta_1}{a} (X_i a) + \epsilon_i
= \beta_0 + \tilde \beta_1 (X_i a) + \epsilon_i
\]</span>
We see a new regression line with a new slope and the same intercept. So, multiplying your <span class="math inline">\(X\)</span> values by value <span class="math inline">\(a\)</span> changes the slope, but not the intercept. For example, <span class="math inline">\(X\)</span> is height in <span class="math inline">\(m\)</span> and <span class="math inline">\(Y\)</span> is weight in <span class="math inline">\(kg\)</span>. Then <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(kg/m\)</span>. Converting <span class="math inline">\(X\)</span> to <span class="math inline">\(cm\)</span> implies multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(100 cm/m\)</span>. To get <span class="math inline">\(\beta_1\)</span> in the right units, we have to divide by <span class="math inline">\(100 cm /m\)</span> to get it to have the right units.</p>
<p><span class="math display">\[
X m \times \frac{100cm}{m} = (100 X) cm
~~\mbox{and}~~
\beta_1 \frac{kg}{m} \times\frac{1 m}{100cm} = 
\left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
\]</span></p>
<p>If we would like to guess the outcome at a particular value of the predictor, say <span class="math inline">\(X\)</span>, the regression model guesses <span class="math display">\[\hat \beta_0 + \hat \beta_1 X\]</span></p>
<p>This doesn’t mean that we can only predict at the fitted values. We can predict at any value of <span class="math inline">\(X\)</span> by plugging in the value of <span class="math inline">\(X\)</span> into the equation. However, we’re going to have more reasonable predictions if the value of <span class="math inline">\(X\)</span> that we plug in is in the cloud of data that we used to build the model. Later on, we’ll also talk about how to account for that kind of uncertainty with prediction intervals. But for the time being, let’s just talk about how we get a prediction. Let’s go through an example to interpret the regression
coefficients and show running of the regression coefficient. The dataset is the diamond dataset from the <code>UsingR</code> package. The data is diamond prices in Singapore dollars and diamond weight in carats, which is a standard measure of diamond mass.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="week-02.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR)</span></code></pre></div>
<pre><code>## Loading required package: MASS</code></pre>
<pre><code>## Loading required package: HistData</code></pre>
<pre><code>## Loading required package: Hmisc</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## Loading required package: Formula</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;Hmisc&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     format.pval, units</code></pre>
<pre><code>## 
## Attaching package: &#39;UsingR&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:survival&#39;:
## 
##     cancer</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="week-02.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamond)</span>
<span id="cb71-2"><a href="week-02.html#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb71-3"><a href="week-02.html#cb71-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(diamond, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> price))</span>
<span id="cb71-4"><a href="week-02.html#cb71-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Mass (carats)&quot;</span>)</span>
<span id="cb71-5"><a href="week-02.html#cb71-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Price (SIN $)&quot;</span>)</span>
<span id="cb71-6"><a href="week-02.html#cb71-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)</span>
<span id="cb71-7"><a href="week-02.html#cb71-7" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha=</span><span class="fl">0.2</span>)</span>
<span id="cb71-8"><a href="week-02.html#cb71-8" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb71-9"><a href="week-02.html#cb71-9" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>In this code we assign variable <code>g</code> to the <code>ggplot</code>, the dataset is diamond, the aesthetic has the horizontal axis variable as carat and the y-axis variable as price, we add a layer where the xlab is <code>Mass in carats</code> and the y label price in Singapore dollars. We also add the points of the black background and then a light alpha blending color on top. Afterwards we add a layer that is <code>geom_smooth</code> where <code>method = "lm"</code> will add the regression line. If you omit any arguments, it’s just going to assume the regression
line with <span class="math inline">\(Y\)</span> as the outcome and <span class="math inline">\(X\)</span> as the predictor. Finally, we indicate the color of the regression line as black and call the plot.</p>
<p>Notice what we are plotting is the fitted line, the line that minimizes the sum of the squared vertical distances between the points and the lines.
By default, <code>lm</code> includes an intercept, if you don’t want an intercept, you have to explicitly force it in the model. We also want the dataset to be the diamond dataset in other words, we have to give it the data frame. Otherwise, <code>lm</code> looks in the regular R environment for variables in the model. After running the code it basically just prints out the coefficients <span class="math inline">\(\beta_0, \beta_1\)</span>, which are the intercept and labels it as Intercept and the regression variable for the carat, the slope for the carat regression variable.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="week-02.html#cb73-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> carat, <span class="at">data =</span> diamond)</span>
<span id="cb73-2"><a href="week-02.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit)</span></code></pre></div>
<pre><code>## (Intercept)       carat 
##   -259.6259   3721.0249</code></pre>
<p>Let’s look at this <span class="math inline">\(3,721\)</span> variable and try to interpret it. It’s saying that we have an expected <span class="math inline">\(3,721\)</span> Singapore dollar increase in price for every carat increase in mass of the diamond. The intercept, <span class="math inline">\(-259\)</span> is the expected price of a <span class="math inline">\(0\)</span> carat diamond not very interesting, because we’re not interested in zero carat diamonds.</p>
<p><strong>A side note</strong>, if you want a much more detailed printout by doing <code>summary(fit)</code> which is the summary of the outputted variable from <code>lm</code> and you get this more elaborate printout.</p>
<pre><code>## 
## Call:
## lm(formula = price ~ carat, data = diamond)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -85.159 -21.448  -0.869  18.972  79.370 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -259.63      17.32  -14.99   &lt;2e-16 ***
## carat        3721.02      81.79   45.50   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.84 on 46 degrees of freedom
## Multiple R-squared:  0.9783, Adjusted R-squared:  0.9778 
## F-statistic:  2070 on 1 and 46 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>If we mean center our <span class="math inline">\(X\)</span> variable, so that the intercept is on a more interpretable scale. Here we assign the output to a different variable, <code>fit2</code> instead of <code>fit</code>, because we don’t want to overwrite the original <code>fit</code>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="week-02.html#cb76-1" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">I</span>(carat <span class="sc">-</span> <span class="fu">mean</span>(carat)), <span class="at">data =</span> diamond)</span>
<span id="cb76-2"><a href="week-02.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit2)</span></code></pre></div>
<pre><code>##            (Intercept) I(carat - mean(carat)) 
##               500.0833              3721.0249</code></pre>
<p>As you notice in code: <code>lm</code> is again the linear model procedure, the outcome stays the same and we use <code>carat - mean(carat)</code>, and the <code>I</code> is to indicate that we want to do arithmetic on the variable. So, we want to subtract the mean of the carat variable from the carat variable. This is a way to mean center the variable.
As we expected the slope stays the same, 3,721, but the intercept has changed to <span class="math inline">\(500\)</span>, meaning <span class="math inline">\(\$ 500\)</span>, Singapore dollars is the expected price of the average sized diamond. In this case, the average diamond is about 0.2 carats. A one carat increase is actually kind of big. What about changing the units to one-tenth of a carat? We can do this just by dividing the coefficient by ten. So we know that we would expect to see a <span class="math inline">\(\$372\)</span> increase in price for every <span class="math inline">\(0.1\)</span> of a carat increase in the mass of a diamond.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="week-02.html#cb78-1" aria-hidden="true" tabindex="-1"></a>fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">I</span>(carat<span class="sc">/</span><span class="dv">10</span>), <span class="at">data =</span> diamond)</span>
<span id="cb78-2"><a href="week-02.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit3)</span></code></pre></div>
<pre><code>## (Intercept) I(carat/10) 
##   -259.6259  37210.2485</code></pre>
<p>In the linear model fit instead of putting in carat, we put in <span class="math inline">\(carat * 10\)</span>, the units of this new variable is one-tenth of a carat. The data is of course, still the diamond dataset.</p>
<p>Imagine if someone came to you with three new diamonds that they had 0.16 carats, 0.27 carats and 0.35 carats, and they wanted to know what you would estimate the price would be. Well, you could do it manually by grabbing the two coefficients in multiplying the intercept or adding the intercept plus the slope times these new values. Let’s do that:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="week-02.html#cb80-1" aria-hidden="true" tabindex="-1"></a>newx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.16</span>, <span class="fl">0.27</span>, <span class="fl">0.34</span>)</span>
<span id="cb80-2"><a href="week-02.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> newx</span></code></pre></div>
<pre><code>## [1]  335.7381  745.0508 1005.5225</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="week-02.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">carat =</span> newx))</span></code></pre></div>
<pre><code>##         1         2         3 
##  335.7381  745.0508 1005.5225</code></pre>
<p>Often, you don’t want to do even that much coding, you want to more general method, especially when you get lots of regression variables. So there’s this general method called
predict that will take the output from several different kinds of model fits. Linear models are one example, but predict is a generic function, and it applies to several different prediction models. The new data is a <code>data.frame(catar=newx)</code> that has the new values of <span class="math inline">\(X\)</span> for the carat variable. Then when we do that, what you’ll see is the same answer. The difference is that it scales up when we have lots of regressors in much more complicated settings. In general, we want to predict using the predict function. If you omit this new data statement if you just do predict fit, it predicts at the observed <span class="math inline">\(X\)</span> values, so it gives you the <span class="math inline">\(\hat Y\)</span> values. If you want it at new <span class="math inline">\(X\)</span> values, you have to give it this new data argument.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="week-02.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamond)</span>
<span id="cb84-2"><a href="week-02.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamond<span class="sc">$</span>carat, diamond<span class="sc">$</span>price,  </span>
<span id="cb84-3"><a href="week-02.html#cb84-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Mass (carats)&quot;</span>, </span>
<span id="cb84-4"><a href="week-02.html#cb84-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Price (SIN $)&quot;</span>, </span>
<span id="cb84-5"><a href="week-02.html#cb84-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">bg =</span> <span class="st">&quot;lightblue&quot;</span>, </span>
<span id="cb84-6"><a href="week-02.html#cb84-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">cex =</span> <span class="fl">1.1</span>, <span class="at">pch =</span> <span class="dv">21</span>,<span class="at">frame =</span> <span class="cn">FALSE</span>)</span>
<span id="cb84-7"><a href="week-02.html#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb84-8"><a href="week-02.html#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(diamond<span class="sc">$</span>carat, <span class="fu">predict</span>(fit), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb84-9"><a href="week-02.html#cb84-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">c</span>(<span class="fl">0.16</span>, <span class="fl">0.16</span>, <span class="fl">0.12</span>), </span>
<span id="cb84-10"><a href="week-02.html#cb84-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(<span class="dv">200</span>, <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.16</span>,</span>
<span id="cb84-11"><a href="week-02.html#cb84-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.16</span>))</span>
<span id="cb84-12"><a href="week-02.html#cb84-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">c</span>(<span class="fl">0.27</span>, <span class="fl">0.27</span>, <span class="fl">0.12</span>), </span>
<span id="cb84-13"><a href="week-02.html#cb84-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(<span class="dv">200</span>, <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.27</span>,</span>
<span id="cb84-14"><a href="week-02.html#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.27</span>))</span>
<span id="cb84-15"><a href="week-02.html#cb84-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">c</span>(<span class="fl">0.34</span>, <span class="fl">0.34</span>, <span class="fl">0.12</span>), </span>
<span id="cb84-16"><a href="week-02.html#cb84-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(<span class="dv">200</span>, <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.34</span>,</span>
<span id="cb84-17"><a href="week-02.html#cb84-17" aria-hidden="true" tabindex="-1"></a>        <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.34</span>))</span>
<span id="cb84-18"><a href="week-02.html#cb84-18" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(newx, <span class="fu">rep</span>(<span class="dv">250</span>, <span class="dv">3</span>), <span class="at">labels =</span> newx, <span class="at">pos =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>To illustrate, here’s our observe data points in blue. The fitted values when we do the predict command, the fitted values in red all of the observed <span class="math inline">\(X\)</span> values and their associated fitted points on the line. These are if we were to draw vertical lines from the observed data points on to the fitted line, they would occur on these red points. When we predicted a new value of <span class="math inline">\(X\)</span>, we’re finding a point along this horizontal axis. In this example we want, 0.16, 0.27 and 0.34. We’re drawing a line up to the fitted regression line and then over to dollars and those are our predicted dollar amounts.</p>
</div>
</div>
<div id="residuals" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Residuals</h2>
<p>Residuals represent variation left unexplained by our model. We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors.</p>
<p>To begin, let’s delve into our illustrative example featuring the diamond dataset. It’s important to recall that in this dataset, the diamonds are priced in Singapore dollars. The key variable under consideration is the weight of the diamonds, expressed in carats. Our objective is to explore the correlation between the weight of diamonds and their corresponding prices, seeking to understand how variations in diamond prices can be elucidated by their mass.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="week-02.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR)</span>
<span id="cb85-2"><a href="week-02.html#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamond)</span>
<span id="cb85-3"><a href="week-02.html#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb85-4"><a href="week-02.html#cb85-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(diamond, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> price))</span>
<span id="cb85-5"><a href="week-02.html#cb85-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Mass (carats)&quot;</span>)</span>
<span id="cb85-6"><a href="week-02.html#cb85-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Price (SIN $)&quot;</span>)</span>
<span id="cb85-7"><a href="week-02.html#cb85-7" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb85-8"><a href="week-02.html#cb85-8" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)</span>
<span id="cb85-9"><a href="week-02.html#cb85-9" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha=</span><span class="fl">0.2</span>)</span>
<span id="cb85-10"><a href="week-02.html#cb85-10" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now, our focus is on elucidating the price (on the vertical axis) through the mass (on the horizontal axis). Without taking mass into account, we’d have a scatter of points projecting onto the vertical axis, displaying considerable variation. Disregarding mass would result in a notable amount of unexplained variation. However, when we factor in mass, the variation diminishes, as we’re now examining the variation around the regression line.</p>
<p>This remaining variation around the regression line is termed residual variation. It represents the portion of variation that persists even after accounting for mass. Initially, there is substantial variation, a significant portion of which is clarified by the linear relationship with mass. Nonetheless, there remains some residual variation. These residual distances are referred to as residuals, and they constitute the focal point of today’s lecture. Residuals prove to be valuable for various diagnostic purposes, including assessing model fit. Let’s refresh our memory regarding the model under consideration.</p>
<p>The outcome in our example,price, is <span class="math inline">\(Y_i\)</span>, which we’re assuming is a line. Observed outcome <span class="math inline">\(i\)</span> is <span class="math inline">\(Y_i\)</span> at predictor value <span class="math inline">\(X_i\)</span>, predicted outcome <span class="math inline">\(i\)</span> is <span class="math inline">\(\hat Y_i\)</span> at predictor value <span class="math inline">\(X_i\)</span> is <span class="math inline">\(\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i\)</span>. Residual, the between the observed and predicted outcome <span class="math inline">\(e_i = Y_i - \hat Y_i\)</span>, which is the vertical distance between the observed data point and the regression line where least squares minimizes <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span>. In essence, it was minimizing the sum of the squared residual, summation <span class="math inline">\(e_i\)</span> squared. One way to think about the residuals are as an estimate of <span class="math inline">\(\epsilon_i\)</span>, though, you have to be careful with that, because as we will see later on, we can decrease the residuals just by adding irrelevant regressors into the equation. Let’s talk about some aspects of residuals that will help us interpret them.</p>
<ul>
<li><span class="math inline">\(E[e_i] = 0\)</span>. (Their population’s expected value is zero.)</li>
<li>If an intercept is included, <span class="math inline">\(\sum_{i=1}^n e_i = 0\)</span> (Their empirical sum, hence the empirical mean also, is zero if you include an intercept.If you don’t include an intercept, this property doesn’t have to hold.)</li>
<li>If a regressor variable, <span class="math inline">\(X_i\)</span>, is included in the model <span class="math inline">\(\sum_{i=1}^n e_i X_i = 0\)</span>. (The generalization of this property is, if you include any regression term in linear regression, the sum of the residuals times that regression variable has to be zero.)</li>
<li>Residuals are useful for investigating poor model fit. (We can create plots that highlight the aspects of poor model fit.)</li>
<li>Positive residuals are above the line, negative residuals are below.</li>
<li>Residuals can be thought of as the outcome (<span class="math inline">\(Y\)</span>) with the linear association of the predictor (<span class="math inline">\(X\)</span>) removed. (A common use of residuals is to think of them as the outcome <span class="math inline">\(Y\)</span> with the linear influence of the predictor <span class="math inline">\(X\)</span> having been removed. For example, if we wanted to in some subsequent model or some subsequent analysis diamond prices, but in a way that has already been adjusted for their weight, calibrating all the diamond prices to be on the same scale regardless of their weight, we would take those residuals from the model fit that has diamond prices as the outcome, and weight as the predictor.)</li>
<li>One differentiates residual variation (variation after removing the predictor) from systematic variation (variation explained by the regression model). (It’s very common to take residuals and carry them forward in a later analysis where you want to think of them as the, the new outcome, having removed the predictor at that point. But, remember with linear regression, you’re only removing the linear component of the predictor. One should differentiate between residual variation, which is variation that is left over after the explanatory variable has been accounted for in a linear fashion, from systematic variation, which is variation explained by the regression model. Again, residual plots can highlight poor model fit. And, we are going to go through some residual plots.)</li>
<li>Residual plots highlight poor model fit.</li>
</ul>
<p>Let’s walk through calculating residuals in this example we’re going to use the diamond dataset.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="week-02.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamond)</span>
<span id="cb87-2"><a href="week-02.html#cb87-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> diamond<span class="sc">$</span>price; x <span class="ot">&lt;-</span> diamond<span class="sc">$</span>carat; n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb87-3"><a href="week-02.html#cb87-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb87-4"><a href="week-02.html#cb87-4" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(fit)</span>
<span id="cb87-5"><a href="week-02.html#cb87-5" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit)</span>
<span id="cb87-6"><a href="week-02.html#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(<span class="fu">abs</span>(e <span class="sc">-</span>(y <span class="sc">-</span> yhat)))</span></code></pre></div>
<pre><code>## [1] 8.242296e-13</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="week-02.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(<span class="fu">abs</span>(e <span class="sc">-</span> (y <span class="sc">-</span> <span class="fu">coef</span>(fit)[<span class="dv">1</span>] <span class="sc">-</span> <span class="fu">coef</span>(fit)[<span class="dv">2</span>] <span class="sc">*</span> x)))</span></code></pre></div>
<pre><code>## [1] 8.242296e-13</code></pre>
<p>We redefine price as <code>y</code> and <code>x</code> as carat, <code>n</code> as the length of the number of pairs. We assign the linear regression object from <code>lm</code> to variable <code>fit</code>. To get the residuals we <code>resid(fit)</code>, and we assign that to <code>e</code>. We also get the fitted values by <code>predict(fit)</code> and assign that to <code>yhat</code>. We can check that the residuals are the difference between the observed outcome and the predicted outcome. We can also check that the residuals are the difference between the observed outcome and the intercept plus the slope times the predictor. To show you that residual’s calculated via <code>resid()</code> functions are the same as the residuals that we calculate manually we take the absolute difference between <code>y - yhat</code> and <code>e</code> and find the one is on the scale of <span class="math inline">\(10^{-13}\)</span> i.e, up to numerical precision, it’s the same thing. Then lastly, we want to show that the residuals are the difference between the observed outcome and the intercept plus the slope times the predictor, again up to numeric precision, exactly the same. To obtain the residuals, the preferred method is to use <code>resid()</code>. However, by demonstrating an alternative code, we aim to shed light on the underlying process of “res” and the specific computation performed by <code>resid()</code>. Ultimately, we would like to demonstrate that the total sum of the residuals equals zero. Technically, it’s <span class="math inline">\(10^{-14}\)</span>, which is sufficiently close to zero. Additionally, the sum of the residuals multiplied by the price variable `x`` must also be zero—albeit at <span class="math inline">\(10^{-15}\)</span>. Therefore, in numerical terms, both cases effectively amount to zero. These residuals represent the magnitudes of the deviations depicted by the red line in the accompanying plot.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="week-02.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamond<span class="sc">$</span>carat, diamond<span class="sc">$</span>price,  </span>
<span id="cb91-2"><a href="week-02.html#cb91-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Mass (carats)&quot;</span>, </span>
<span id="cb91-3"><a href="week-02.html#cb91-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Price (SIN $)&quot;</span>, </span>
<span id="cb91-4"><a href="week-02.html#cb91-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">bg =</span> <span class="st">&quot;lightblue&quot;</span>, </span>
<span id="cb91-5"><a href="week-02.html#cb91-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">21</span>,<span class="at">frame =</span> <span class="cn">FALSE</span>)</span>
<span id="cb91-6"><a href="week-02.html#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb91-7"><a href="week-02.html#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span> <span class="sc">:</span> n) </span>
<span id="cb91-8"><a href="week-02.html#cb91-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">c</span>(x[i], x[i]), <span class="fu">c</span>(y[i], yhat[i]), <span class="at">col =</span> <span class="st">&quot;red&quot;</span> , <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Notice all of the blank space in the graph, making the plot kind of useless for that purpose, why don’t we plot the residuals on the vertical axis versus mass on the horizontal axis?</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="week-02.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, e,  </span>
<span id="cb92-2"><a href="week-02.html#cb92-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Mass (carats)&quot;</span>, </span>
<span id="cb92-3"><a href="week-02.html#cb92-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals (SIN $)&quot;</span>, </span>
<span id="cb92-4"><a href="week-02.html#cb92-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">bg =</span> <span class="st">&quot;lightblue&quot;</span>, </span>
<span id="cb92-5"><a href="week-02.html#cb92-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">21</span>,<span class="at">frame =</span> <span class="cn">FALSE</span>)</span>
<span id="cb92-6"><a href="week-02.html#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb92-7"><a href="week-02.html#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span> <span class="sc">:</span> n) </span>
<span id="cb92-8"><a href="week-02.html#cb92-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">c</span>(x[i], x[i]), <span class="fu">c</span>(e[i], <span class="dv">0</span>), <span class="at">col =</span> <span class="st">&quot;red&quot;</span> , <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Now we can see the residual variation much more clearly. One important point is: the residuals should be mostly patternless. Also, remember that if you include an intercept, residuals have to sum to zero. We can see some interesting patterns by honing in on the residual plot here. For example, we can see that there were lots of diamonds of exactly the same mass which gets lost in the scatter plot. Next, we want to go through some pathological residual plots, just to highlight what residual plots can do for us.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="week-02.html#cb93-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>); y <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> .<span class="dv">2</span>); </span>
<span id="cb93-2"><a href="week-02.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb93-3"><a href="week-02.html#cb93-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb93-4"><a href="week-02.html#cb93-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb93-5"><a href="week-02.html#cb93-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb93-6"><a href="week-02.html#cb93-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb93-7"><a href="week-02.html#cb93-7" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Here X is just going to be uniform <span class="math inline">\([-3,3]\)</span>, <code>y</code> is equal to <code>x</code>, so it’s an identity line, but then we add another term that’s <code>sin(x)</code>. This looks like an identity line, but kind of oscillating around it a little bit with some normal noise on top of it. Before we move on to the residual plot, let us make a comment. This model is actually not the correct model for this data and this might happen in practice. This doesn’t mean that this model is unimportant, right? There is a linear trend and the model is accounting for it, it’s just not accounting for the secondary variation in the <code>sin</code> term. To emphasize just because you aren’t fitting the actually correct model, that doesn’t mean the model is itself useless, in regression, having the exact right model is not always the primary goal. You can get meaningful information about trends from incorrect models.</p>
<p>Let’s me plot the residuals’ versus the <code>x</code> variable.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="week-02.html#cb95-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">resid</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x))), </span>
<span id="cb95-2"><a href="week-02.html#cb95-2" aria-hidden="true" tabindex="-1"></a>           <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb95-3"><a href="week-02.html#cb95-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="dv">2</span>); </span>
<span id="cb95-4"><a href="week-02.html#cb95-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb95-5"><a href="week-02.html#cb95-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb95-6"><a href="week-02.html#cb95-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;X&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Residual&quot;</span>)</span>
<span id="cb95-7"><a href="week-02.html#cb95-7" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>You can see that the <code>sin</code> term is now extremely apparent. This is what the residual plot has done highlighting the model inadequacy. Another example is the following plot, where by appearances, the plot falls perfectly on a line.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="week-02.html#cb96-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">6</span>); y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,  <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> .<span class="dv">001</span> <span class="sc">*</span> x); </span>
<span id="cb96-2"><a href="week-02.html#cb96-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb96-3"><a href="week-02.html#cb96-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb96-4"><a href="week-02.html#cb96-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb96-5"><a href="week-02.html#cb96-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb96-6"><a href="week-02.html#cb96-6" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>But when you highlight the residuals, it looks quite different.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="week-02.html#cb98-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">resid</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x))), </span>
<span id="cb98-2"><a href="week-02.html#cb98-2" aria-hidden="true" tabindex="-1"></a>           <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb98-3"><a href="week-02.html#cb98-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="dv">2</span>); </span>
<span id="cb98-4"><a href="week-02.html#cb98-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb98-5"><a href="week-02.html#cb98-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)</span>
<span id="cb98-6"><a href="week-02.html#cb98-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;X&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Residual&quot;</span>)</span>
<span id="cb98-7"><a href="week-02.html#cb98-7" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Plotting the residuals shows the trend toward greater variability as you head along the <code>x</code> variable. That property,
where the variability increases with the x variables called heteroscedasticity. Heteroscedasticity is one of those things
that residual plots are quite good at diagnosing and you couldn’t see it.</p>
<p>Let’s run the residual plot for the diamond data.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="week-02.html#cb99-1" aria-hidden="true" tabindex="-1"></a>diamond<span class="sc">$</span>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(<span class="fu">lm</span>(price <span class="sc">~</span> carat, <span class="at">data =</span> diamond))</span>
<span id="cb99-2"><a href="week-02.html#cb99-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(diamond, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> e))</span>
<span id="cb99-3"><a href="week-02.html#cb99-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Mass (carats)&quot;</span>)</span>
<span id="cb99-4"><a href="week-02.html#cb99-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Residual price (SIN $)&quot;</span>)</span>
<span id="cb99-5"><a href="week-02.html#cb99-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb99-6"><a href="week-02.html#cb99-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">7</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)</span>
<span id="cb99-7"><a href="week-02.html#cb99-7" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha=</span><span class="fl">0.2</span>)</span>
<span id="cb99-8"><a href="week-02.html#cb99-8" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The x-label is Mass in carats, the y-label is Residual price and just to emphasize the residuals have the same units as the ys. There doesn’t appear to be a lot of pattern in the plot, meaning it’s a pretty good fit.</p>
<p>Let us illustrate something about variability in a diamond dataset that will help us set the stage for defining some new properties about our regression model fit. So we create two residual vectors. The first residual vector is the one where we just fit an intercept, so the residuals are just the deviations around the average price. The second is the variation around the regression line with carats as the explanatory variable and price as the outcome. Then we create a factor variable that labels the set of residuals. The first one is labeled as a bunch of intercept only model residuals and the second set is labeled as a bunch of intercept and slope residuals.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="week-02.html#cb100-1" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">resid</span>(<span class="fu">lm</span>(price <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> diamond)),</span>
<span id="cb100-2"><a href="week-02.html#cb100-2" aria-hidden="true" tabindex="-1"></a>      <span class="fu">resid</span>(<span class="fu">lm</span>(price <span class="sc">~</span> carat, <span class="at">data =</span> diamond)))</span>
<span id="cb100-3"><a href="week-02.html#cb100-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;Itc&quot;</span>, <span class="fu">nrow</span>(diamond)),</span>
<span id="cb100-4"><a href="week-02.html#cb100-4" aria-hidden="true" tabindex="-1"></a>               <span class="fu">rep</span>(<span class="st">&quot;Itc, slope&quot;</span>, <span class="fu">nrow</span>(diamond))))</span>
<span id="cb100-5"><a href="week-02.html#cb100-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">e =</span> e, <span class="at">fit =</span> fit), <span class="fu">aes</span>(<span class="at">y =</span> e, <span class="at">x =</span> fit, <span class="at">fill =</span> fit))</span>
<span id="cb100-6"><a href="week-02.html#cb100-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_dotplot</span>(<span class="at">binaxis =</span> <span class="st">&quot;y&quot;</span>, <span class="at">size =</span> <span class="dv">2</span>, <span class="at">stackdir =</span> <span class="st">&quot;center&quot;</span>, <span class="at">binwidth =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## Warning: Ignoring unknown parameters: size</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="week-02.html#cb102-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Fitting approach&quot;</span>)</span>
<span id="cb102-2"><a href="week-02.html#cb102-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Residual price&quot;</span>)</span>
<span id="cb102-3"><a href="week-02.html#cb102-3" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>What we see on the left-hand plot with just the intercept is the variation in diamond prices around the average diamond price. What we’re seeing in the rightmost plot is displaying the variation around the regression line. So we have explained a lot of the variation with the relationship with mass. We’re going to talk about <span class="math inline">\(R^2\)</span>, which basically says, we can decompose the total variation, the variation explained by the regression model and the variation that’s left over after accounting for the regression model.</p>
<p>Residual variation is the variation around the regression line (<span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span> where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>). The residuals are the vertical distances between the outcomes and the fitted regression line. If we include an intercept, the residuals have to sum to zero, which means their mean is zero.
The variance of the residuals, is the average squared residual (<span class="math inline">\(\sigma^2\)</span> is <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n e_i^2\)</span>).
Most people use <span class="math inline">\(\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2\)</span>, they <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n\)</span> so that <span class="math inline">\(E[\hat \sigma^2] = \sigma^2\)</span>.
The way to think about that is, we include the intercept the residuals have to sum to zero, that puts a constraint. If you know n minus one of them, then, you know the <span class="math inline">\(n^{th}\)</span> if you have a line term in there, if you have a co-variant in there, then, that puts a second constrain on the residuals. So, you lose two degrees of freedom. If you put another regression variable in there, you have another constraint, you lose three degrees of freedom. So in that sense you really don’t have n residuals, you have <span class="math inline">\(n-2\)</span> of them, because if you knew <span class="math inline">\(n-2\)</span> of them you could figure out the last two. And that’s why it’s one over <span class="math inline">\(n-2\)</span>.
You can grab the residual variation out of the <code>lm</code> fit and assign it to a variable.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="week-02.html#cb103-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> diamond<span class="sc">$</span>price; x <span class="ot">&lt;-</span> diamond<span class="sc">$</span>carat; n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb103-2"><a href="week-02.html#cb103-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb103-3"><a href="week-02.html#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 31.84052</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="week-02.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">resid</span>(fit)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n <span class="sc">-</span> <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 31.84052</code></pre>
<p>If you want to grab it as an object that you can assign to something, just put dollar sign sigma. Then you can assign sigma to any other variable. The line <code>sqrt(sum(resid(fit)^2) / (n - 2))</code> will result in the value and is showing what the <code>lm</code> function is doing behind the scenes.</p>
<p>Now let’s go back to the following plot where we look at the total variability in diamond prices, and compare what happens to the variability when we explain some of that variability with a regression line.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="week-02.html#cb107-1" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">resid</span>(<span class="fu">lm</span>(price <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> diamond)),</span>
<span id="cb107-2"><a href="week-02.html#cb107-2" aria-hidden="true" tabindex="-1"></a>      <span class="fu">resid</span>(<span class="fu">lm</span>(price <span class="sc">~</span> carat, <span class="at">data =</span> diamond)))</span>
<span id="cb107-3"><a href="week-02.html#cb107-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;Itc&quot;</span>, <span class="fu">nrow</span>(diamond)),</span>
<span id="cb107-4"><a href="week-02.html#cb107-4" aria-hidden="true" tabindex="-1"></a>               <span class="fu">rep</span>(<span class="st">&quot;Itc, slope&quot;</span>, <span class="fu">nrow</span>(diamond))))</span>
<span id="cb107-5"><a href="week-02.html#cb107-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">e =</span> e, <span class="at">fit =</span> fit), <span class="fu">aes</span>(<span class="at">y =</span> e, <span class="at">x =</span> fit, <span class="at">fill =</span> fit))</span>
<span id="cb107-6"><a href="week-02.html#cb107-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_dotplot</span>(<span class="at">binaxis =</span> <span class="st">&quot;y&quot;</span>, <span class="at">size =</span> <span class="dv">2</span>, <span class="at">stackdir =</span> <span class="st">&quot;center&quot;</span>, <span class="at">binwidth =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## Warning: Ignoring unknown parameters: size</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="week-02.html#cb109-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Fitting approach&quot;</span>)</span>
<span id="cb109-2"><a href="week-02.html#cb109-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Residual price&quot;</span>)</span>
<span id="cb109-3"><a href="week-02.html#cb109-3" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The total variability is just the deviations of the data, <span class="math inline">\(\sum_{i=1}^n (Y_i - \bar Y)^2\)</span> the average squared deviation of the data around its mean. To make things easy, let’s forget about the denominator and just talk about the sum of the squared deviations. We might call the regression variability as the component of that variability that then gets explained away by the regression line. We would take the points on the regression line, the heights, which is the variability in the response and explained by the regression line, <span class="math inline">\(\sum_{i=1}^n (\hat Y_i - \bar Y)^2\)</span>. The error variability is what’s leftover around the regression line <span class="math inline">\(\sum_{i=1}^n (Y_i - \hat Y_i)^2\)</span>.
The interesting identity is that the total variability disregarding everything except for where they’re centered at is equal to the regression variability, that is the variability
explained by the model plus the residual variability, the variability left over and not explained by the model.
<span class="math display">\[
\sum_{i=1}^n (Y_i - \bar Y)^2 
= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n  (\hat Y_i - \bar Y)^2 
\]</span>
Because the residual variation and the regression model variation add up to the total variation we can define a quantity that represents the percentage of the total variation that’s represented by the model. This is called the coefficient of determination, <span class="math inline">\(R^2\)</span>. R squared is the percentage of the total variability that is explained by the linear relationship with the predictor
<span class="math display">\[
R^2 = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
\]</span>
So R squared for our diamond example, is the percentage of the variation in diamond price, that is explained by the regression relationship with mass.</p>
<p><strong>Some facts about <span class="math inline">\(R^2\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(R^2\)</span> is the percentage of variation explained by the regression model.</li>
<li><span class="math inline">\(0 \leq R^2 \leq 1\)</span> (because the regression variability and the error variability and the sums of the squares add up to the total sums of squares, and they are all positive)</li>
<li><span class="math inline">\(R^2\)</span> is the sample correlation squared. (If we define R as the sample correlation between the predictor and the outcome, then R squared is literally that sample correlation R, squared.)</li>
<li><span class="math inline">\(R^2\)</span> can be a misleading summary of model fit. (For example, if you have somewhat noisy data and delete a lot of the points in the middle you can get a much higher R squared. Or if you just add arbitrary regression variables into a linear model fit, you increase R squared and decrease mean squared error)
<ul>
<li>Deleting data can inflate <span class="math inline">\(R^2\)</span>.</li>
<li>(For later.) Adding terms to a regression model always increases <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
</ul>
<p>Anscombe created a particularly stark example of a bunch of data sets with an equivalent R squared, equivalent mean, and variances in the x’s and the y’s, and identical regression relationships, but when you look at the scatter plots, you can see that the fit has very different meanings in each of the cases.</p>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-20-1.png" width="672" />
The first is a nice regression line, exactly sort of along the lines of what we think of, when we think of just a slightly noisy x,y relationship. In the second one clearly there’s a missing term in order to address some of the curvature in the data. In the third one, there’s an outlier. Finally, in the fourth one, all the data stacked up at one particular location and there’s one point way out at the end. So you could imagine getting this if you had the first example and you deleted a lot of the points in the middle. In all these cases you have an equivalent R squared. But the summary to the single number certainly has thrown out a lot of the important information that you get from a simple scatter plot.</p>
<div id="optional-reading-how-to-derive-r-squared" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Optional reading How to derive R squared:</h3>
<p><span class="math display">\[
\begin{align}
\sum_{i=1}^n (Y_i - \bar Y)^2 
&amp; = \sum_{i=1}^n (Y_i - \hat Y_i + \hat Y_i - \bar Y)^2 \\
&amp; = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 
2 \sum_{i=1}^n  (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + 
\sum_{i=1}^n  (\hat Y_i - \bar Y)^2 \\
\end{align}
\]</span></p>
<div id="scratch-work" class="section level4" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Scratch work</h4>
<p><span class="math inline">\((Y_i - \hat Y_i) = \{Y_i - (\bar Y - \hat \beta_1 \bar X) - \hat \beta_1 X_i\} = (Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X)\)</span></p>
<p><span class="math inline">\((\hat Y_i - \bar Y) = (\bar Y - \hat \beta_1 \bar X - \hat \beta_1 X_i - \bar Y ) = \hat \beta_1 (X_i - \bar X)\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) = \sum_{i=1}^n \{(Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X))\}\{\hat \beta_1 (X_i - \bar X)\}\)</span></p>
<p><span class="math inline">\(=\hat \beta_1 \sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X) -\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2\)</span></p>
<p><span class="math inline">\(= \hat \beta_1^2 \sum_{i=1}^n (X_i - \bar X)^2-\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2 = 0\)</span></p>
</div>
<div id="the-relation-between-r-squared-and-r" class="section level4" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> The relation between R squared and r</h4>
<p>Recall that <span class="math inline">\((\hat Y_i - \bar Y) = \hat \beta_1 (X_i - \bar X)\)</span>
so that
<span class="math display">\[
R^2 = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= \hat \beta_1^2  \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= Cor(Y, X)^2
\]</span>
Since, recall,
<span class="math display">\[
\hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)}
\]</span>
So, <span class="math inline">\(R^2\)</span> is literally <span class="math inline">\(r\)</span> squared.</p>
</div>
</div>
</div>
<div id="inference-in-regression" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Inference in regression</h2>
<p>Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference.</p>
<p>These statements apply generally, and, of course, to the regression setting that we’ve been studying. In the next few lectures, we’ll cover inference in regression where we make some Gaussian assumptions about the errors.</p>
<p>Before we begin talking about inference, let’s just revisit our model so that it’s fresh in our mind. <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span> where <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>. For the time being, we’re going to assume that the true model is known, and this will be the basis for most of this class. We also assume that you’ve seen confidence intervals and hypothesis tests before. If you feel the need, you should go back and review them. Also, remember <span class="math inline">\(\hat \beta_0 = \bar Y - \hat \beta_1 \bar X\)</span>, <span class="math inline">\(\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)}\)</span>.</p>
<p>We would like to review some of the basic concepts from statistical inference. Statistics like <span class="math inline">\(\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}}\)</span> often have the following properties.</p>
<ol style="list-style-type: decimal">
<li>Is normally distributed and has a finite sample Student’s T distribution if the variance is replaced with a sample estimate (under normality assumptions).</li>
<li>Can be used to test <span class="math inline">\(H_0 : \theta = \theta_0\)</span> versus <span class="math inline">\(H_a : \theta &gt;, &lt;, \neq \theta_0\)</span>.</li>
<li>Can be used to create a confidence interval for <span class="math inline">\(\theta\)</span> via <span class="math inline">\(\hat \theta \pm Q_{1-\alpha/2} \hat \sigma_{\hat \theta}\)</span> where <span class="math inline">\(Q_{1-\alpha/2}\)</span> is the relevant quantile from either a normal or T distribution. (For example, if our <span class="math inline">\(\alpha\)</span> is 5%, so we want a 95% confidence interval, we take the <span class="math inline">\(97.5^{th}\)</span> quantile.)</li>
</ol>
<p>In the case of regression with iid sampling assumptions and normal errors, our inferences will follow very similarily to what you saw in your inference class. We won’t cover asymptotics for regression analysis, but suffice it to say that under assumptions on the ways in which the <span class="math inline">\(X\)</span> values are collected, the iid sampling model, and mean model, the normal results hold to create intervals and confidence intervals. In other words, it’s not mandatory for the errors to be Gaussian for our statistical inferences in regression to hold. You can appeal to large sample theory, though it’s a little bit more complicated.</p>
<p>The variance of our regression slope is actually a highly informative formula. <span class="math display">\[\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) = \sigma^2 / \sum_{i=1}^n (X_i - \bar X)^2\]</span>
This is variance of <span class="math inline">\(\hat \beta_1\)</span>, showing how variable the points are around the true regression line, <span class="math inline">\(\sigma^2\)</span>, and how variable my X’s are. The numerator, how variable the points are around the regression line, is somewhat understandable as to why that would get better estimates of the regression slope if that were smaller. However, it’s maybe less intuitive to understand why we want more variance in our predictor in order to get lower variance in our regression slope. To understand it imagine a dataset where the regressors, the predictors, are all packed in very tightly, closely together, then it’s clear we’re not going to estimate a very good line. It could sort of bend around that cloud of points very easily and get equivalent fits.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="week-02.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a dataset with more random points around 2.5, 2.5</span></span>
<span id="cb110-2"><a href="week-02.html#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">102</span>)</span>
<span id="cb110-3"><a href="week-02.html#cb110-3" aria-hidden="true" tabindex="-1"></a>num_points <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb110-4"><a href="week-02.html#cb110-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>(num_points, <span class="at">min =</span> <span class="dv">2</span>, <span class="at">max =</span> <span class="dv">3</span>)</span>
<span id="cb110-5"><a href="week-02.html#cb110-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">runif</span>(num_points, <span class="at">min =</span> <span class="dv">2</span>, <span class="at">max =</span> <span class="dv">3</span>)</span>
<span id="cb110-6"><a href="week-02.html#cb110-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-7"><a href="week-02.html#cb110-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the dataset</span></span>
<span id="cb110-8"><a href="week-02.html#cb110-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="at">pch =</span> <span class="st">&quot;+&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex=</span><span class="fl">1.75</span>)</span>
<span id="cb110-9"><a href="week-02.html#cb110-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-10"><a href="week-02.html#cb110-10" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="fu">I</span>(X))</span>
<span id="cb110-11"><a href="week-02.html#cb110-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(model, <span class="at">col =</span> <span class="st">&#39;black&#39;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-12"><a href="week-02.html#cb110-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-13"><a href="week-02.html#cb110-13" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="fu">I</span>(<span class="sc">-</span>X<span class="sc">+</span><span class="dv">5</span>))</span>
<span id="cb110-14"><a href="week-02.html#cb110-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(model, <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>On the other hand, if we spread our axis out, we will get a better fitted regression line with lower variance for the slope. It turns out the lowest you can make that variance is to push half the observations to one end and the other half of the observations to another end; however, you’re banking on having a line in between those two because you haven’t collected any data to evaluate that property.</p>
<p>The variance of the intercept, which is maybe a little less informative because intercepts are often a little less of interest than the slopes.
<span class="math display">\[\sigma_{\hat \beta_0}^2 = Var(\hat \beta_0)  = \left(\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }\right)\sigma^2\]</span></p>
<p>In practice, <span class="math inline">\(\sigma\)</span> is replaced by its estimate. It’s probably not surprising that under iid Gaussian errors <span class="math inline">\(\frac{\hat \beta_j - \beta_j}{\hat \sigma_{\hat \beta_j}}\)</span> follows a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom and a normal distribution for large <span class="math inline">\(n\)</span>. This can be used to create confidence intervals and perform hypothesis tests.</p>
<p>In the following example we demonstrate the formulas we are giving are exactly the formulas that R is using when it performs its calculations.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="week-02.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR); <span class="fu">data</span>(diamond)</span>
<span id="cb111-2"><a href="week-02.html#cb111-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> diamond<span class="sc">$</span>price; x <span class="ot">&lt;-</span> diamond<span class="sc">$</span>carat; n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb111-3"><a href="week-02.html#cb111-3" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, x) <span class="sc">*</span> <span class="fu">sd</span>(y) <span class="sc">/</span> <span class="fu">sd</span>(x)</span>
<span id="cb111-4"><a href="week-02.html#cb111-4" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> beta1 <span class="sc">*</span> <span class="fu">mean</span>(x)</span>
<span id="cb111-5"><a href="week-02.html#cb111-5" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> y <span class="sc">-</span> beta0 <span class="sc">-</span> beta1 <span class="sc">*</span> x</span>
<span id="cb111-6"><a href="week-02.html#cb111-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(e<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n<span class="dv">-2</span>)) </span>
<span id="cb111-7"><a href="week-02.html#cb111-7" aria-hidden="true" tabindex="-1"></a>ssx <span class="ot">&lt;-</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb111-8"><a href="week-02.html#cb111-8" aria-hidden="true" tabindex="-1"></a>seBeta0 <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> n <span class="sc">+</span> <span class="fu">mean</span>(x) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> ssx) <span class="sc">^</span> .<span class="dv">5</span> <span class="sc">*</span> sigma </span>
<span id="cb111-9"><a href="week-02.html#cb111-9" aria-hidden="true" tabindex="-1"></a>seBeta1 <span class="ot">&lt;-</span> sigma <span class="sc">/</span> <span class="fu">sqrt</span>(ssx)</span>
<span id="cb111-10"><a href="week-02.html#cb111-10" aria-hidden="true" tabindex="-1"></a>tBeta0 <span class="ot">&lt;-</span> beta0 <span class="sc">/</span> seBeta0; tBeta1 <span class="ot">&lt;-</span> beta1 <span class="sc">/</span> seBeta1</span>
<span id="cb111-11"><a href="week-02.html#cb111-11" aria-hidden="true" tabindex="-1"></a>pBeta0 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">pt</span>(<span class="fu">abs</span>(tBeta0), <span class="at">df =</span> n <span class="sc">-</span> <span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb111-12"><a href="week-02.html#cb111-12" aria-hidden="true" tabindex="-1"></a>pBeta1 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">pt</span>(<span class="fu">abs</span>(tBeta1), <span class="at">df =</span> n <span class="sc">-</span> <span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb111-13"><a href="week-02.html#cb111-13" aria-hidden="true" tabindex="-1"></a>coefTable <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">c</span>(beta0, seBeta0, tBeta0, pBeta0), <span class="fu">c</span>(beta1, seBeta1, tBeta1, pBeta1))</span>
<span id="cb111-14"><a href="week-02.html#cb111-14" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(coefTable) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>, <span class="st">&quot;t value&quot;</span>, <span class="st">&quot;P(&gt;|t|)&quot;</span>)</span>
<span id="cb111-15"><a href="week-02.html#cb111-15" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(coefTable) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="st">&quot;x&quot;</span>)</span>
<span id="cb111-16"><a href="week-02.html#cb111-16" aria-hidden="true" tabindex="-1"></a>coefTable</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value      P(&gt;|t|)
## (Intercept) -259.6259   17.31886 -14.99094 2.523271e-19
## x           3721.0249   81.78588  45.49715 6.751260e-40</code></pre>
<p>We again use the diamond dataset in the <code>UsingR</code> library. Let’s define the variables y, x, n like before, and <span class="math inline">\(\beta_1 , \beta_0\)</span>. The residuals are response y minus the predicted values, <span class="math inline">\(beta_0 + \beta_1 * x\)</span>. We create the two t-statistics if you’re testing a hypothesis that <span class="math inline">\(\beta_0\)</span> is zero or <span class="math inline">\(\beta_1\)</span> is zero, that is the estimate. Here’s the estimate divided by its standard error. We don’t have to subtract off the true value, because the true value is assumed to be zero under this hypothesis. Next we calculate the two p values. If you’ve taken the inference class, then you know how to go from a t-statistic to a p value. In next step, we create the coefficient table created manually without having done any <code>lm</code> or any built in higher level R function. We specify the row names and column names.</p>
<p>However, there is an easy way to do the same thing in R.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="week-02.html#cb113-1" aria-hidden="true" tabindex="-1"></a>coefTable</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value      P(&gt;|t|)
## (Intercept) -259.6259   17.31886 -14.99094 2.523271e-19
## x           3721.0249   81.78588  45.49715 6.751260e-40</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="week-02.html#cb115-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x); </span>
<span id="cb115-2"><a href="week-02.html#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) -259.6259   17.31886 -14.99094 2.523271e-19
## x           3721.0249   81.78588  45.49715 6.751260e-40</code></pre>
<p>You’ll see everything is exactly the same.</p>
<p>Next we want to get a confidence interval for the intercept and the slope.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="week-02.html#cb117-1" aria-hidden="true" tabindex="-1"></a>sumCoef <span class="ot">&lt;-</span> <span class="fu">summary</span>(fit)<span class="sc">$</span>coefficients</span>
<span id="cb117-2"><a href="week-02.html#cb117-2" aria-hidden="true" tabindex="-1"></a>sumCoef[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> fit<span class="sc">$</span>df) <span class="sc">*</span> sumCoef[<span class="dv">1</span>, <span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1] -294.4870 -224.7649</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="week-02.html#cb119-1" aria-hidden="true" tabindex="-1"></a>(sumCoef[<span class="dv">2</span>,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> fit<span class="sc">$</span>df) <span class="sc">*</span> sumCoef[<span class="dv">2</span>, <span class="dv">2</span>]) <span class="sc">/</span> <span class="dv">10</span></span></code></pre></div>
<pre><code>## [1] 355.6398 388.5651</code></pre>
<p>Here we just need the table part of the summary, just the coefficient. With 95% confidence, we estimate that a 0.1 carat increase in diamond size results in a 355.6 to 388.6 increase in price in (Singapore) dollars, which is we estimate that a 0.1 carat increase in diamond size results in a 356 to 389 increase in price in Singapore dollars.</p>
<div id="prediction" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Prediction</h3>
<p>Prediction is a central concept for the data scientist. In fact, we have an entire course, Practical Machine Learning on advanced prediction techniques. However, regression and generalized linear models which we will cover later on in the course are some of the most core techniques for performing prediction, they often produce very good predictions,
they’re parsimonious and interpretable, and as an added bonus we can get inference on top of our predictions without doing any sort of data re sampling. By inference we mean you can get predictions, confidence intervals around the predictions to evaluate the uncertainty in those predictions, so that’s very easy in regression and pretty easy in generalized linear models and quite difficult in some more advanced machine learning algorithms, you may have to do data resampling or other techniques. We might want to predict a response, which might be the price of a diamond at a particular mass, in carats, or we might want to predict a child’s height for a particular value of the parent’s height. The obvious estimate in both cases is just take the <span class="math inline">\(X\)</span>, the predictor value multiply it by the relevant estimated slope, <span class="math inline">\(\hat \beta_1\)</span> and then add the intercept.</p>
<ul>
<li>The obvious estimate for prediction at point <span class="math inline">\(x_0\)</span> is <span class="math display">\[\hat \beta_0 + \hat \beta_1 x_0\]</span></li>
</ul>
<p>Being a good statisticians requires us to evaluate some uncertainty in the prediction, and it is nice to have a prediction interval. There’s a small intricacy between trying to predict a regression line at a particular point, and trying to predict a future <span class="math inline">\(Y\)</span> at that same point. Those are two different ideas.</p>
<ul>
<li>Line at <span class="math inline">\(x_0\)</span>, <span class="math inline">\(\hat \sigma\sqrt{\frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\)</span></li>
<li>Prediction interval at <span class="math inline">\(x_0\)</span>, <span class="math inline">\(\hat \sigma\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\)</span></li>
</ul>
<p>What we have here, and it makes sense that our prediction variance first relates around how variable the points are around our regression line, <span class="math inline">\(\hat \sigma\)</span> and we have the term <span class="math inline">\(\frac{1}{n}\)</span> that also kind of makes sense. Typically our standard errors decrease at some rate, <span class="math inline">\(\sqrt{\frac{1}{n}}\)</span>. If we’re predicting a new <span class="math inline">\(Y\)</span>, then we have the added 1 out front, so we get a wider interval. If we want to predict a new value at a specific point versus trying to predict what the regression line is at that point. We will talk more about that later, for now let’s focus on the very end term that on both equations: <span class="math inline">\(\frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}\)</span> consider the numerator of this statistic our prediction error is going to be the lowest when <span class="math inline">\(X\)</span> not is equal to <span class="math inline">\(\bar X\)</span> the prediction variance is smallest when we predict at the average mass of a diamond or at the average height of the parents. The denominator is basically how variable the Xs are. The more variable the Xs are, the smaller this term becomes and the lower the prediction error is. Like the slope estimate where the more variable the regressors were, the less variable the slope estimate was. The same thing happens in prediction error, and is an essential part of using regression for prediction, where we get easy and convenient prediction uncertainty associated with the parsimonious predictors.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="week-02.html#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb121-2"><a href="week-02.html#cb121-2" aria-hidden="true" tabindex="-1"></a>newx <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length =</span> <span class="dv">100</span>))</span>
<span id="cb121-3"><a href="week-02.html#cb121-3" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">predict</span>(fit, <span class="at">newdata=</span> newx,<span class="at">interval =</span> (<span class="st">&quot;confidence&quot;</span>)))</span>
<span id="cb121-4"><a href="week-02.html#cb121-4" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">predict</span>(fit, <span class="at">newdata =</span> newx,<span class="at">interval =</span> (<span class="st">&quot;prediction&quot;</span>)))</span>
<span id="cb121-5"><a href="week-02.html#cb121-5" aria-hidden="true" tabindex="-1"></a>p1<span class="sc">$</span>interval <span class="ot">=</span> <span class="st">&quot;confidence&quot;</span></span>
<span id="cb121-6"><a href="week-02.html#cb121-6" aria-hidden="true" tabindex="-1"></a>p2<span class="sc">$</span>interval <span class="ot">=</span> <span class="st">&quot;prediction&quot;</span></span>
<span id="cb121-7"><a href="week-02.html#cb121-7" aria-hidden="true" tabindex="-1"></a>p1<span class="sc">$</span>x <span class="ot">=</span> newx<span class="sc">$</span>x</span>
<span id="cb121-8"><a href="week-02.html#cb121-8" aria-hidden="true" tabindex="-1"></a>p2<span class="sc">$</span>x <span class="ot">=</span> newx<span class="sc">$</span>x</span>
<span id="cb121-9"><a href="week-02.html#cb121-9" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">rbind</span>(p1, p2)</span>
<span id="cb121-10"><a href="week-02.html#cb121-10" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(dat)[<span class="dv">1</span>] <span class="ot">=</span> <span class="st">&quot;y&quot;</span></span>
<span id="cb121-11"><a href="week-02.html#cb121-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-12"><a href="week-02.html#cb121-12" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb121-13"><a href="week-02.html#cb121-13" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lwr, <span class="at">ymax =</span> upr, <span class="at">fill =</span> interval), <span class="at">alpha =</span> <span class="fl">0.2</span>) </span>
<span id="cb121-14"><a href="week-02.html#cb121-14" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_line</span>()</span>
<span id="cb121-15"><a href="week-02.html#cb121-15" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> g <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y=</span>y), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">4</span>)</span>
<span id="cb121-16"><a href="week-02.html#cb121-16" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p><img src="resources/images/Week02_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>In <code>predict</code> function we provide the output of <code>lm</code>. For a lot of prediction algorithms, especially linear models and generalized linear models, random forests in R, the <code>predict</code> function is a generic method that applies to them, <code>interval = ("confidence")</code> indicates that we want the confidence interval, not a prediction interval, that’s R’s Code for
creating the interval around the estimated line at that particular value of x not for a potential new y at that particular value of x, if we want an interval for potential new Y at that particular value of X, we change the <code>interval = ("confidence")</code> to <code>interval = ("prediction")</code>, as we do on the fourth line.</p>
<p>The blue is the prediction interval, this is for predicting a new line, and the salmon color is for prediction of the line at those particular values of x. The confidence interval is much narrower than the prediction interval. It is because of that 1 plus for the prediction interval.</p>
<p>Imagine if we collected an infinite amount of data at all different values of x along this line. Well, then, we would pretty much know the regression line exactly, if that were the case, we would be extremely confident about predictions on the line, where the line was at a particular x value. As we collected more and more data, that salmon colored confidence interval will get narrower and narrower around the line to the point where it was just the line itself. That’s what we would expect to happen. That’s just the idea of statistical sampling working. On the other hand, the prediction interval, there’s variability in the Ys, that has nothing to do with how well we estimated <span class="math inline">\(\beta_0, \beta_1\)</span> and in fact, if I were given the correct <span class="math inline">\(\beta_0, \beta_1\)</span>. There would still be variability in the Ys, because of the error term. Consequently, if we wanted to predict a new y there would be some uncertainty that would be inherent in that prediction. That’s why the prediction interval is always going to be wider than the confidence interval. It doesn’t go away with N. It doesn’t go away as we collect more X’s or anything like that. It’s inherent, and that’s why the prediction interval has a certain amount of width that’s never going to go away.</p>
<p>The last thing as you may notice both of the intervals get narrower toward the center of the data cloud and then get wider as you head out into the tails. That’s just simply saying that we’re more confident in our predictions closer to the mean of the X’s. Because of that one plus term in prediction intervals this phenomena is less obvious in blue color than the salmon one. If we were to go well beyond where we collected data, then these intervals would really become a lot wider which is what we’d want, because we would be extrapolating and we want to predict where we did not collect data.</p>
<p><strong>Summary</strong></p>
<ul>
<li>Both intervals have varying widths.
<ul>
<li>Least width at the mean of the Xs.</li>
</ul></li>
<li>We are quite confident in the regression line, so that interval is very narrow.
<ul>
<li>If we knew <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> this interval would have zero width.</li>
</ul></li>
<li>The prediction interval must incorporate the variabilibity in the data around the line.
<ul>
<li>Even if we knew <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> this interval would still have width.</li>
</ul></li>
</ul>
</div>
</div>
<div id="for-the-project" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> For the project</h2>
<p>You need to know a little bit of knitr. In this video, which you may have to refer back to when you start the project, will get you started on knitr.</p>
<p>In this section we will learn how to use knitr to create reproducible reports. We will also learn how to use R Markdown to create reproducible documents. You’ll need a little bit of knitr to create your R project. We open the go <code>File&gt; New File&gt; R Markdown</code>, this will populate a simple knitr document. Here we can run R commands in a code block, which is defined as three right tick marks followed by <code>{r}</code> if you insert a comma after the r you will open up a bunch of options, <code>cache</code> tells R whether or not to keep it, <code>eval=</code> tells whether or not it should evaluate the code and <code>echo=</code> where echo TRUE shows the code and echo FALSE does not show the code. Once you are done with the document click Knit HTML
and it will knit and create an HTML document. That’s just a standard HTML document and you can bring up the document in a browser window. And that’s knitr in a nutshell.</p>
</div>
<div id="practical-r-exercises-in-swirl-1" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Practical R Exercises in swirl</h2>
<p>During this week of the course you should complete the following lessons in the Regression Models swirl course:</p>
<ol style="list-style-type: decimal">
<li>Residual Variation</li>
<li>Introduction to Multivariable Regression</li>
<li>MultiVar Examples</li>
</ol>
</div>
<div id="week-2-quiz" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Week 2 Quiz</h2>
<ol style="list-style-type: decimal">
<li>Consider the following data with x as the predictor and y as as the outcome. Give a P-value for the two sided hypothesis test of whether <span class="math inline">\(β_1\)</span> from a linear regression model is 0 or not.</li>
</ol>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="week-02.html#cb122-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.61</span>, <span class="fl">0.93</span>, <span class="fl">0.83</span>, <span class="fl">0.35</span>, <span class="fl">0.54</span>, <span class="fl">0.16</span>, <span class="fl">0.91</span>, <span class="fl">0.62</span>, <span class="fl">0.62</span>)</span>
<span id="cb122-2"><a href="week-02.html#cb122-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.67</span>, <span class="fl">0.84</span>, <span class="fl">0.6</span>, <span class="fl">0.18</span>, <span class="fl">0.85</span>, <span class="fl">0.47</span>, <span class="fl">1.1</span>, <span class="fl">0.65</span>, <span class="fl">0.36</span>)</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li><p>Consider the previous problem, give the estimate of the residual standard deviation.</p></li>
<li><p>In the <code>mtcars</code> data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint?</p></li>
<li><p>Refer to the previous question. Read the help file for <code>mtcars</code>. What is the weight coefficient interpreted as?</p></li>
<li><p>Consider again the <code>mtcars</code> data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint?</p></li>
<li><p>Consider again the <code>mtcars</code> data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint.</p></li>
<li><p>If my X from a linear regression is measured in centimeters and I convert it to meters what would happen to the slope coefficient?</p></li>
<li><p>I have an outcome, Y and a predictor, X and fit a linear regression model with <span class="math inline">\(Y = β_0 + β_1 * X + ϵ\)</span> to obtain <span class="math inline">\(\hat β_0, \hat β_1\)</span> . What would be the consequence to the subsequent slope and intercept if I were to refit the model with a new regressor, <span class="math inline">\(X + c\)</span> for some constant <span class="math inline">\(c\)</span>?</p></li>
<li><p>Refer back to the mtcars data set with mpg as an outcome and weight (wt) as the predictor. About what is the ratio of the the sum of the squared errors, <span class="math inline">\(\sum_{i=1}^n (Y_i - \hat Y_i)^2\)</span> when comparing a model with just an intercept (denominator) to the model with the intercept and slope (numerator)?</p></li>
<li><p>Do the residuals always have to sum to 0 in linear regression?</p></li>
</ol>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="week-01.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-03.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
