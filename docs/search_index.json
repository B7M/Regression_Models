[["index.html", "Course Name About this Course 0.1 Available course formats", " Course Name February, 2024 About this Course 0.1 Available course formats This course is available in multiple formats which allows you to take it in the way that best suites your needs. You can take it for certificate which can be for free or fee. The material for this course can be viewed without login requirement on this Bookdown website. This format might be most appropriate for you if you rely on screen-reader technology. This course can be taken for free certification through Leanpub. This course can be taken on Coursera for certification here (but it is not available for free on Coursera). Our courses are open source, you can find the source material for this course on GitHub. ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) ## also installing the dependencies &#39;jpeg&#39;, &#39;checkmate&#39;, &#39;Formula&#39;, &#39;latticeExtra&#39;, &#39;gridExtra&#39;, &#39;htmlTable&#39;, &#39;viridis&#39;, &#39;HistData&#39;, &#39;Hmisc&#39; ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) ## also installing the dependency &#39;plyr&#39; ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) ## Loading required package: MASS ## Loading required package: HistData ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## ## Attaching package: &#39;UsingR&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cancer ## Warning: package &#39;reshape&#39; was built under R version 4.0.3 "],["week-01.html", "Chapter 1 Week 01 1.1 Introduction 1.2 Introduction to regression and least squares 1.3 Linear least squares 1.4 Regression to the Mean 1.5 Practical R Exercises in swirl 1.6 Week 1 Quiz", " Chapter 1 Week 01 1.1 Introduction 1.1.1 Welcome to Regression Models I am happy that you’ve chosen to take Regression Models, part of the Johns Hopkins Data Science Specialization on Coursera! This course presents the fundamentals of regression modeling that you will need for the rest of the specialization and ultimately for your work in the field of data science. We believe that the key word in Data Science is “science”. Our course track is focused on providing you with three things: (1) an introduction to the key ideas behind working with data in a scientific way that will produce new and reproducible insight, (2) an introduction to the tools that will allow you to execute on a data analytic strategy, from raw data in a database to a completed report with interactive graphics, and (3) on giving you plenty of hands on practice so you can learn the techniques for yourself. Regression Models represents a both fundamental and foundational component of the series, and it presents the single most practical data analysis toolset. Using only a bare minimum of mathematics, we will attempt to provide you with the fundamentals for the application and practice of regression. We are excited about the opportunity to attempt to scale Data Science education. We intend for the courses to be self-contained, fast-paced, and interactive, and we intend to run them frequently to give people with busy schedules the opportunity to work on material at their own pace. 1.1.2 Some Basics A couple of first week housekeeping items. First, make sure that you’ve had R Programming , the Data Scientist’s Toolbox, Reproducible Research and Statistical Inference before taking this class. At a minimum you must know: very basic git, basic R and most of the Statistical Inference Coursera class. The small amount of knitr that you need for the project you can pick up quickly. An important aspect of this class is to peruse the materials in the github repository. All of the most up to date material can be found here. You should clone this repository as your first step in this class and make sure to fetch updates periodically. (Please issue pull requests so that we may improve the materials!) It is one of the most essential components of the Specialization that you start to use Git frequently. We’re practicing what we preach as well by using the tools in the series to create the series, especially git. Note my GitHub repo will generally be more up to date than the Data Science Specialization Repo. The lectures are in the index.Rmd lecture files. In Developing Data Products, we cover how to create these sorts of slides. However, for the time being, you should be able to open them in R Studio and look at their contents. You will see all of the R code to recreate the lectures. Going through the R code is the best way to familiarize yourself with the lecture materials. 1.1.2.1 YouTube If you’d prefer to watch the videos on YouTube, you can find them here and here. If you’d like to keep up with the instructors I’m (bcaffo?) on twitter, Roger is (rdpeng?) and Jeff is (jtleek?). The Department of Biostat here is (jhubiostat?). 1.1.3 Syllabus (xxx) Course Title: Regression Models Course Instructor(s):The primary instructor of this class is Brian Caffo. Brian is a professor at Johns Hopkins Biostatistics and co-directs the SMART working group. This class is co-taught by Roger Peng and Jeff Leek. In addition, Sean Kross and Nick Carchedi have been helping greatly. 1.1.3.1 Course Description: Linear models, as their name implies, relates an outcome to a set of predictors of interest using linear assumptions. Regression models, a subset of linear models, are the most important statistical analysis tool in a data scientist’s toolkit. This course covers regression analysis, least squares and inference using regression models. Special cases of the regression model, ANOVA and ANCOVA will be covered as well. Analysis of residuals and variability will be investigated. The course will cover modern thinking on model selection and novel uses of regression models including scatterplot smoothing. 1.1.3.2 Course Content This class has three main components: Least squares and linear regression Multivariable regression Generalized linear models The full list of topics are as follows: Module 1, least squares and linear regression 01_01 Introduction 01_02 Notation 01_03 Ordinary least squares 01_04 Regression to the mean 01_05 Linear regression 01_06 Residuals 01_07 Regression inference Module 2, Multivariable regression 02_01 Multivariate regression 02_02 Multivariate examples 02_03 Adjustment 02_04 Residual variation and diagnostics 02_05 Multiple variables Module 3, Generalized linear models 03_01 GLMs 03_02 Binary outcomes 03_03 Count outcomes 03_04 Olio Module 4, Logistic Regression and Poisson Regression 04_01 Logistic Regression 04_02Poisson Regression 04_03 Hodgepodge 1.1.3.3 Book: Regression Models for Data Science in R. A companion book is available here. The book is published via leanpub, and the suggested price is $14.99. You can get it for free or pay what you feel it is worth. 1.1.3.4 Quizzes There are four weekly quizzes. You must earn a grade of at least 80% to pass a quiz. You may attempt each quiz up to 3 times in 8 hours. The score from your most successful attempt will count toward your final grade. 1.1.3.5 Course Project The Course Project is an opportunity to demonstrate the skills you have learned during the course. It is graded through peer assessment. You must earn a grade of at least 80% to pass the peer assessment. 1.1.3.6 Grading Policy You must score at least 80% on all assignments (Quizzes &amp; Project) to pass the course. Your final grade will be calculated as follows: Quiz 1 = 15% Quiz 2 = 15% Quiz 3 = 15% Quiz 4 = 15% Course Project = 40% 1.1.3.6.1 swirl Programming Assignment (optional) In this course, you have the option to use the swirl R package to practice some of the concepts we cover in lectures. While these lessons will give you valuable practice and you are encouraged to complete as many as possible, please note that they are completely optional and you can get full marks in the class without completing them. 1.1.3.7 Differences of opinion Keep in mind that currently data analysis is as much art as it is science - so we may have a difference of opinion - and that is ok! Please refrain from angry, sarcastic, or abusive comments on the message boards. Our goal is to create a supportive community that helps the learning of all students, from the most advanced to those who are just seeing this material for the first time. 1.1.4 Data Science Specialization Community Site Since the beginning of the Data Science Specialization, we’ve noticed the unbelievable passion students have about our courses and the generosity they show toward each other on the course forums. A couple students have created quality content around the subjects we discuss, and many of these materials are so good we feel that they should be shared with all of our students. We’re excited to announce that we’ve created a site using GitHub Pages to serve as a directory for content that the community has created. If you’ve created materials relating to any of the courses in the Data Science Specialization, please send us a pull request so we can add a link to your content on our site. You can find out more about contributing here. We can’t wait to see what you’ve created and where the community can take this site! 1.1.5 Where to get more advanced material If you want more advanced material, I’ve been working on another version of this class. Eventually I hope to have a second Coursera class as well. Currently, you can get the E-Book in progress here (it’s variable pricing including free!) In addition, you can watch the videos as they’re being developed here. 1.2 Introduction to regression and least squares Regression models are the workhorse of data science. They are the most well described, practical and theoretically understood models in statistics. A data scientist well versed in regression models will be able to solve an incredible array of problems. Perhaps the key insight for regression models is that they produce highly interpretable model fits. This is unlike machine learning algorithms, which often sacrifice interpretability for improved prediction performance or automation. These are, of course, valuable attributes in their own rights. However, the benefit of simplicity, parsimony and intrepretability offered by regression models (and their close generalizations) should make them a first tool of choice for any practical problem. 1.2.1 Introduction to Regression Hello, I’m Brian Caffo, and I’d like to welcome you to the introduction to regression lecture in the regression Coursera class, part of our data science specialization. Co-taught by my colleagues Jeff Leek and Roger Peng, we all belong to the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. Regression is a cornerstone for data scientists. Before delving into complex machine learning, linear regression or its generalization, linear models, are often the go-to procedures. The roots of regression trace back to Francis Galton, who coined the term and concept, along with correlation, closely tied to linear regression. Galton’s prediction of a child’s height from a parent’s height remains historically significant. Jeff Leek highlights its continued relevance in modern genetic analysis, comparing it to Victorian Era measurements. Moving to a more contemporary example, a blog post by Rafael Irazarry on Simply Statistics explores the relationship between Kobe Bryant’s ball-hogging and the Lakers’ performance, utilizing linear regression. In a modern example, Simply Statistics blog talks about “the Lakers wins” that Data supports claim that if Kobe stops ball hogging the Lakers will win more.The heart of our class is understanding how to formulate and interpret statements like for example in the Simply Statistics blog post “Linear regression suggests an increase of 1% in the percent of shots taken by Kobe results in a drop of 1.16 points.” We’ll delve into good statistical practices, including providing standard errors. We might want to find a parsimonious and easily described mean relationships between the parent’s and child’s height. So we don’t want anything complicated. We want the simplest possible relationship, and that is what regression is best at. While machine learning and other techniques generate highly elaborate, in many cases, accurate prediction models, they tend to not be parsimonious. They tend not to explain the data, and they tend not to generate new parsimonious knowledge, whereas this is what regression is good at. This is what regression is in fact best at. We can talk about variation that’s unexplained by the regression model. The so called residual variation. We’re going to connect the results back to the subject of inference. How do we take our data, which is just a sample, it only talks about that data set, and try to figure out what assumptions are needed to extrapolate it to a larger population. This is a deep subject called statistical inference. We have a whole another course of Statistical Inference as part of data science specialization. But we’re going to apply the tools of inference, which we are hoping most of you will have had as a prerequisite. We’re going to apply the tools of inference to this new subject of regression. Let’s look at Francis Galton’s data, he first used this data in 1885. He’s really an interesting character in history, in general and definitely in the history of statistics. You need to run install.packages(\"UsingR\"). Here UsingR is the package for the book, Using R for Introductory Statistics. It is a great book, and they’ve very kindly packaged all these data sets together in a single R package. So you need to use UsingR then the library UsingR to get a lot of the data sets that we are going to talk about. So let’s first look at the marginal distribution of the parents. In other words, distribution of the parents disregarding children. And the marginal distribution of the children, disregarding parents. install.packages(&quot;UsingR&quot;) Parent distribution is all heterosexual couples, correcting for sex by multiplying the female heights by 1.08. library(UsingR); data(galton); library(reshape); long&lt;-melt(galton); ## Using as id variables g&lt;- ggplot(long, aes(x=value, fill=variable)) g&lt;- g+ geom_histogram(color=&#39;black&#39;, binwidth=1) g&lt;- g+ facet_grid(.~variable) g On the left, we have the children’s heights. The X-axis is in inches, the scale goes from 60 inches to 75. The Y-axis is the count, the number of children that fall in each bin of heights. On the right in the more bluish teal color, we have the parents heights. We’ve broken the association by the children and the parents by not doing a scatter plot, and only looking at the marginal distribution of the children, and the marginal distribution of the parents by themselves. We would like to use these distributions to introduce least squares, and then we’ll build on the bivaried association after that. So consider only the child’s height,forget for the moment about using the parent’s height to predict the child’s heights. We just want to find maybe the best prediction of the child’s heights without any other information. Well, probably the best predictor would be the middle and how could one define the middle? One definition, let \\(y_i\\), be the height for child \\(i\\), where in this dataset \\(i=1,2,...,n=928\\). So the middle is the value of\\(\\mu\\) that minimizes \\[\\sum_{i=1}^n(y_i-\\mu)^2\\] That’s how we define the middle. It’s also related to physics in this so called physical center of mass of the histogram that we showed on the previously. Imagine of those bars as being physical entities, having weight and you are trying to figure out where you would put your finger to balance it out. That would be the physical center of mass. You might have guessed that the center of the data has to be the mean. Let’s use our studio’s manipulate function to experiment with trying to find that center of mass. library(manipulate) myHist&lt;-function(mu){ mse&lt;-mean((galton$child - mu)^2) g &lt;- ggplot(galton, aes(x = child)) + geom_histogram(fill = &quot;salmon&quot;, colour = &quot;black&quot;, binwidth=1) g &lt;- g + geom_vline(xintercept = mu, size = 3) g &lt;- g + ggtitle(paste(&quot;mu = &quot;, mu, &quot;, MSE = &quot;, round(mse, 2), sep = &quot;&quot;)) g } manipulate(myHist(mu),mu=slider(62,74,step=0.5)) fig xxx Because we’re using manipulate we can move the slider around and monitor the value of \\(\\mu\\) and the mean squared error, that is the sum of the squared distances between the observed data points and that particular value of \\(\\mu\\). If you move the slider around, you would notice notice as we get toward the center of the histogram, the mean squared error is going down and if you keep moving the slider way up, it get’s up large again. You can see \\(\\mu\\) is the point that balanced out this histogram. Notice For those that are interested, we cover some simple proofs of some of the statements made. If this isn’t your thing, just skip these sections. However, if you’re interested, get a pencil and paper to work along! \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\mu)^2 &amp; = \\ \\sum_{i=1}^n (Y_i - \\bar Y + \\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 \\sum_{i=1}^n (Y_i - \\bar Y) (\\bar Y - \\mu) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 (\\bar Y - \\mu) \\sum_{i=1}^n (Y_i - \\bar Y) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 (\\bar Y - \\mu) (\\sum_{i=1}^n Y_i - n \\bar Y) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\sum_{i=1}^n (\\bar Y - \\mu)^2\\\\ &amp; \\geq \\sum_{i=1}^n (Y_i - \\bar Y)^2 \\ \\end{align} \\] The equations above show for any value of \\(\\mu\\), the function \\(\\sum_{i=1}^n (Y_i - \\mu)^2\\) is larger than or equal to the specific case when we plug in \\(\\bar Y\\). Therefore, \\(\\bar Y\\) has to be the unique minimizer of that equation. At this stage, we haven’t utilized the parent’s heights in our analysis. The initial step in examining this type of data is to construct a scatter plot of child heights against parent heights. Here we employ ggplot, but the plot has several shortcomings. ggplot(galton, aes(x = parent, y = child)) + geom_point() Notably, there’s over-plotting due to numerous parent-child pairs sharing the same x, y values. To address this, we provide an improved plot where the point size reflects the number of parent-child combinations at a specific x, y location. Additionally, color indicates frequency, with lighter colors representing higher frequencies. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:Hmisc&#39;: ## ## src, summarize ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union freqData &lt;- as.data.frame(table(galton$child, galton$parent)) names(freqData) &lt;- c(&quot;child&quot;, &quot;parent&quot;, &quot;freq&quot;) freqData$child &lt;- as.numeric(as.character(freqData$child)) freqData$parent &lt;- as.numeric(as.character(freqData$parent)) g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child)) g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; ) g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+20, show_guide = FALSE)) ## Warning: Ignoring unknown aesthetics: show_guide g &lt;- g + geom_point(aes(colour=freq, size = freq)) g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;) g In order to find the best line, all we have to find is the slope. Well, here’s how we could potentially do that. We would want to find the slope beta that minimizes the sum of the squared distances between the observed data points the \\(Y_i\\) and the fitted data points on the line, \\(\\beta X_i\\). We’ll square that distance and add them up and this is directly analogous to finding the least squares mean. This is sort of using the origin as a pivot point and picking the line that minimizes the sum of the squared vertical distances between the points and the line. Notice that there is a point in regression to the origin is useful for explaining things, because we only have one parameter, the slope and we don’t have two parameters, the slope and the intercept. But it’s generally bad practice to force regression lines through the point (0, 0). So, an easy way around this is to subtract the mean from the parent’s heights and the mean from the child’s heights, so that the zero, zero point is right in the middle of the data and that will make this solution a little bit more palatable. y &lt;- galton$child - mean(galton$child) x &lt;- galton$parent - mean(galton$parent) freqData &lt;- as.data.frame(table(x, y)) names(freqData) &lt;- c(&quot;child&quot;, &quot;parent&quot;, &quot;freq&quot;) freqData$child &lt;- as.numeric(as.character(freqData$child)) freqData$parent &lt;- as.numeric(as.character(freqData$parent)) myPlot &lt;- function(beta){ g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child)) g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; ) g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+20, show_guide = FALSE)) g &lt;- g + geom_point(aes(colour=freq, size = freq)) g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;) g &lt;- g + geom_abline(intercept = 0, slope = beta, size = 3) mse &lt;- mean( (y - beta * x) ^2 ) g &lt;- g + ggtitle(paste(&quot;beta = &quot;, beta, &quot;mse = &quot;, round(mse, 3))) g } myPlot(0.5) ## Warning: Ignoring unknown aesthetics: show_guide We can find the slope of the line very quickly in R using the lm function. The lm function stands for linear model. We’re going to regress the child’s height on the parent’s height. We’re going to subtract the mean from the child’s height and the mean from the parent’s height, to make sure line is going through the origin. Doing so will give us a line that has slope of 0.646. lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton) ## ## Call: ## lm(formula = I(child - mean(child)) ~ I(parent - mean(parent)) - ## 1, data = galton) ## ## Coefficients: ## I(parent - mean(parent)) ## 0.6463 Now what we’re going to do in subsequent sections is to talk about how we get these values? What is the motivation behind it and all the things we can do with this fitted line, we’re going to spend maybe the next several sections talking about this. You have actually learned a lot of material in this very first part, well done! 1.3 Linear least squares 1.3.1 Notations and background Ordinary least squares (OLS) is the workhorse of statistics. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line through some data. In the next few sections, we cover the basics of linear least squares. We start with defining our notation. These are things you probably already saw in the prerequisite for this course in a Statistical Inference course in Data Science Specialization. However, because they’re so fundamental to regression, we’re going to cover them again, so they’re fresh in our minds. We will try to minimize the amount of mathematics that’s required for this class. Throughout the course we will neither require calculus nor linear algebra. And when it does get a little bit more mathematical, we will let you know when you can skip over those sections. We might write \\(X_1,X_2,...,Xn\\) to describe \\(n\\) data points. As an example, consider the data set \\({1, 2, 5}\\), where \\(X_1=1\\), \\(X_2=2\\), \\(X_3=5\\) and \\(n\\) in this case is 3. There’s nothing in particular about the letter \\(X\\). We could have just as easily described \\(Y_1\\) to \\(Y_n\\). The last bit of notation that’s important, is we’re typically going to use Greek letters for things we don’t know, such as \\(\\mu\\) for a population mean and we’ll use non Greek letters or regular letters to denote things that we can observe. So, \\(\\bar X\\) is something we can observe. \\(\\mu\\) is something we can’t observe and would like to estimate. We can define the empirical mean as \\[ \\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i. \\] Notice if we subtract the mean from data points, we get data that has mean 0. That is, if we define \\(\\tilde X_i = X_i - \\bar X.\\) The mean of the \\(\\tilde X_i\\) is 0. This process is called “centering” the random variables. Recall from the previous section that the mean is the least squares solution for minimizing \\(\\sum_{i=1}^n (X_i - \\mu)^2\\). Since we talked about means, let’s talk about variances. The variances is usually denoted by \\(S^2\\). It’s defined as \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^n X_i^2 - n \\bar X ^ 2 \\right) \\] This is nothing other than basically the average squared deviation of the observations around the mean. The empirical standard deviation is defined as \\(S = \\sqrt{S^2}\\). Notice that the standard deviation has the same units as the data. It’s nice to work with standard deviations because the variance is expressed in whatever units \\(X\\) has squared, whereas the standard deviation is just expressed in the normal units of \\(X\\). Another interesting fact related to standard deviation is scaling, so if we subtract a mean off from every observation, we get a resulting data set that has mean 0. If we divide every observation by the standard deviation, the resulting data set will have standard deviation 1. This is called scaling the data. If we take our original data now and subtract off \\(\\bar X\\), then take the resulting centered data and scale it by \\(S\\). We get a new data set, let’s call them \\(Z_i\\). \\[ Z_i = \\frac{X_i - \\bar X}{s} \\] This process of centering and then scaling is called normalizing the data. As an example, if something has a value 2 from normalized data, that means that the data point was 2 standard deviations larger than the mean. As its name would suggest, normalization is an attempt to make non-comparable data sets comparable. The empirical covariance is the most central quantity in regression. Imagine we have two vectors, \\(X\\) and \\(Y\\), and they’re lined up. So \\(X_i\\) might be the BMI and \\(Y_i\\) might be the blood pressure for subject \\(i\\). You could meaningfully do a scatter plot. Then we just define the covariance between X and Y as: \\[ Cov(X, Y) = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X) (Y_i - \\bar Y) = \\frac{1}{n-1}\\left( \\sum_{i=1}^n X_i Y_i - n \\bar X \\bar Y\\right) \\] The correlation is defined as: \\[ Cor(X, Y) = \\frac{Cov(X, Y)}{S_x S_y} \\] where \\(S_x\\) and \\(S_y\\) are the estimates of standard deviations for the \\(X\\) observations and \\(Y\\) observations, respectively. In other words, the correlation is simply the covariance then standardized into a unitless quantity. So, the correlation is the covariance of \\(X\\) and \\(Y\\), which has units, basically units of X times units of Y. Some facts about correlation: * \\(Cor(X, Y) = Cor(Y, X)\\) * \\(-1 \\leq Cor(X, Y) \\leq 1\\) * \\(Cor(X,Y) = 1\\) and \\(Cor(X, Y) = -1\\) only when the \\(X\\) or \\(Y\\) observations fall perfectly on a positive or negative sloped line, respectively. * \\(Cor(X, Y)\\) measures the strength of the linear relationship between the \\(X\\) and \\(Y\\) data, with stronger relationships as \\(Cor(X,Y)\\) heads towards -1 or 1. * \\(Cor(X, Y) = 0\\) implies no linear relationship. 1.3.2 Linear Least Squares Consider again, when we’re looking at the scatter plot of the parent’s heights by the child’s heights from the Galton data, the size of the circle represents the frequency of that particular x, y combination. ## Warning: Ignoring unknown aesthetics: show_guide We’d like to use the parent’s heights to explain the child’s heights and we’re going to do it using linear regression. We’re going to use our notation that we developed in our last section. So let’s let \\(Y\\) be the \\(i^{th}\\) child’s height and \\(X_i\\) be the \\(i^{th}\\) parents’ height. Now we want to find the best line, where we want the line to look like child’s height is an intercept. Child’s Height = \\(\\beta_0\\) + Parent’s Height \\(\\beta_1\\), \\(\\beta_0\\) and \\(\\beta_!\\) are parameters we would like to know that we don’t know. Well, we need a criteria for the term best. We need to figure out what we mean by the best line that fits the data. Well, one criteria is the famous least squares criteria. And the basic gist of the equation is we want to minimize the sum of the squared vertical distances between the data points, the height of the data points, the child’s heights and the points on the line, on the fitted line. And we can write this as \\[ \\sum_{i=1}^n \\{Y_i - (\\beta_0 + \\beta_1 X_i)\\}^2 \\] This is the sum of the squared vertical distances between the data points and the fitted line. We want to minimize this quantity. We want to find the \\(\\beta_0\\) and \\(\\beta_1\\) that minimize this quantity. This is called the least squares criteria. We put little hats over \\(\\beta_o\\) and \\(\\beta_1\\) to indicate the estimated values. The least squares model fit to the line \\(Y = \\beta_0 + \\beta_1 X\\) through the data pairs \\((X_i, Y_i)\\) with \\(Y_i\\) as the outcome obtains the line \\(Y = \\hat \\beta_0 + \\hat \\beta_1 X\\) where \\[\\hat \\beta_1 = Cor(Y, X) \\frac{Sd(Y)}{Sd(X)} ~~~ \\hat \\beta_0 = \\bar Y - \\hat \\beta_1 \\bar X\\] The solution works out to be \\(\\hat \\beta_1\\) is the correlation between \\(Y\\) and \\(X\\) times the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\). The estimated intercept \\(\\hat \\beta_0 = \\bar Y \\beta_1 hat * \\bar X\\). So let’s go through a couple of consequences of this being the result. \\(\\hat \\beta_1\\) has the units of \\(Y / X\\), \\(\\hat \\beta_0\\) has the units of \\(Y\\). We can see this because the correlation is a unitless quantity. The line passes through the point \\((\\bar X, \\bar Y\\)) The slope of the regression line with \\(X\\) as the outcome and \\(Y\\) as the predictor is \\(Cor(Y, X) Sd(X)/ Sd(Y)\\). The slope is the same one you would get if you centered the data, \\((X_i - \\bar X, Y_i - \\bar Y)\\), and did regression through the origin. If you normalized the data, \\(\\{ \\frac{X_i - \\bar X}{Sd(X)}, \\frac{Y_i - \\bar Y}{Sd(Y)}\\}\\), the slope is \\(Cor(Y, X)\\). 1.3.3 Linear Least Squares Coding Example Here we will go through a coding example to show how to calculate the least squares estimates. We plot the Galton parents’ height and childrens’ height data that we are going to look at. library(dplyr) freqData &lt;- as.data.frame(table(galton$child, galton$parent)) names(freqData) &lt;- c(&quot;child&quot;, &quot;parent&quot;, &quot;freq&quot;) freqData$child &lt;- as.numeric(as.character(freqData$child)) freqData$parent &lt;- as.numeric(as.character(freqData$parent)) g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child)) g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; ) g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+20, show_guide = FALSE)) ## Warning: Ignoring unknown aesthetics: show_guide g &lt;- g + geom_point(aes(colour=freq, size = freq)) g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;) g Now we indicate that the solution that we specified is the same solution that R will give you with its built in regression function. The function lm in R stands for linear model. Regression is a component of linear models, and so, this function is the general function whether you want regression or you want some of the more elaborate versions of regression that we’re going to cover later on. So we want lm, the outcome \\(\\tilde Y\\), the predictor \\(X\\). coef takes the output of the linear model and just grabs the coefficients. beta1 &lt;- cor(y, x) * sd(x) / sd(y) beta0 &lt;- mean(x) - beta1 * mean(y) rbind(c(beta0, beta1), coef(lm(x ~ y))) ## (Intercept) y ## [1,] 8.207028e-16 0.3256475 ## [2,] 1.258492e-15 0.3256475 As we expected you see we get the same numbers, 23.94 and 0.64, 0.65. Very briefly now, we just want to mention that if we reverse the \\(Y\\) and \\(X\\) relationship the formula, of course holds but now with standard deviation of \\(X\\) in the numerator and standard deviation of \\(Y\\) in the denominator. beta1 &lt;- cor(y, x) * sd(x) / sd(y) beta0 &lt;- mean(x) - beta1 * mean(y) rbind(c(beta0, beta1), coef(lm(x ~ y))) ## (Intercept) y ## [1,] 8.207028e-16 0.3256475 ## [2,] 1.258492e-15 0.3256475 If we concatenate these slope and intercept estimates with those that you get with lm where \\(X\\) is on the left hand side of the ~ and \\(Y\\) is on the right hand side of ~, reversed from what it was previously. So our formula is correct and we know how to use it and we know what happens when we reverse the \\(X,Y\\) relationship. Another point that was made thus far in the course was that regression through the origin yielded the same slope as linear regression with a not necessarily zero intercept. If you mean centered the \\(Y\\)’s and mean centered the \\(X\\)’s first. So let’s just check that computationally. Recall that the regression to the origin equation for the slope was just the sum of the \\(Y\\) variable times the \\(X\\) variable, divided by the sum of the \\(X\\) variable squared. So, let’s run that and get our coefficient that is estimated through a regression to the origin. yc &lt;- y - mean(y) xc &lt;- x - mean(x) beta1 &lt;- sum(yc * xc) / sum(xc ^ 2) c(beta1, coef(lm(y ~ x))[2]) ## x ## 0.6462906 0.6462906 We want to very briefly also just show you how you can actually do regression to the origin. In this case I’ll get the same number if I take the centered \\(Y\\) and use the centered \\(X\\) as a predictor, to subtract out the intercept, you put a minus one to get rid of the intercept. Another point that was made before, was that if we were to normalize the \\(Y\\) or the \\(X\\) so that they have standard deviation one, the slope would be the correlation. So let’s just double check that quickly. Here, We normalize the child’s heights by subtracting off the mean and dividing by the standard deviation. We do the same thing for \\(X\\) variables. We have gotten rid of the, the original units, the inches. yn &lt;- (y - mean(y))/sd(y) xn &lt;- (x - mean(x))/sd(x) c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2]) ## xn ## 0.4587624 0.4587624 0.4587624 ## Warning: Ignoring unknown aesthetics: show_guide Here we are showing the somewhat fancy plot for this data. We would also note that ggplot2 does a very good thing for us on our behalf. It automatically gives us a confidence interval around the line. We’ll talk about how to generate this confidence interval later on in the lecture. But it’s very nice that they’re thinking of statistical uncertainty automatically. 1.3.4 Mathematical Details (Optional) XXX 1.4 Regression to the Mean Regression to the mean was an important milestone in the discovery of regression. So we’re going to talk about it. It was discovered by Francis Galton. Regression to mean asks questions like this. Why is it that the children of tall parents tend to be tall, but not as tall as their parents? Why do children of short parents tend to be short, but not as short as their parents? Why do parents of very short children, tend to be short, but not a short as their child? And the same with parents of very tall children? We can try this with anything that is measured with error. Why do the best performing athletes this year tend to do a little worse the following? Why do the best performers on hard exams always do a little worse on the next hard exam? These phenomena are all examples of so-called regression to the mean. Regression to the mean, was invented by Francis Galton in the paper “Regression towards mediocrity in hereditary stature” The Journal of the Anthropological Institute of Great Britain and Ireland , Vol. 15, (1886). The idea served as a foundation for the discovery of linear regression. Regression to the mean often comes up in sports. If you have a player who has a phenomenal year, the next year they tend to do a little bit worse. If you have a player who has a terrible year, the next year they tend to do a little bit better. Another example would be often people talk about stocks in the same way. Some of the best performing stocks tend to go down. These phenomena could all be examples of so called regression to the mean. We will talk about why these happen and whether or not something is intrinsic or whether it is a regression to the mean effect. Regression to the mean was invented by Francis Galton. We like to think of regression to the mean by thinking of the case where it’s a 100% regression to the mean. So imagine if we were to simulate pairs of standard normals, i.e. they have nothing to do with one another, they’re independent standard normals. If we were to take the largest one, the chance that its pair in the second vector is smaller will be high. And this is simply saying that the probability that \\(Y\\) is less than \\(X\\), given \\(X\\) is going to get bigger as \\(X\\) heads to very large values. The same thing in other words, is that probability \\(Y\\) is greater than \\(X\\). Given that \\(X\\) equals \\(X\\) is going to get bigger as \\(X\\) heads to smaller values. This extreme version of regression in the mean where there’s 100% regression to the mean is what we like to think about. \\(P(Y &lt; x | X = x)\\) gets bigger as \\(x\\) heads into the very large values. \\(P(Y &gt; x | X = x)\\) gets bigger as \\(x\\) heads to very small values. However, in most cases there’s some blend of some, some intrinsic component, and a noise. For example, consider a scenario where every student in this class takes two very challenging quizzes. While those at the top likely have a better understanding of the material, quizzes are imperfect instruments, introducing inherent error or noise. This means that even the top performers might benefit from some luck or randomness. Consequently, a top performer, who probably knows the material a bit better than others, may experience a slight dip in performance on the second quiz due to this inherent variability. Conversely, even the worst performers might fare a bit better on one quiz due to chance. This concept extends beyond academics. It’s intriguing to reflect on how much of the discussion about sports revolves around the idea of regression to the mean. For instance, a baseball player with a phenomenal batting average one year might experience a slightly lower average the next year, illustrating the natural tendency for extreme performances to move closer to the average over time. The question is are these examples of just regression to the mean? If so, it would be nice to figure out how to quantify it. This is what Francis Galton did with regression in the first treatment of regression to the mean. Let’s delve into how Francis Galton employed the concept of regression, particularly using correlation, which is intimately related to linear regression. The goal is to quantify regression to the mean, and I’ll illustrate this with a visual representation. Before delving into the R code, let me outline the setup. In this case, I’m assigning \\(X\\) to be the child’s height and \\(Y\\) to be the parent’s height. I’m using a dataset where the parent is a single parent, specifically the father. Both the \\(X\\) and \\(Y\\) values have been normalized, meaning they have a mean of 0 and a variance of 1. Assuming you’re familiar with this normalization process, the regression line will pass through the point (0, 0). Notably, regardless of whether the child’s height is the outcome or the parent’s height is the outcome, the slope of the regression line is simply the correlation. Now, a quirk worth mentioning when creating the plot is that if \\(X\\) is the outcome and you happen to plot it on the horizontal axis, the slope of the line needs to be 1 over the correlation. This is due to the specific orientation of the axes. Keep this in mind as we proceed with the R code. In the code below we are using the dataset from the usingR library, specifically the father.son data. Here’s how we define the variables: Y: Son’s heights, normalized by subtracting the mean and dividing by the standard deviation. X: Father’s heights, similarly normalized. Now, both X and Y should have a mean of 0 and a variance of 1. We use the Greek letter \\(\\rho\\) (rho) to represent the correlation between \\(X\\) and \\(Y\\). If you would check the value of rho, turns out to be about 0.5. This indicates a correlation of 0.5 between the father’s height and the son’s height. Now, let’s create the plot. After loading the ggplot2 library, we assign the ggplot to the variable g and adding points with a black background and salmon-colored foreground. The use of alpha blending makes the points somewhat transparent. We set the x-axis and y-axis limits to be -4 to +4 on both axes. This range is chosen as it should cover most of the data, considering the extremely low probability of standardized random variables being below -4 or above +4. Chebyshev’s theorem supports this choice, especially if you’ve covered it in the Statistical Inference course. Next, we add a layer for the identity line. Afterward, we’ll add the horizontal and vertical axes. library(UsingR) data(father.son) y &lt;- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight) x &lt;- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight) rho &lt;- cor(x, y) library(ggplot2) g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) g = g + geom_point(size = 6, colour = &quot;black&quot;, alpha = 0.2) g = g + geom_point(size = 4, colour = &quot;salmon&quot;, alpha = 0.2) g = g + xlim(-4, 4) + ylim(-4, 4) g = g + geom_abline(intercept = 0, slope = 1) g = g + geom_vline(xintercept = 0) g = g + geom_hline(yintercept = 0) g = g + geom_abline(intercept = 0, slope = rho, size = 2) g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2) g = ggplot(data.frame(x, y), aes(x = x, y = y)) g = g + geom_point(size = 5, alpha = .2, colour = &quot;black&quot;) g = g + geom_point(size = 4, alpha = .2, colour = &quot;red&quot;) g = g + geom_vline(xintercept = 0) g = g + geom_hline(yintercept = 0) g = g + geom_abline(position = &quot;identity&quot;) ## Warning: Ignoring unknown parameters: position g Now, let’s create two lines. First, we’ll treat the son’s height as the outcome and the father’s height as the predictor. Then, we’ll add the line treating the son’s height as the predictor and the father’s height as the outcome. Since the axes are rotated, the slope needs to be 1 over rho. library(UsingR) data(father.son) y &lt;- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight) x &lt;- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight) rho &lt;- cor(x, y) library(ggplot2) g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) g = g + geom_point(size = 6, colour = &quot;black&quot;, alpha = 0.2) g = g + geom_point(size = 4, colour = &quot;salmon&quot;, alpha = 0.2) g = g + xlim(-4, 4) + ylim(-4, 4) g = g + geom_abline(intercept = 0, slope = 1) g = g + geom_vline(xintercept = 0) g = g + geom_hline(yintercept = 0) g = g + geom_abline(intercept = 0, slope = rho, size = 2) g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2) g = ggplot(data.frame(x, y), aes(x = x, y = y)) g = g + geom_point(size = 5, alpha = .2, colour = &quot;black&quot;) g = g + geom_point(size = 4, alpha = .2, colour = &quot;red&quot;) g = g + geom_vline(xintercept = 0) g = g + geom_hline(yintercept = 0) g = g + geom_abline(position = &quot;identity&quot;) ## Warning: Ignoring unknown parameters: position g = g + geom_abline(intercept = 0, slope = rho, size = 2) g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2) g = g + xlab(&quot;Father&#39;s height, normalized&quot;) g = g + ylab(&quot;Son&#39;s height, normalized&quot;) g Now, let’s discuss regression to the mean in relation to this plot. If the observations perfectly aligned on a line, it would be the identity line, given that both \\(X\\) and \\(Y\\) have been normalized. The father’s height is plotted as the \\(X\\) variable, and the son’s height is plotted as the \\(Y\\) variable. For instance, if we had a father’s height of 2 with no noise, the prediction for the son’s height would also be 2, representing 2 standard deviations above the mean for both fathers and sons. However, in the presence of noise, the prediction deviates from 2 but falls on the regression line. This prediction is obtained by multiplying the father’s height (=2) by the slope (=correlation). The result is a prediction between 2 and 0, precisely 2 multiplied by the correlation. This phenomenon is known as regression to the mean. The extent to which this correlation is shrunk towards the horizontal line indicates the degree of regression to the mean. Consider the extreme cases for better understanding. In a scenario with no noise, the line would fall perfectly on the identity line. Conversely, if there was only noise, indicating no informative relationship between father’s and son’s heights (correlation = 0), the line would lie on the horizontal axis, predicting a constant height of 0 for sons based on fathers. This concept holds when considering the son’s height as the predictor and the father’s height as the outcome. The regression to the mean is observed in how much the line is shrunk towards the vertical axis. This notion, introduced by Francis Galton, played a pivotal role in the development of modern regression. Although it remains a fundamental idea, regression to the mean continues to have significance in statistical analyses, particularly in the study of longitudinal data where it’s crucial to consider this phenomenon. In summary: * If you had to predict a son’s normalized height, it would be \\(Cor(Y, X) * X_i\\) * If you had to predict a father’s normalized height, it would be \\(Cor(Y, X) * Y_i\\) * Multiplication by this correlation shrinks toward 0 (regression toward the mean) * If the correlation is 1 there is no regression to the mean (if father’s height perfectly determine’s child’s height and vice versa) * Note, regression to the mean has been thought about quite a bit and generalized 1.5 Practical R Exercises in swirl During this course we’ll be using the swirl software package for R in order to illustrate some key concepts. The swirl package turns the R console into an interactive learning environment. Using swirl will also give you the opportunity to construct and explore your own regression models. Install R swirl requires R 3.0.2 or later. If you have an older version of R, please update before going any further. If you’re not sure what version of R you have, type R.version.string at the R prompt. You can download the latest version of R from https://www.r-project.org/. Optional but highly recommended: Install RStudio. You can download the latest version of RStudio at https://www.rstudio.com/products/rstudio/. Install swirl Since swirl is an R package, you can easily install it by entering a single command from the R console: If you are on a Linux operating system, please visit our Installing swirl on Linux page for special instructions: install.packages(\"swirl\") If you’ve installed swirl in the past make sure you have version 2.2.21 or later. You can check this with: packageVersion(\"swirl\") Load swirl Every time you want to use swirl, you need to first load the package. From the R console: library(swirl). Install the Regression Models course swirl offers a variety of interactive courses, but for our purposes, you want the one called Regression Models. If this is your first time using swirl, it will prompt you to install the Regression Models course automatically. If you’ve used swirl in the past, you will need to type the following from the R prompt: install_course(\"Regression Models\"). Start swirl and complete the lessons Type the following from the R console to start swirl: For the first part of this course you should complete the following lessons: - Introduction - Residuals - Least Squares Estimation Good luck and have fun! 1.6 Week 1 Quiz Consider the data set given by the R code x &lt;- c(0.18, -1.54, 0.42, 0.95)and weights given by w &lt;- c(2, 1, 3, 1) give the value of \\(μ\\) that minimizes the least squares equation \\(\\sum_{i=1}^n w_i (x_i - \\mu)^2\\). Consider the following data set fit the regression through the origin and get the slope treating yas the outcome and x as the regressor. (Hint, do not center the data since we want regression through the origin, not through the means of the data.) x &lt;- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42) y &lt;- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05) Do data(mtcars) from the datasets package and fit the regression model with mpg as the outcome and weight as the predictor. What is the slope coefficient? Consider data with an outcome (\\(Y\\)) and a predictor (\\(X\\)). The standard deviation of the predictor is one half that of the outcome. The correlation between the two variables is .5. What value would the slope coefficient for the regression model with \\(Y\\) as the outcome and \\(X\\) as the predictor? Students were given two hard tests and scores were normalized to have empirical mean 0 and variance 1. The correlation between the scores on the two tests was 0.4. What would be the expected score on Quiz 2 for a student who had a normalized score of 1.5 on Quiz 1? Consider the data given by x &lt;- c(8.58, 10.46, 9.01, 9.64, 8.86). What is the value of the first measurement if x were normalized (to have mean 0 and variance 1)? Consider the following data set (used above as well). What is the intercept for fitting the model with x as the predictor and y as the outcome? x &lt;- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42) y &lt;- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05) You know that both the predictor and response have mean 0. What can be said about the intercept when you fit a linear regression? It must be identically 0. It is undefined as you have to divide by zero. It must be exactly one. Nothing about the intercept can be said from the information given. Consider the data given by x &lt;- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42). What value minimizes the sum of the squared distances between these points and itself? Let the slope having fit \\(Y\\) as the outcome and \\(X\\) as the predictor be denoted as \\(β_1\\). Let the slope from fitting \\(X\\) as the outcome and \\(Y\\) as the predictor be denoted as \\(γ_1\\). Suppose that you divide \\(β_1\\) by \\(γ_1\\); in other words consider \\(β_1/γ_1\\). What is this ratio always equal to? "],["week-02.html", "Chapter 2 Week 02 2.1 Statistical linear regression models 2.2 Residuals 2.3 Inference in regression 2.4 For the project 2.5 Practical R Exercises in swirl 2.6 Week 2 Quiz", " Chapter 2 Week 02 2.1 Statistical linear regression models Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know how to extend our estimates to a population. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity. 2.1.1 Statistical Linear Regression Models Finding a good regression line using least squares is a mathematical procedure. However, we’d like to do statistics. We’d like to draw emphasis based on our data. In other words we’d like to generalize from our data to a population using statistical models. Consider the probabilistic model for linear regression \\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_{i}\\] The values of \\(\\beta_0, \\beta_1\\) are the population parameters that we would like to estimate. \\(X_i\\) is a collection of explanatory variables that we do know, and \\(\\epsilon_i\\) is iid Gaussian errors. Here the \\(\\epsilon_{i}\\) are assumed iid \\(N(0, \\sigma^2)\\). Understanding independent errors in regression can be approached in various ways. One relatively straightforward interpretation is to consider them as the cumulative effect of unmodeled variables that might collectively influence the response. These unmodeled variables act on the response in a manner that can be statistically modeled as independent and identically distributed Gaussian errors. Setting aside the complexities of interpretation, let’s focus on the mechanics of working with statistical inference for regression. It’s important to note that the expected value of the response given a specific value of the regressor is simply the line at that regressor, represented as \\(β_0 + β₁x_i\\). Additionally, the variance of the response at any given value of the regressor is denoted as σ². It’s crucial to clarify that this variance pertains to the variation around the regression line and not the overall response variance. Conditioning on X reduces the variation, making it lower than the unconditional response variance. Note, \\(E[Y_i ~|~ X_i = x_i] = \\mu_i = \\beta_0 + \\beta_1 x_i\\) Note, \\(Var(Y_i ~|~ X_i = x_i) = \\sigma^2\\). Both the expected value and variance mentioned here are population quantities. Although there are sample analogs that estimate these values, it’s essential to recognize that, at this point, we are referring to population quantities—these are the estimands that we ideally want to know. Now that we have a formal statistical framework, we can interpret our regression coefficients with respect to that framework. Take for example, the intercept. It is the expected value \\(Y\\) given that the regressor is 0. \\[E[Y | X = 0] = \\beta_0 + \\beta_1 \\times 0 = \\beta_0\\] Note that the regressor being equal to zero is often not of interest in the study. For example, if the regression variable is blood pressure, probably you’re not interested in the response for among people with blood pressure of zero. However, there is an easy fix for this. Consider just shifting our regression variable by a constant \\(a\\). \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i = \\beta_0 + a \\beta_1 + \\beta_1 (X_i - a) + \\epsilon_i = \\tilde \\beta_0 + \\beta_1 (X_i - a) + \\epsilon_i \\] We see a new regression line with a new intercept and the same slope. So, shifting your \\(X\\) values by value \\(a\\) changes the intercept, but not the slope. Often \\(a\\) is set to \\(\\bar X\\) so that the intercept is interpretted as the expected response at the average \\(X\\) value. For slope, we can interpret it as the expected change in response for a 1 unit change in the predictor. \\[ E[Y ~|~ X = x+1] - E[Y ~|~ X = x] = \\beta_0 + \\beta_1 (x + 1) - (\\beta_0 + \\beta_1 x ) = \\beta_1 \\] * Consider the impact of changing the units of \\(X\\). \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i = \\beta_0 + \\frac{\\beta_1}{a} (X_i a) + \\epsilon_i = \\beta_0 + \\tilde \\beta_1 (X_i a) + \\epsilon_i \\] We see a new regression line with a new slope and the same intercept. So, multiplying your \\(X\\) values by value \\(a\\) changes the slope, but not the intercept. For example, \\(X\\) is height in \\(m\\) and \\(Y\\) is weight in \\(kg\\). Then \\(\\beta_1\\) is \\(kg/m\\). Converting \\(X\\) to \\(cm\\) implies multiplying \\(X\\) by \\(100 cm/m\\). To get \\(\\beta_1\\) in the right units, we have to divide by \\(100 cm /m\\) to get it to have the right units. \\[ X m \\times \\frac{100cm}{m} = (100 X) cm ~~\\mbox{and}~~ \\beta_1 \\frac{kg}{m} \\times\\frac{1 m}{100cm} = \\left(\\frac{\\beta_1}{100}\\right)\\frac{kg}{cm} \\] If we would like to guess the outcome at a particular value of the predictor, say \\(X\\), the regression model guesses \\[\\hat \\beta_0 + \\hat \\beta_1 X\\] This doesn’t mean that we can only predict at the fitted values. We can predict at any value of \\(X\\) by plugging in the value of \\(X\\) into the equation. However, we’re going to have more reasonable predictions if the value of \\(X\\) that we plug in is in the cloud of data that we used to build the model. Later on, we’ll also talk about how to account for that kind of uncertainty with prediction intervals. But for the time being, let’s just talk about how we get a prediction. Let’s go through an example to interpret the regression coefficients and show running of the regression coefficient. The dataset is the diamond dataset from the UsingR package. The data is diamond prices in Singapore dollars and diamond weight in carats, which is a standard measure of diamond mass. library(UsingR) ## Loading required package: MASS ## Loading required package: HistData ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## ## Attaching package: &#39;UsingR&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cancer data(diamond) library(ggplot2) g = ggplot(diamond, aes(x = carat, y = price)) g = g + xlab(&quot;Mass (carats)&quot;) g = g + ylab(&quot;Price (SIN $)&quot;) g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha=0.5) g = g + geom_point(size = 5, colour = &quot;blue&quot;, alpha=0.2) g = g + geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) g ## `geom_smooth()` using formula &#39;y ~ x&#39; In this code we assign variable g to the ggplot, the dataset is diamond, the aesthetic has the horizontal axis variable as carat and the y-axis variable as price, we add a layer where the xlab is Mass in carats and the y label price in Singapore dollars. We also add the points of the black background and then a light alpha blending color on top. Afterwards we add a layer that is geom_smooth where method = \"lm\" will add the regression line. If you omit any arguments, it’s just going to assume the regression line with \\(Y\\) as the outcome and \\(X\\) as the predictor. Finally, we indicate the color of the regression line as black and call the plot. Notice what we are plotting is the fitted line, the line that minimizes the sum of the squared vertical distances between the points and the lines. By default, lm includes an intercept, if you don’t want an intercept, you have to explicitly force it in the model. We also want the dataset to be the diamond dataset in other words, we have to give it the data frame. Otherwise, lm looks in the regular R environment for variables in the model. After running the code it basically just prints out the coefficients \\(\\beta_0, \\beta_1\\), which are the intercept and labels it as Intercept and the regression variable for the carat, the slope for the carat regression variable. fit &lt;- lm(price ~ carat, data = diamond) coef(fit) ## (Intercept) carat ## -259.6259 3721.0249 Let’s look at this \\(3,721\\) variable and try to interpret it. It’s saying that we have an expected \\(3,721\\) Singapore dollar increase in price for every carat increase in mass of the diamond. The intercept, \\(-259\\) is the expected price of a \\(0\\) carat diamond not very interesting, because we’re not interested in zero carat diamonds. A side note, if you want a much more detailed printout by doing summary(fit) which is the summary of the outputted variable from lm and you get this more elaborate printout. ## ## Call: ## lm(formula = price ~ carat, data = diamond) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.159 -21.448 -0.869 18.972 79.370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -259.63 17.32 -14.99 &lt;2e-16 *** ## carat 3721.02 81.79 45.50 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.84 on 46 degrees of freedom ## Multiple R-squared: 0.9783, Adjusted R-squared: 0.9778 ## F-statistic: 2070 on 1 and 46 DF, p-value: &lt; 2.2e-16 If we mean center our \\(X\\) variable, so that the intercept is on a more interpretable scale. Here we assign the output to a different variable, fit2 instead of fit, because we don’t want to overwrite the original fit. fit2 &lt;- lm(price ~ I(carat - mean(carat)), data = diamond) coef(fit2) ## (Intercept) I(carat - mean(carat)) ## 500.0833 3721.0249 As you notice in code: lm is again the linear model procedure, the outcome stays the same and we use carat - mean(carat), and the I is to indicate that we want to do arithmetic on the variable. So, we want to subtract the mean of the carat variable from the carat variable. This is a way to mean center the variable. As we expected the slope stays the same, 3,721, but the intercept has changed to \\(500\\), meaning \\(\\$ 500\\), Singapore dollars is the expected price of the average sized diamond. In this case, the average diamond is about 0.2 carats. A one carat increase is actually kind of big. What about changing the units to one-tenth of a carat? We can do this just by dividing the coefficient by ten. So we know that we would expect to see a \\(\\$372\\) increase in price for every \\(0.1\\) of a carat increase in the mass of a diamond. fit3 &lt;- lm(price ~ I(carat/10), data = diamond) coef(fit3) ## (Intercept) I(carat/10) ## -259.6259 37210.2485 In the linear model fit instead of putting in carat, we put in \\(carat * 10\\), the units of this new variable is one-tenth of a carat. The data is of course, still the diamond dataset. Imagine if someone came to you with three new diamonds that they had 0.16 carats, 0.27 carats and 0.35 carats, and they wanted to know what you would estimate the price would be. Well, you could do it manually by grabbing the two coefficients in multiplying the intercept or adding the intercept plus the slope times these new values. Let’s do that: newx &lt;- c(0.16, 0.27, 0.34) coef(fit)[1] + coef(fit)[2] * newx ## [1] 335.7381 745.0508 1005.5225 predict(fit, newdata = data.frame(carat = newx)) ## 1 2 3 ## 335.7381 745.0508 1005.5225 Often, you don’t want to do even that much coding, you want to more general method, especially when you get lots of regression variables. So there’s this general method called predict that will take the output from several different kinds of model fits. Linear models are one example, but predict is a generic function, and it applies to several different prediction models. The new data is a data.frame(catar=newx) that has the new values of \\(X\\) for the carat variable. Then when we do that, what you’ll see is the same answer. The difference is that it scales up when we have lots of regressors in much more complicated settings. In general, we want to predict using the predict function. If you omit this new data statement if you just do predict fit, it predicts at the observed \\(X\\) values, so it gives you the \\(\\hat Y\\) values. If you want it at new \\(X\\) values, you have to give it this new data argument. data(diamond) plot(diamond$carat, diamond$price, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;, bg = &quot;lightblue&quot;, col = &quot;black&quot;, cex = 1.1, pch = 21,frame = FALSE) abline(fit, lwd = 2) points(diamond$carat, predict(fit), pch = 19, col = &quot;red&quot;) lines(c(0.16, 0.16, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.16, coef(fit)[1] + coef(fit)[2] * 0.16)) lines(c(0.27, 0.27, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.27, coef(fit)[1] + coef(fit)[2] * 0.27)) lines(c(0.34, 0.34, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.34, coef(fit)[1] + coef(fit)[2] * 0.34)) text(newx, rep(250, 3), labels = newx, pos = 2) To illustrate, here’s our observe data points in blue. The fitted values when we do the predict command, the fitted values in red all of the observed \\(X\\) values and their associated fitted points on the line. These are if we were to draw vertical lines from the observed data points on to the fitted line, they would occur on these red points. When we predicted a new value of \\(X\\), we’re finding a point along this horizontal axis. In this example we want, 0.16, 0.27 and 0.34. We’re drawing a line up to the fitted regression line and then over to dollars and those are our predicted dollar amounts. 2.2 Residuals Residuals represent variation left unexplained by our model. We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors. To begin, let’s delve into our illustrative example featuring the diamond dataset. It’s important to recall that in this dataset, the diamonds are priced in Singapore dollars. The key variable under consideration is the weight of the diamonds, expressed in carats. Our objective is to explore the correlation between the weight of diamonds and their corresponding prices, seeking to understand how variations in diamond prices can be elucidated by their mass. library(UsingR) data(diamond) library(ggplot2) g = ggplot(diamond, aes(x = carat, y = price)) g = g + xlab(&quot;Mass (carats)&quot;) g = g + ylab(&quot;Price (SIN $)&quot;) g = g + geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha=0.5) g = g + geom_point(size = 5, colour = &quot;blue&quot;, alpha=0.2) g ## `geom_smooth()` using formula &#39;y ~ x&#39; Now, our focus is on elucidating the price (on the vertical axis) through the mass (on the horizontal axis). Without taking mass into account, we’d have a scatter of points projecting onto the vertical axis, displaying considerable variation. Disregarding mass would result in a notable amount of unexplained variation. However, when we factor in mass, the variation diminishes, as we’re now examining the variation around the regression line. This remaining variation around the regression line is termed residual variation. It represents the portion of variation that persists even after accounting for mass. Initially, there is substantial variation, a significant portion of which is clarified by the linear relationship with mass. Nonetheless, there remains some residual variation. These residual distances are referred to as residuals, and they constitute the focal point of today’s lecture. Residuals prove to be valuable for various diagnostic purposes, including assessing model fit. Let’s refresh our memory regarding the model under consideration. The outcome in our example,price, is \\(Y_i\\), which we’re assuming is a line. Observed outcome \\(i\\) is \\(Y_i\\) at predictor value \\(X_i\\), predicted outcome \\(i\\) is \\(\\hat Y_i\\) at predictor value \\(X_i\\) is \\(\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\\). Residual, the between the observed and predicted outcome \\(e_i = Y_i - \\hat Y_i\\), which is the vertical distance between the observed data point and the regression line where least squares minimizes \\(\\sum_{i=1}^n e_i^2\\). In essence, it was minimizing the sum of the squared residual, summation \\(e_i\\) squared. One way to think about the residuals are as an estimate of \\(\\epsilon_i\\), though, you have to be careful with that, because as we will see later on, we can decrease the residuals just by adding irrelevant regressors into the equation. Let’s talk about some aspects of residuals that will help us interpret them. \\(E[e_i] = 0\\). (Their population’s expected value is zero.) If an intercept is included, \\(\\sum_{i=1}^n e_i = 0\\) (Their empirical sum, hence the empirical mean also, is zero if you include an intercept.If you don’t include an intercept, this property doesn’t have to hold.) If a regressor variable, \\(X_i\\), is included in the model \\(\\sum_{i=1}^n e_i X_i = 0\\). (The generalization of this property is, if you include any regression term in linear regression, the sum of the residuals times that regression variable has to be zero.) Residuals are useful for investigating poor model fit. (We can create plots that highlight the aspects of poor model fit.) Positive residuals are above the line, negative residuals are below. Residuals can be thought of as the outcome (\\(Y\\)) with the linear association of the predictor (\\(X\\)) removed. (A common use of residuals is to think of them as the outcome \\(Y\\) with the linear influence of the predictor \\(X\\) having been removed. For example, if we wanted to in some subsequent model or some subsequent analysis diamond prices, but in a way that has already been adjusted for their weight, calibrating all the diamond prices to be on the same scale regardless of their weight, we would take those residuals from the model fit that has diamond prices as the outcome, and weight as the predictor.) One differentiates residual variation (variation after removing the predictor) from systematic variation (variation explained by the regression model). (It’s very common to take residuals and carry them forward in a later analysis where you want to think of them as the, the new outcome, having removed the predictor at that point. But, remember with linear regression, you’re only removing the linear component of the predictor. One should differentiate between residual variation, which is variation that is left over after the explanatory variable has been accounted for in a linear fashion, from systematic variation, which is variation explained by the regression model. Again, residual plots can highlight poor model fit. And, we are going to go through some residual plots.) Residual plots highlight poor model fit. Let’s walk through calculating residuals in this example we’re going to use the diamond dataset. data(diamond) y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y) fit &lt;- lm(y ~ x) e &lt;- resid(fit) yhat &lt;- predict(fit) max(abs(e -(y - yhat))) ## [1] 8.242296e-13 max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x))) ## [1] 8.242296e-13 We redefine price as y and x as carat, n as the length of the number of pairs. We assign the linear regression object from lm to variable fit. To get the residuals we resid(fit), and we assign that to e. We also get the fitted values by predict(fit) and assign that to yhat. We can check that the residuals are the difference between the observed outcome and the predicted outcome. We can also check that the residuals are the difference between the observed outcome and the intercept plus the slope times the predictor. To show you that residual’s calculated via resid() functions are the same as the residuals that we calculate manually we take the absolute difference between y - yhat and e and find the one is on the scale of \\(10^{-13}\\) i.e, up to numerical precision, it’s the same thing. Then lastly, we want to show that the residuals are the difference between the observed outcome and the intercept plus the slope times the predictor, again up to numeric precision, exactly the same. To obtain the residuals, the preferred method is to use resid(). However, by demonstrating an alternative code, we aim to shed light on the underlying process of “res” and the specific computation performed by resid(). Ultimately, we would like to demonstrate that the total sum of the residuals equals zero. Technically, it’s \\(10^{-14}\\), which is sufficiently close to zero. Additionally, the sum of the residuals multiplied by the price variable `x`` must also be zero—albeit at \\(10^{-15}\\). Therefore, in numerical terms, both cases effectively amount to zero. These residuals represent the magnitudes of the deviations depicted by the red line in the accompanying plot. plot(diamond$carat, diamond$price, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;, bg = &quot;lightblue&quot;, col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE) abline(fit, lwd = 2) for (i in 1 : n) lines(c(x[i], x[i]), c(y[i], yhat[i]), col = &quot;red&quot; , lwd = 2) Notice all of the blank space in the graph, making the plot kind of useless for that purpose, why don’t we plot the residuals on the vertical axis versus mass on the horizontal axis? plot(x, e, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Residuals (SIN $)&quot;, bg = &quot;lightblue&quot;, col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE) abline(h = 0, lwd = 2) for (i in 1 : n) lines(c(x[i], x[i]), c(e[i], 0), col = &quot;red&quot; , lwd = 2) Now we can see the residual variation much more clearly. One important point is: the residuals should be mostly patternless. Also, remember that if you include an intercept, residuals have to sum to zero. We can see some interesting patterns by honing in on the residual plot here. For example, we can see that there were lots of diamonds of exactly the same mass which gets lost in the scatter plot. Next, we want to go through some pathological residual plots, just to highlight what residual plots can do for us. x = runif(100, -3, 3); y = x + sin(x) + rnorm(100, sd = .2); library(ggplot2) g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) g = g + geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha = 0.4) g = g + geom_point(size = 5, colour = &quot;red&quot;, alpha = 0.4) g ## `geom_smooth()` using formula &#39;y ~ x&#39; Here X is just going to be uniform \\([-3,3]\\), y is equal to x, so it’s an identity line, but then we add another term that’s sin(x). This looks like an identity line, but kind of oscillating around it a little bit with some normal noise on top of it. Before we move on to the residual plot, let us make a comment. This model is actually not the correct model for this data and this might happen in practice. This doesn’t mean that this model is unimportant, right? There is a linear trend and the model is accounting for it, it’s just not accounting for the secondary variation in the sin term. To emphasize just because you aren’t fitting the actually correct model, that doesn’t mean the model is itself useless, in regression, having the exact right model is not always the primary goal. You can get meaningful information about trends from incorrect models. Let’s me plot the residuals’ versus the x variable. g = ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y)) g = g + geom_hline(yintercept = 0, size = 2); g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha = 0.4) g = g + geom_point(size = 5, colour = &quot;red&quot;, alpha = 0.4) g = g + xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;) g You can see that the sin term is now extremely apparent. This is what the residual plot has done highlighting the model inadequacy. Another example is the following plot, where by appearances, the plot falls perfectly on a line. x &lt;- runif(100, 0, 6); y &lt;- x + rnorm(100, mean = 0, sd = .001 * x); g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) g = g + geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha = 0.4) g = g + geom_point(size = 5, colour = &quot;red&quot;, alpha = 0.4) g ## `geom_smooth()` using formula &#39;y ~ x&#39; But when you highlight the residuals, it looks quite different. g = ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y)) g = g + geom_hline(yintercept = 0, size = 2); g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha = 0.4) g = g + geom_point(size = 5, colour = &quot;red&quot;, alpha = 0.4) g = g + xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;) g Plotting the residuals shows the trend toward greater variability as you head along the x variable. That property, where the variability increases with the x variables called heteroscedasticity. Heteroscedasticity is one of those things that residual plots are quite good at diagnosing and you couldn’t see it. Let’s run the residual plot for the diamond data. diamond$e &lt;- resid(lm(price ~ carat, data = diamond)) g = ggplot(diamond, aes(x = carat, y = e)) g = g + xlab(&quot;Mass (carats)&quot;) g = g + ylab(&quot;Residual price (SIN $)&quot;) g = g + geom_hline(yintercept = 0, size = 2) g = g + geom_point(size = 7, colour = &quot;black&quot;, alpha=0.5) g = g + geom_point(size = 5, colour = &quot;blue&quot;, alpha=0.2) g The x-label is Mass in carats, the y-label is Residual price and just to emphasize the residuals have the same units as the ys. There doesn’t appear to be a lot of pattern in the plot, meaning it’s a pretty good fit. Let us illustrate something about variability in a diamond dataset that will help us set the stage for defining some new properties about our regression model fit. So we create two residual vectors. The first residual vector is the one where we just fit an intercept, so the residuals are just the deviations around the average price. The second is the variation around the regression line with carats as the explanatory variable and price as the outcome. Then we create a factor variable that labels the set of residuals. The first one is labeled as a bunch of intercept only model residuals and the second set is labeled as a bunch of intercept and slope residuals. e = c(resid(lm(price ~ 1, data = diamond)), resid(lm(price ~ carat, data = diamond))) fit = factor(c(rep(&quot;Itc&quot;, nrow(diamond)), rep(&quot;Itc, slope&quot;, nrow(diamond)))) g = ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit)) g = g + geom_dotplot(binaxis = &quot;y&quot;, size = 2, stackdir = &quot;center&quot;, binwidth = 20) ## Warning: Ignoring unknown parameters: size g = g + xlab(&quot;Fitting approach&quot;) g = g + ylab(&quot;Residual price&quot;) g What we see on the left-hand plot with just the intercept is the variation in diamond prices around the average diamond price. What we’re seeing in the rightmost plot is displaying the variation around the regression line. So we have explained a lot of the variation with the relationship with mass. We’re going to talk about \\(R^2\\), which basically says, we can decompose the total variation, the variation explained by the regression model and the variation that’s left over after accounting for the regression model. Residual variation is the variation around the regression line (\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)). The residuals are the vertical distances between the outcomes and the fitted regression line. If we include an intercept, the residuals have to sum to zero, which means their mean is zero. The variance of the residuals, is the average squared residual (\\(\\sigma^2\\) is \\(\\frac{1}{n}\\sum_{i=1}^n e_i^2\\)). Most people use \\(\\hat \\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2\\), they \\(n-2\\) instead of \\(n\\) so that \\(E[\\hat \\sigma^2] = \\sigma^2\\). The way to think about that is, we include the intercept the residuals have to sum to zero, that puts a constraint. If you know n minus one of them, then, you know the \\(n^{th}\\) if you have a line term in there, if you have a co-variant in there, then, that puts a second constrain on the residuals. So, you lose two degrees of freedom. If you put another regression variable in there, you have another constraint, you lose three degrees of freedom. So in that sense you really don’t have n residuals, you have \\(n-2\\) of them, because if you knew \\(n-2\\) of them you could figure out the last two. And that’s why it’s one over \\(n-2\\). You can grab the residual variation out of the lm fit and assign it to a variable. y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y) fit &lt;- lm(y ~ x) summary(fit)$sigma ## [1] 31.84052 sqrt(sum(resid(fit)^2) / (n - 2)) ## [1] 31.84052 If you want to grab it as an object that you can assign to something, just put dollar sign sigma. Then you can assign sigma to any other variable. The line sqrt(sum(resid(fit)^2) / (n - 2)) will result in the value and is showing what the lm function is doing behind the scenes. Now let’s go back to the following plot where we look at the total variability in diamond prices, and compare what happens to the variability when we explain some of that variability with a regression line. e = c(resid(lm(price ~ 1, data = diamond)), resid(lm(price ~ carat, data = diamond))) fit = factor(c(rep(&quot;Itc&quot;, nrow(diamond)), rep(&quot;Itc, slope&quot;, nrow(diamond)))) g = ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit)) g = g + geom_dotplot(binaxis = &quot;y&quot;, size = 2, stackdir = &quot;center&quot;, binwidth = 20) ## Warning: Ignoring unknown parameters: size g = g + xlab(&quot;Fitting approach&quot;) g = g + ylab(&quot;Residual price&quot;) g The total variability is just the deviations of the data, \\(\\sum_{i=1}^n (Y_i - \\bar Y)^2\\) the average squared deviation of the data around its mean. To make things easy, let’s forget about the denominator and just talk about the sum of the squared deviations. We might call the regression variability as the component of that variability that then gets explained away by the regression line. We would take the points on the regression line, the heights, which is the variability in the response and explained by the regression line, \\(\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2\\). The error variability is what’s leftover around the regression line \\(\\sum_{i=1}^n (Y_i - \\hat Y_i)^2\\). The interesting identity is that the total variability disregarding everything except for where they’re centered at is equal to the regression variability, that is the variability explained by the model plus the residual variability, the variability left over and not explained by the model. \\[ \\sum_{i=1}^n (Y_i - \\bar Y)^2 = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 + \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 \\] Because the residual variation and the regression model variation add up to the total variation we can define a quantity that represents the percentage of the total variation that’s represented by the model. This is called the coefficient of determination, \\(R^2\\). R squared is the percentage of the total variability that is explained by the linear relationship with the predictor \\[ R^2 = \\frac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2} \\] So R squared for our diamond example, is the percentage of the variation in diamond price, that is explained by the regression relationship with mass. Some facts about \\(R^2\\): \\(R^2\\) is the percentage of variation explained by the regression model. \\(0 \\leq R^2 \\leq 1\\) (because the regression variability and the error variability and the sums of the squares add up to the total sums of squares, and they are all positive) \\(R^2\\) is the sample correlation squared. (If we define R as the sample correlation between the predictor and the outcome, then R squared is literally that sample correlation R, squared.) \\(R^2\\) can be a misleading summary of model fit. (For example, if you have somewhat noisy data and delete a lot of the points in the middle you can get a much higher R squared. Or if you just add arbitrary regression variables into a linear model fit, you increase R squared and decrease mean squared error) Deleting data can inflate \\(R^2\\). (For later.) Adding terms to a regression model always increases \\(R^2\\). Anscombe created a particularly stark example of a bunch of data sets with an equivalent R squared, equivalent mean, and variances in the x’s and the y’s, and identical regression relationships, but when you look at the scatter plots, you can see that the fit has very different meanings in each of the cases. The first is a nice regression line, exactly sort of along the lines of what we think of, when we think of just a slightly noisy x,y relationship. In the second one clearly there’s a missing term in order to address some of the curvature in the data. In the third one, there’s an outlier. Finally, in the fourth one, all the data stacked up at one particular location and there’s one point way out at the end. So you could imagine getting this if you had the first example and you deleted a lot of the points in the middle. In all these cases you have an equivalent R squared. But the summary to the single number certainly has thrown out a lot of the important information that you get from a simple scatter plot. 2.2.1 Optional reading How to derive R squared: \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\bar Y)^2 &amp; = \\sum_{i=1}^n (Y_i - \\hat Y_i + \\hat Y_i - \\bar Y)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 + 2 \\sum_{i=1}^n (Y_i - \\hat Y_i)(\\hat Y_i - \\bar Y) + \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 \\\\ \\end{align} \\] 2.2.1.1 Scratch work \\((Y_i - \\hat Y_i) = \\{Y_i - (\\bar Y - \\hat \\beta_1 \\bar X) - \\hat \\beta_1 X_i\\} = (Y_i - \\bar Y) - \\hat \\beta_1 (X_i - \\bar X)\\) \\((\\hat Y_i - \\bar Y) = (\\bar Y - \\hat \\beta_1 \\bar X - \\hat \\beta_1 X_i - \\bar Y ) = \\hat \\beta_1 (X_i - \\bar X)\\) \\(\\sum_{i=1}^n (Y_i - \\hat Y_i)(\\hat Y_i - \\bar Y) = \\sum_{i=1}^n \\{(Y_i - \\bar Y) - \\hat \\beta_1 (X_i - \\bar X))\\}\\{\\hat \\beta_1 (X_i - \\bar X)\\}\\) \\(=\\hat \\beta_1 \\sum_{i=1}^n (Y_i - \\bar Y)(X_i - \\bar X) -\\hat\\beta_1^2\\sum_{i=1}^n (X_i - \\bar X)^2\\) \\(= \\hat \\beta_1^2 \\sum_{i=1}^n (X_i - \\bar X)^2-\\hat\\beta_1^2\\sum_{i=1}^n (X_i - \\bar X)^2 = 0\\) 2.2.1.2 The relation between R squared and r Recall that \\((\\hat Y_i - \\bar Y) = \\hat \\beta_1 (X_i - \\bar X)\\) so that \\[ R^2 = \\frac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2} = \\hat \\beta_1^2 \\frac{\\sum_{i=1}^n(X_i - \\bar X)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2} = Cor(Y, X)^2 \\] Since, recall, \\[ \\hat \\beta_1 = Cor(Y, X)\\frac{Sd(Y)}{Sd(X)} \\] So, \\(R^2\\) is literally \\(r\\) squared. 2.3 Inference in regression Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference. These statements apply generally, and, of course, to the regression setting that we’ve been studying. In the next few lectures, we’ll cover inference in regression where we make some Gaussian assumptions about the errors. Before we begin talking about inference, let’s just revisit our model so that it’s fresh in our mind. \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) where \\(\\epsilon \\sim N(0, \\sigma^2)\\). For the time being, we’re going to assume that the true model is known, and this will be the basis for most of this class. We also assume that you’ve seen confidence intervals and hypothesis tests before. If you feel the need, you should go back and review them. Also, remember \\(\\hat \\beta_0 = \\bar Y - \\hat \\beta_1 \\bar X\\), \\(\\hat \\beta_1 = Cor(Y, X) \\frac{Sd(Y)}{Sd(X)}\\). We would like to review some of the basic concepts from statistical inference. Statistics like \\(\\frac{\\hat \\theta - \\theta}{\\hat \\sigma_{\\hat \\theta}}\\) often have the following properties. Is normally distributed and has a finite sample Student’s T distribution if the variance is replaced with a sample estimate (under normality assumptions). Can be used to test \\(H_0 : \\theta = \\theta_0\\) versus \\(H_a : \\theta &gt;, &lt;, \\neq \\theta_0\\). Can be used to create a confidence interval for \\(\\theta\\) via \\(\\hat \\theta \\pm Q_{1-\\alpha/2} \\hat \\sigma_{\\hat \\theta}\\) where \\(Q_{1-\\alpha/2}\\) is the relevant quantile from either a normal or T distribution. (For example, if our \\(\\alpha\\) is 5%, so we want a 95% confidence interval, we take the \\(97.5^{th}\\) quantile.) In the case of regression with iid sampling assumptions and normal errors, our inferences will follow very similarily to what you saw in your inference class. We won’t cover asymptotics for regression analysis, but suffice it to say that under assumptions on the ways in which the \\(X\\) values are collected, the iid sampling model, and mean model, the normal results hold to create intervals and confidence intervals. In other words, it’s not mandatory for the errors to be Gaussian for our statistical inferences in regression to hold. You can appeal to large sample theory, though it’s a little bit more complicated. The variance of our regression slope is actually a highly informative formula. \\[\\sigma_{\\hat \\beta_1}^2 = Var(\\hat \\beta_1) = \\sigma^2 / \\sum_{i=1}^n (X_i - \\bar X)^2\\] This is variance of \\(\\hat \\beta_1\\), showing how variable the points are around the true regression line, \\(\\sigma^2\\), and how variable my X’s are. The numerator, how variable the points are around the regression line, is somewhat understandable as to why that would get better estimates of the regression slope if that were smaller. However, it’s maybe less intuitive to understand why we want more variance in our predictor in order to get lower variance in our regression slope. To understand it imagine a dataset where the regressors, the predictors, are all packed in very tightly, closely together, then it’s clear we’re not going to estimate a very good line. It could sort of bend around that cloud of points very easily and get equivalent fits. # Generate a dataset with more random points around 2.5, 2.5 set.seed(102) num_points &lt;- 25 X &lt;- runif(num_points, min = 2, max = 3) Y &lt;- runif(num_points, min = 2, max = 3) # Plot the dataset plot(X, Y, xlim = c(0, 5), ylim = c(0, 5), xlab = &quot;X&quot;, ylab = &quot;Y&quot;, pch = &quot;+&quot;, col = &quot;red&quot;, cex=1.75) model &lt;- lm(Y ~ I(X)) abline(model, col = &#39;black&#39;,lwd = 2) model &lt;- lm(Y ~ I(-X+5)) abline(model, col = &#39;blue&#39;,lwd = 2) On the other hand, if we spread our axis out, we will get a better fitted regression line with lower variance for the slope. It turns out the lowest you can make that variance is to push half the observations to one end and the other half of the observations to another end; however, you’re banking on having a line in between those two because you haven’t collected any data to evaluate that property. The variance of the intercept, which is maybe a little less informative because intercepts are often a little less of interest than the slopes. \\[\\sigma_{\\hat \\beta_0}^2 = Var(\\hat \\beta_0) = \\left(\\frac{1}{n} + \\frac{\\bar X^2}{\\sum_{i=1}^n (X_i - \\bar X)^2 }\\right)\\sigma^2\\] In practice, \\(\\sigma\\) is replaced by its estimate. It’s probably not surprising that under iid Gaussian errors \\(\\frac{\\hat \\beta_j - \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\) follows a \\(t\\) distribution with \\(n-2\\) degrees of freedom and a normal distribution for large \\(n\\). This can be used to create confidence intervals and perform hypothesis tests. In the following example we demonstrate the formulas we are giving are exactly the formulas that R is using when it performs its calculations. library(UsingR); data(diamond) y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y) beta1 &lt;- cor(y, x) * sd(y) / sd(x) beta0 &lt;- mean(y) - beta1 * mean(x) e &lt;- y - beta0 - beta1 * x sigma &lt;- sqrt(sum(e^2) / (n-2)) ssx &lt;- sum((x - mean(x))^2) seBeta0 &lt;- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma seBeta1 &lt;- sigma / sqrt(ssx) tBeta0 &lt;- beta0 / seBeta0; tBeta1 &lt;- beta1 / seBeta1 pBeta0 &lt;- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE) pBeta1 &lt;- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE) coefTable &lt;- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1)) colnames(coefTable) &lt;- c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;P(&gt;|t|)&quot;) rownames(coefTable) &lt;- c(&quot;(Intercept)&quot;, &quot;x&quot;) coefTable ## Estimate Std. Error t value P(&gt;|t|) ## (Intercept) -259.6259 17.31886 -14.99094 2.523271e-19 ## x 3721.0249 81.78588 45.49715 6.751260e-40 We again use the diamond dataset in the UsingR library. Let’s define the variables y, x, n like before, and \\(\\beta_1 , \\beta_0\\). The residuals are response y minus the predicted values, \\(beta_0 + \\beta_1 * x\\). We create the two t-statistics if you’re testing a hypothesis that \\(\\beta_0\\) is zero or \\(\\beta_1\\) is zero, that is the estimate. Here’s the estimate divided by its standard error. We don’t have to subtract off the true value, because the true value is assumed to be zero under this hypothesis. Next we calculate the two p values. If you’ve taken the inference class, then you know how to go from a t-statistic to a p value. In next step, we create the coefficient table created manually without having done any lm or any built in higher level R function. We specify the row names and column names. However, there is an easy way to do the same thing in R. coefTable ## Estimate Std. Error t value P(&gt;|t|) ## (Intercept) -259.6259 17.31886 -14.99094 2.523271e-19 ## x 3721.0249 81.78588 45.49715 6.751260e-40 fit &lt;- lm(y ~ x); summary(fit)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -259.6259 17.31886 -14.99094 2.523271e-19 ## x 3721.0249 81.78588 45.49715 6.751260e-40 You’ll see everything is exactly the same. Next we want to get a confidence interval for the intercept and the slope. sumCoef &lt;- summary(fit)$coefficients sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2] ## [1] -294.4870 -224.7649 (sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10 ## [1] 355.6398 388.5651 Here we just need the table part of the summary, just the coefficient. With 95% confidence, we estimate that a 0.1 carat increase in diamond size results in a 355.6 to 388.6 increase in price in (Singapore) dollars, which is we estimate that a 0.1 carat increase in diamond size results in a 356 to 389 increase in price in Singapore dollars. 2.3.1 Prediction Prediction is a central concept for the data scientist. In fact, we have an entire course, Practical Machine Learning on advanced prediction techniques. However, regression and generalized linear models which we will cover later on in the course are some of the most core techniques for performing prediction, they often produce very good predictions, they’re parsimonious and interpretable, and as an added bonus we can get inference on top of our predictions without doing any sort of data re sampling. By inference we mean you can get predictions, confidence intervals around the predictions to evaluate the uncertainty in those predictions, so that’s very easy in regression and pretty easy in generalized linear models and quite difficult in some more advanced machine learning algorithms, you may have to do data resampling or other techniques. We might want to predict a response, which might be the price of a diamond at a particular mass, in carats, or we might want to predict a child’s height for a particular value of the parent’s height. The obvious estimate in both cases is just take the \\(X\\), the predictor value multiply it by the relevant estimated slope, \\(\\hat \\beta_1\\) and then add the intercept. The obvious estimate for prediction at point \\(x_0\\) is \\[\\hat \\beta_0 + \\hat \\beta_1 x_0\\] Being a good statisticians requires us to evaluate some uncertainty in the prediction, and it is nice to have a prediction interval. There’s a small intricacy between trying to predict a regression line at a particular point, and trying to predict a future \\(Y\\) at that same point. Those are two different ideas. Line at \\(x_0\\), \\(\\hat \\sigma\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar X)^2}{\\sum_{i=1}^n (X_i - \\bar X)^2}}\\) Prediction interval at \\(x_0\\), \\(\\hat \\sigma\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar X)^2}{\\sum_{i=1}^n (X_i - \\bar X)^2}}\\) What we have here, and it makes sense that our prediction variance first relates around how variable the points are around our regression line, \\(\\hat \\sigma\\) and we have the term \\(\\frac{1}{n}\\) that also kind of makes sense. Typically our standard errors decrease at some rate, \\(\\sqrt{\\frac{1}{n}}\\). If we’re predicting a new \\(Y\\), then we have the added 1 out front, so we get a wider interval. If we want to predict a new value at a specific point versus trying to predict what the regression line is at that point. We will talk more about that later, for now let’s focus on the very end term that on both equations: \\(\\frac{(x_0 - \\bar X)^2}{\\sum_{i=1}^n (X_i - \\bar X)^2}\\) consider the numerator of this statistic our prediction error is going to be the lowest when \\(X\\) not is equal to \\(\\bar X\\) the prediction variance is smallest when we predict at the average mass of a diamond or at the average height of the parents. The denominator is basically how variable the Xs are. The more variable the Xs are, the smaller this term becomes and the lower the prediction error is. Like the slope estimate where the more variable the regressors were, the less variable the slope estimate was. The same thing happens in prediction error, and is an essential part of using regression for prediction, where we get easy and convenient prediction uncertainty associated with the parsimonious predictors. library(ggplot2) newx = data.frame(x = seq(min(x), max(x), length = 100)) p1 = data.frame(predict(fit, newdata= newx,interval = (&quot;confidence&quot;))) p2 = data.frame(predict(fit, newdata = newx,interval = (&quot;prediction&quot;))) p1$interval = &quot;confidence&quot; p2$interval = &quot;prediction&quot; p1$x = newx$x p2$x = newx$x dat = rbind(p1, p2) names(dat)[1] = &quot;y&quot; g = ggplot(dat, aes(x = x, y = y)) g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2) g = g + geom_line() g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4) g In predict function we provide the output of lm. For a lot of prediction algorithms, especially linear models and generalized linear models, random forests in R, the predict function is a generic method that applies to them, interval = (\"confidence\") indicates that we want the confidence interval, not a prediction interval, that’s R’s Code for creating the interval around the estimated line at that particular value of x not for a potential new y at that particular value of x, if we want an interval for potential new Y at that particular value of X, we change the interval = (\"confidence\") to interval = (\"prediction\"), as we do on the fourth line. The blue is the prediction interval, this is for predicting a new line, and the salmon color is for prediction of the line at those particular values of x. The confidence interval is much narrower than the prediction interval. It is because of that 1 plus for the prediction interval. Imagine if we collected an infinite amount of data at all different values of x along this line. Well, then, we would pretty much know the regression line exactly, if that were the case, we would be extremely confident about predictions on the line, where the line was at a particular x value. As we collected more and more data, that salmon colored confidence interval will get narrower and narrower around the line to the point where it was just the line itself. That’s what we would expect to happen. That’s just the idea of statistical sampling working. On the other hand, the prediction interval, there’s variability in the Ys, that has nothing to do with how well we estimated \\(\\beta_0, \\beta_1\\) and in fact, if I were given the correct \\(\\beta_0, \\beta_1\\). There would still be variability in the Ys, because of the error term. Consequently, if we wanted to predict a new y there would be some uncertainty that would be inherent in that prediction. That’s why the prediction interval is always going to be wider than the confidence interval. It doesn’t go away with N. It doesn’t go away as we collect more X’s or anything like that. It’s inherent, and that’s why the prediction interval has a certain amount of width that’s never going to go away. The last thing as you may notice both of the intervals get narrower toward the center of the data cloud and then get wider as you head out into the tails. That’s just simply saying that we’re more confident in our predictions closer to the mean of the X’s. Because of that one plus term in prediction intervals this phenomena is less obvious in blue color than the salmon one. If we were to go well beyond where we collected data, then these intervals would really become a lot wider which is what we’d want, because we would be extrapolating and we want to predict where we did not collect data. Summary Both intervals have varying widths. Least width at the mean of the Xs. We are quite confident in the regression line, so that interval is very narrow. If we knew \\(\\beta_0\\) and \\(\\beta_1\\) this interval would have zero width. The prediction interval must incorporate the variabilibity in the data around the line. Even if we knew \\(\\beta_0\\) and \\(\\beta_1\\) this interval would still have width. 2.4 For the project You need to know a little bit of knitr. In this video, which you may have to refer back to when you start the project, will get you started on knitr. In this section we will learn how to use knitr to create reproducible reports. We will also learn how to use R Markdown to create reproducible documents. You’ll need a little bit of knitr to create your R project. We open the go File&gt; New File&gt; R Markdown, this will populate a simple knitr document. Here we can run R commands in a code block, which is defined as three right tick marks followed by {r} if you insert a comma after the r you will open up a bunch of options, cache tells R whether or not to keep it, eval= tells whether or not it should evaluate the code and echo= where echo TRUE shows the code and echo FALSE does not show the code. Once you are done with the document click Knit HTML and it will knit and create an HTML document. That’s just a standard HTML document and you can bring up the document in a browser window. And that’s knitr in a nutshell. 2.5 Practical R Exercises in swirl During this week of the course you should complete the following lessons in the Regression Models swirl course: Residual Variation Introduction to Multivariable Regression MultiVar Examples 2.6 Week 2 Quiz Consider the following data with x as the predictor and y as as the outcome. Give a P-value for the two sided hypothesis test of whether \\(β_1\\) from a linear regression model is 0 or not. x &lt;- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62) y &lt;- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36) Consider the previous problem, give the estimate of the residual standard deviation. In the mtcars data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint? Refer to the previous question. Read the help file for mtcars. What is the weight coefficient interpreted as? Consider again the mtcars data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint? Consider again the mtcars data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint. If my X from a linear regression is measured in centimeters and I convert it to meters what would happen to the slope coefficient? I have an outcome, Y and a predictor, X and fit a linear regression model with \\(Y = β_0 + β_1 * X + ϵ\\) to obtain \\(\\hat β_0, \\hat β_1\\) . What would be the consequence to the subsequent slope and intercept if I were to refit the model with a new regressor, \\(X + c\\) for some constant \\(c\\)? Refer back to the mtcars data set with mpg as an outcome and weight (wt) as the predictor. About what is the ratio of the the sum of the squared errors, \\(\\sum_{i=1}^n (Y_i - \\hat Y_i)^2\\) when comparing a model with just an intercept (denominator) to the model with the intercept and slope (numerator)? Do the residuals always have to sum to 0 in linear regression? ## Installing package into &#39;/usr/local/lib/R/site-library&#39; ## (as &#39;lib&#39; is unspecified) "],["week-03.html", "Chapter 3 Week 03 3.1 Multi-variable regression 3.2 Multi-variable regression tips and tricks 3.3 Adjustment 3.4 Residuals again 3.5 Model selection 3.6 Practical R Exercises in swirl 3.7 Week 3 Quiz 3.8 (OPTIONAL) Practice exercise in regression modeling", " Chapter 3 Week 03 3.1 Multi-variable regression We now extend linear regression so that our models can contain more variables. A natural first approach is to assume additive effects, basically extending our line to a plane, or generalized version of a plane as we add more variables. Multi-variable regression represents one of the most widely used and successful methods in statistics. If you’re utilizing predictor X to forecast a response Y and discover a meaningful relationship, there’s a potential issue if the predictor hasn’t been randomly assigned to the subjects or units being observed. In such cases, there’s always a concern that there might be another variable, whether known or unknown, that could account for the observed relationship. For example, imagine if you had a friend who downloaded some data, where they had all sorts of health information from people and also their dietary information. This person claims to have found an interesting relationship: breath mint usage has a significant regression relationship with forced expiratory volume(FEV), a measure of lung function. You would be skeptical there’s very little basis for a biological relationship there. Breath mints are just sugar! But maybe, but what you’ve really be thinking is what other variables might explain this relationship? You might have two hypotheses: this person dug through lots and lots of variables and just found the one that was significant, and it’s just a chance of association, which is the problem of multiplicity. In addition it is likely, you would think the real problem is smokers tend to use more breath mints, and smoking has this relationship with lung function. It’s well-established that chronic exposure to a smoker, even second-hand smoke has negative impacts on lung function. So it’s probably smoking it probably has nothing to do with the breath mints, it’s a indirect effect of breath mints through smoking, not a direct effect of breath mints on lung function. This would be the hypothesis. To establish that there’s a breath mint effect beyond smoking we could consider smokers by themselves, and see whether their lung function differs by their breath mint usage, and consider non-smokers by themselves, and see whether their lung function differs by breath mint usage, where we conditioned on smoking status. This way we would compare like with like. Multivariable regression is sort of automated way to do that in a linear fashion. It makes fair enough assumptions, in automated way. In this section we will explain how it works and we will also talk a little bit about its limitations. Multivariable regression is trying to look at the relationship of a predictor and a response, while having, at some level, accounted for other variables. Moreover, multivariable regression is actually a good prediction model. For example, a Kaggle competition wanted to predict the number of days a person would be in the hospital in subsequent years given their claims history and number of days they were in the hospital in previous years. The insurance companies seek to harness an extensive dataset derived from claims, aiming to predict a singular numerical outcome. However, the conventional approach of simple linear regression would be insufficient when confronted with multiple predictors. How can we extend the scope of simple linear regression to accommodate a multitude of regressors for predictive purposes? The procedure is similar to simple linear regression where there’s more predictor terms, X values. For example, \\(X_1\\) might be the number of insurance claims in the previous year, and \\(X_2\\) might be whether or not the person had a particular cardiac problem, and so on. The first variable is typically just a constant one, so there’s an intercept that’s included, a term that’s just \\(\\beta_0\\) by itself. Interestingly in this competition, we found that multivariable regression could get people very close to the winning entry, while other machine learning methods like random forest, and boosting only improved the results minorly on top of multivariable regression. Note: in case of breath mint study, one of the predictors, \\(X_1\\) might be breath mint usage (a binary variable), and \\(X_2\\) might be how much a person smoked. The general linear model extends simple linear regression (SLR) by adding terms linearly into the model. \\[ Y_i = \\beta_0 X_{0i} + \\beta_1 X_{1i} + \\ldots + \\beta_{p} X_{pi} + \\epsilon_{i} = \\sum_{k=0}^p X_{ik} \\beta_j + \\epsilon_{i} \\] Where \\(X_{1i}=1\\) typically, the \\(\\beta_j\\) are the coefficients of the model. Least squares (and hence ML estimates under iid Gaussianity of the errors) minimizes \\[ \\sum_{i=1}^n \\left(Y_i - \\sum_{k=1}^p X_{ki} \\beta_j\\right)^2 \\] Note, the important linearity is linearity in the coefficients. Thus \\[ Y_i = \\beta_1 X_{1i}^2 + \\beta_2 X_{2i}^2 + \\ldots + \\beta_{p} X_{pi}^2 + \\epsilon_{i} \\] is still a linear model. (We’ve just squared the elements of the predictor variables.) 3.1.1 How to get the coefficients, derivation of formulas Here we will go through the derivation of formulas to show how the least squares estimates are obtained. This derivation is not required for the course, but it may be helpful for those who are interested in understanding how the estimates are obtained. Just to review, if you have regression to the origin, you want a line that’s forced to the origin that has no intercepts. You have the single predictor \\(X\\) and a single predictor of \\(Y\\) and you want no intercept, \\(E[Y_i]=X_{1i}\\beta_1\\). The slope estimate was \\(\\sum X_i Y_i / \\sum X_i^2\\). Now lets try to derive the least squares estimate when we have two regressors, which can be generalized to models with more variables. In \\(E[Y_i] = X_{1i}\\beta_1 + X_{2i}\\beta_2 = \\mu_i\\), Least squares tries to minimize: \\[ \\sum_{i=1}^n (Y_i - X_{1i} \\beta_1 - X_{2i} \\beta_2)^2 \\] Here we try to give a development that is more intuitive than what you would get with something like linear algebra. \\[\\Sum(y_i - X_{0i} \\beta_0 - X_{1i} \\beta_1\\] Imagine we knew \\(\\beta_1\\) or fix \\(\\beta_1\\), then we can write \\(\\tilde y_i = y_i - x_{0i} \\beta_0\\) and subsequently \\(\\Sum(\\tilde y_i - X_{1i} \\beta_1\\). This is exactly regression through the origin with just the single regressor. So we can write \\(\\beta_1 = \\sum \\tilde y_i X_{1i} / \\sum X_{1i}^2\\). Now we can plug this back into the original equation and we get: \\[ \\sum_{i=1}^n (Y_i - X_{1i} \\beta_1 - X_{2i} \\sum \\tilde y_i X_{1i} / \\sum X_{1i}^2)^2 \\] This is an equation that only involves \\(\\beta_0\\) and a regression through the origin for \\(\\beta_0\\). What it works out to be, and this is the interesting part, is that the regression slope for \\(\\beta_0\\), is exactly what you would obtain if you took the residual of \\(X_1\\) out of \\(X_0\\), and \\(X_1\\) out of \\(Y\\) and then just did regression to the origin. Multivariable regression calculates the coefficient for \\(X_0\\), \\(\\beta_0\\), as if you had removed the effect of \\(X_1\\) from both \\(Y\\) and \\(X_0\\). Similarly, the regression coefficient for \\(X_1\\), \\(\\beta_1\\), is what you would get if you were to remove the effect of \\(X_0\\) from both \\(Y\\) and \\(X_1\\). This is why multivariable regression is thought of as having adjusted for the other variables. A coefficient from a multivariable regression is the coefficient where the linear effect of all the other variables on that predictor and response has been removed. 3.1.2 Results In \\(E[Y_i] = X_{0i}\\beta_0 + X_{1i}\\beta_1\\), we have two covariates, \\(X_1 , X_2\\). \\[\\hat \\beta_0 = \\frac{\\sum_{i=1}^n e_{i, Y | X_1} e_{i, X_0 | X_1}}{\\sum_{i=1}^n e_{i, X_0 | X_1}^2}\\] \\(\\beta_0\\) is what you would get with regression through the origin if you removed the second coefficient \\(X_1\\). Similarly, the same thing could be said about the coefficient for \\(X_1 \\beta_1\\). \\(\\hat \\beta_1\\) is the linear regression where linear effect of \\(X_0\\) out of both the response \\(Y\\), and the second predictor, \\(X_1\\). This is why multivariable regression relationships are considered as having been adjusted for all the other variables. 3.1.3 Example with two variables, simple linear regression \\(Y_{i} = \\beta_0 X_{0i} + \\beta_1 X_{1i}\\) where \\(X_{0i} = 1\\) is an intercept term. Notice the fitted coefficient of \\(X_{1i}\\) on \\(Y_{i}\\) is \\(\\bar Y\\). The residuals are \\(e_{i, Y | X_1} = Y_i - \\bar Y\\). Thus the fitted coefficient of \\(X_{1i}\\) on \\(X_{0i}\\) is \\(\\bar X_1\\), which is the residuals \\(e_{i, X_0 | X_1}= X_{0i} - \\bar X_0\\). We can write: \\[ \\hat \\beta_1 = \\frac{\\sum_{i=1}^n e_{i, Y | X_0} e_{i, X_1 | X_0}}{\\sum_{i=1}^n e_{i, X_1 | X_0}^2} = \\frac{\\sum_{i=1}^n (X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n (X_i - \\bar X)^2} = Cor(X, Y) \\frac{Sd(Y)}{Sd(X)} \\] 3.1.4 The general case More generally, multivariate regression estimates are exactly those having removed the linear relationship of the other variables from both the regressor and response. Least squares solutions have to minimize\\[\\sum_{i=1}^n (Y_i - X_{1i}\\beta_1 - \\ldots - X_{pi}\\beta_p)^2\\]. The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals. In this sense, multivariate regression “adjusts” a coefficient for the linear impact of the other variables. 3.1.5 Examples with multiple-variables In the following simulation we have 100 observations and want to generate three predictors, x, x2, x3, where they are all just standard normal. When we write y = 1 + x + x2 + x3, all my coefficients are 1, meaning the population model used for simulation, they’re all 1. Next we add some random noise, that’s the error term. n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n) y = 1 + x + x2 + x3 + rnorm(n, sd = .1) ey = resid(lm(y ~ x2 + x3)) ex = resid(lm(x ~ x2 + x3)) sum(ey * ex) / sum(ex ^ 2) coef(lm(ey ~ ex - 1)) coef(lm(y ~ x + x2 + x3)) Here we want to point out, coef(lm(ey ~ ex - 1)) is the same coefficient as if we regress y on x, x2 and x3, and an intercept coef(lm(y ~ x + x2 + x3)). You see the x term here is exactly the same as the regression through the origin estimate with the residuals. 3.1.6 Interpretation of coefficients The regression predictor, given the collection of covariants take a specific value, \\(x_1\\) to \\(x_p\\), is just the sum of the \\(x_k\\beta_k\\). \\[E[Y | X_1 = x_1, \\ldots, X_p = x_p] = \\sum_{k=1}^p x_{k} \\beta_k\\] If one of the predictors, say \\(X_1\\), is incremented by 1 i.e. \\(X_1\\) instead of \\(x_1\\) takes \\(x_1+1\\), then the regression coefficient \\(\\beta_1\\) is the expected change in the response. \\[ E[Y | X_1 = x_1 + 1, \\ldots, X_p = x_p] = (x_1 + 1) \\beta_1 + \\sum_{k=2}^p x_{k} \\beta_k \\] If we subtract the two terms the expected value of the response from the responce where the first co-efficient takes the value of \\(x_1 +1\\) works out to be \\(\\beta_1\\). \\[ E[Y | X_1 = x_1 + 1, \\ldots, X_p = x_p] - E[Y | X_1 = x_1, \\ldots, X_p = x_p]\\] \\[= (x_1 + 1) \\beta_1 + \\sum_{k=2}^p x_{k} \\beta_k + \\sum_{k=1}^p x_{k} \\beta_k = \\beta_1 \\] Notice all the other \\(x_2\\) to \\(x_p\\) were held fixed, the interpretation of a multivariate regression coefficient is the expected change in the response per unit change in the regressor, holding all of the other regressors fixed. The basic components of the linear models are exactly the same as in simple linear regression. Model \\(Y_i = \\sum_{k=1}^p X_{ik} \\beta_{k} + \\epsilon_{i}\\) where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) Fitted responses \\(\\hat Y_i = \\sum_{k=1}^p X_{ik} \\hat \\beta_{k}\\) Residuals \\(e_i = Y_i - \\hat Y_i\\) Variance estimate \\(\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^n e_i ^2\\) (note the \\(n-p\\) degrees of freedom) To get predicted responses at new values, \\(x_1, \\ldots, x_p\\), simply plug them into the linear model \\(\\sum_{k=1}^p x_{k} \\hat \\beta_{k}\\) Coefficients have standard errors, \\(\\hat \\sigma_{\\hat \\beta_k}\\), and \\(\\frac{\\hat \\beta_k - \\beta_k}{\\hat \\sigma_{\\hat \\beta_k}}\\) follows a \\(T\\) distribution with \\(n-p\\) degrees of freedom. Predicted responses have standard errors and we can calculate predicted and expected response intervals. These should all be pretty familiar because they’re basically the same as what we did for linear aggression, the difference is we have more terms now. Remember in linear aggression we had two terms, we had an intercept and a covariant now we’re just adding more covariants potentially. One point to note is that the variance estimate is not quite the same as the average squared residuals. In linear regression we divided by \\(n-2\\), now we divide by \\(n-p\\). That’s kind of a technical point because if you know \\(n-p\\) of the residuals you implicitly know the last \\(p\\) of them due to some linear constraints. That’s a minor point you can think of the residuals variants estimate is nothing other than the average square residuals for the most part with \\(N-p\\) part not withstanding. In a sense all the things we knew about from linear regression carryover to multi-variable regression. To end this section, we want to emphasize how important linear models are to the data scientist. Before you do any machine learning or any complex algorithm, linear models should be your first attempt. They offer parsimonious and well understood easily describe relationships between predictors and response. There are some modern machine learning algorithms that can beat some of the properties of linear models, like the imposed linearity. Nonetheless, linear models should always be your starting point. There’s some amazing things you can do with linear models that you may not think that would be possible. For example, you can take a time series like a music sound or something like that, and decompose it into its harmonics. This is so-called discrete Fourier transform can be thought of the as the fit from a linear model. You can flexibly fit rather complicated functions and curves and things like that using linear models. You can fit factor variables as predictors. ANOVA and ANCOVA are special cases of linear models. You can uncover complex multivariate relationships within a response and you can build fairly accurate prediction models. 3.2 Multi-variable regression tips and tricks Let’s start this discussion with the famous Swiss Fertility Data. Using the following command you can load the data and see the documentation. require(datasets); data(swiss); ?swiss The data shows standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. A data frame with 47 observations on 6 variables, each of which is in percent, i.e., in [0, 100]. The variables are: [,1] Fertility a common standardized fertility measure [,2] Agriculture % of males involved in agriculture as occupation [,3] Examination % draftees receiving highest mark on army examination [,4] Education % education beyond primary school for draftees [,5] Catholic % catholic (as opposed to protestant) [,6] Infant.Mortality live births who live less than 1 year All variables but Fertility give proportions of the population. Visualizing some of the basic scatter plots is always a good practice. library(GGally) ## Loading required package: ggplot2 ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 g &lt;- ggpairs( swiss, lower = list(continuous = &quot;smooth&quot;), wrap = function(...) { ggally_smooth(..., method = &quot;loess&quot;) } ) ## Warning in warn_if_args_exist(list(...)): Extra arguments: &#39;wrap&#39; are being ## ignored. If these are meant to be aesthetics, submit them using the &#39;mapping&#39; ## variable within ggpairs with ggplot2::aes or ggplot2::aes_string. g In this plot you see fertility is on the x-axis for all the plots in the first column, agriculture is on the x-axis for all of the plots in the second column. Agriculture is also on the y-axis for the first graph. In addition, the corresponding upper triangular part of the matrix gives the correlation between the two variables. For example, fertility and agriculture the relation turns out to be fairly linear with confidence prediction band around it. The correlation between the two is 0.35. Let’s investigate the relationship where agriculture, the percent of the province that works in the agricultural industry, with fertility. summary(lm(Fertility ~ . , data = swiss))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.9151817 10.70603759 6.250229 1.906051e-07 ## Agriculture -0.1721140 0.07030392 -2.448142 1.872715e-02 ## Examination -0.2580082 0.25387820 -1.016268 3.154617e-01 ## Education -0.8709401 0.18302860 -4.758492 2.430605e-05 ## Catholic 0.1041153 0.03525785 2.952969 5.190079e-03 ## Infant.Mortality 1.0770481 0.38171965 2.821568 7.335715e-03 Tilde period in lm function is a shorthand for all the other variables in the data frame. The output of the summary function gives the coefficients of the model. The first column gives the estimated coefficients, the second column gives the standard errors of the coefficients, the third column gives the t-statistics, and the fourth column gives the p-values. The p-values are the probability of observing a t-statistic as extreme as the one observed, if the true coefficient were 0. The number \\(-0.17\\) in the Agriculture variable row is interpreted as: we expect a 0.17 decrease, in standardized fertility for every 1% increase in the percentage of males involved in agriculture, holding the other variables constant. Meaning we hold examination and education, percent Catholic and infant mortality constant. The next column, the standard error 0.07, talks about how precise that coefficient is. It talks about the statistical variability of that coefficient. If we wanted to perform a hypothesis test, we would take the estimate, subtract off the hypothesized value, which in this case is zero, and divide it by the standard error of the estimate. Which is the definition of T-statistic. R conveniently provides it to us, -2.448. We can calculate the probability of getting a t-statistic as extreme as that. As small as negative 2.448 or smaller, and because we’re doing a two-sided test, we would double that p-value. The degrees of freedom are \\(n - #coefficients\\), including the intercept. But again, R does that on our behalf, and that works out to be 0.018. By standard thresholding rules, type one error rate of say 5%, that would be statistically significant. In the following section we will see how the process of model selection changes the estimates. We start by contrasting the model with a model that just has agriculture as predictor, the previous model had all the other variables in this predictor. summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.3043752 4.25125562 14.185074 3.216304e-18 ## Agriculture 0.1942017 0.07671176 2.531577 1.491720e-02 The agriculture variable is about the same magnitude, 0.19 instead of 0.17 but with changed signs. Instead of agriculture having a negative effect on fertility, it has a positive effect on fertility. Adjusting for the other variables changes the actual direction of the effect of agriculture on fertility. This is the impact of something so-called Simpson’s Paradox. Notice in both cases the agriculture coefficient is strongly statistically significant. We would like to create (via simulation) an example where an effect can reverse itself. It can help us understand Simpson’s paradox could happen. Keep in mind, regression is a dynamic process, where you have to think about what variables to include and why. If there hasn’t been randomization to protect you from confounding, you have to go through a scientific dynamic process of putting confounders in and out and thinking about what they’re doing to your effective interest in order to evaluate it. n &lt;- 100; x2 &lt;- 1 : n; x1 &lt;- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01) plot(x1) summary(lm(y ~ x1))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.285371 1.188274 1.923269 5.734884e-02 ## x1 95.542492 2.052955 46.539011 1.249965e-68 summary(lm(y ~ x1 + x2))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.001300827 0.0019747025 -0.6587457 5.116198e-01 ## x1 -0.991610625 0.0162560952 -60.9993120 3.570744e-79 ## x2 0.999940168 0.0001647753 6068.5062647 2.027095e-272 The second regressor, \\(x_2\\), is the values \\(1-n\\), \\(x_1\\) is a variable that depends on \\(x_2\\) and random noise. Think of \\(x_2\\) as something we might measure regularly, like days, and \\(x_1\\) as something like a saving account where the balance goes up with time and random fluctuations. The random fluctuations impact the spending, so the money doesn’t necessarily always just go up. It goes up and down sporadically, but the linear trend is going up. Let’s assume y is happiness with a measure like y = -x1 + x2 + noise. The true generating model y is negatively associated with -x1 suggesting happiness is negatively associated with money and positively associated with x2, so it goes up with time and down with x1 with some random normal noise. We know from the model y = -x1 + x2 + noise the outcome depends negatively on x1 with a coefficient of minus 1, and depends positively on x2 with a coefficient of plus 1. If fit x1 by itself we get an enormous coefficient, 95, which is clearly wrong. It’s nothing near to the negative 1 that it’s supposed to be or that we would hope it would be. It is picking up the residual effect of x2 that’s a big driver of y, but when we fit the correct model, x1 and x2, together we will get the correct coefficients, about minus 1 for x1, and about plus 1 for x2. You can imagine why this would happen by answering: what is regression doing? It’s taking x1 and removing the linear effect of x2. Let’s do some plots to highlight this, just to show us how it works a little bit. dat = data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2))) library(ggplot2) g = ggplot(dat, aes(y = y, x = x1, colour = x2)) g = g + geom_point(colour=&quot;grey50&quot;, size = 5) + geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) g = g + geom_point(size = 4) g ## `geom_smooth()` using formula &#39;y ~ x&#39; There is a clear positive linear relationship, between the x1 and y. However, with x2, which is the color, there’s also clear positive gradient. As y goes up, so does x2. And also you can see as x1 goes up, so does x2. So you can see the confounding that’s happening here. g2 = ggplot(dat, aes(y = ey, x = ex1, colour = x2)) g2 = g2 + geom_point(colour=&quot;grey50&quot;, size = 5) + geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + geom_point(size = 4) g2 ## `geom_smooth()` using formula &#39;y ~ x&#39; If we plot the residuals you can see that for the residual y and the residual x1, there’s a clear negative linear relationship, and if you stare at it enough, you realize that the slope of this line should be around negative 1. You can also see that the x2 variable is clearly not related to the residual x1 variable. It is important to remember the above explanation doesn’t mean that throwing every variable into your regression model is the right thing to do. There’s consequences to throwing in unnecessary variables. It can make your model less interpretable, it can make your model less stable, and it can make your model less generalizable. It’s important to think about what variables you’re putting in and why. In the earlier example about Swiss data the agriculture effect reversed itself after we included the other variables in the model. You will find that this happens quite a bit when education and examination are included. Educational attainment is negatively correlated with the percent working in agriculture, a correlation of -0.64. In addition, education and examination are kind of measuring the same thing. Their correlation, those two variables is 0.7. The percent of males in the province working in agriculture is negatively related to educational attainment (correlation of -0.6395225) and Education and Examination (correlation of 0.6984153) are obviously measuring similar things. The question is: is the positive marginal an artifact for not having accounted for, say, Education level? (Education does have a stronger effect, by the way.) At the minimum, anyone claiming that provinces that are more agricultural have higher fertility rates would immediately be open to criticism. Notice What if we include an unnecessary variable? Here we introduce z which adds no new linear information, since it’s a linear combination of variables already included. R just drops terms that are linear combinations of other terms. z &lt;- swiss$Agriculture + swiss$Education lm(Fertility ~ . + z, data = swiss) ## Warning in terms.formula(formula, data = data): &#39;varlist&#39; has changed (from ## nvar=6) to new 7 after EncodeVars() -- should no longer happen! ## ## Call: ## lm(formula = Fertility ~ . + z, data = swiss) ## ## Coefficients: ## (Intercept) Agriculture Examination Education ## 66.9152 -0.1721 -0.2580 -0.8709 ## Catholic Infant.Mortality z ## 0.1041 1.0770 NA 3.2.1 Dummy variables are smart You might be surprised to find out how flexible linear regression models are. For example, you can fit factor variables as regressors and come up with things like analysis of variance as a special case of linear models. Consider the linear model \\(Y_i = \\beta_0 + X_{i1} \\beta_1 + \\epsilon_{i}\\) where each \\(X_{i1}\\) is binary so that it is a 1 if measurement \\(i\\) is in a group and 0 otherwise. (Treated versus not in a clinical trial, for example.) The estimated mean for the treated group is the mean of the people who are treated. Then for people in the treated group we can write \\(E[Y_i] = \\beta_0 + \\beta_1\\). \\(\\beta_1\\) is interpreted as the increase, or decrease if it’s negative, in the mean response for those that were treated. Similarly for people without treatment we have \\(E[Y_i] = \\beta_0\\). The LS fits work out to be \\(\\hat \\beta_0 + \\hat \\beta_1\\) is the mean for those in the group and \\(\\hat \\beta_0\\) is the mean for those not in the group. You see that linear regression provides the fitted values and tell you about the means for both of the groups, in addition it gives you an inference for comparing the two groups automatically. Note including a binary variable that is 1 for those not in the group would be redundant. It would create three parameters to describe two means. We can generalize this to more than two groups. If we have a three-level variable, we can create two binary variables, one for each level, and then we can compare the means of the three groups. For example, imagine you have some outcome but you want to compare it to U.S. political party affiliation. In this case, let’s say you were only considering those who were Democrats, Republicans, or registered Independents. Well, you can do that by having a variable X1, that’s one for Republicans and zero for otherwise, a variable X2 that’s one for Democrats and zero for otherwise, we omit the X3 for Independents because of redundancy. If we know that you’re not Republican and not a Democrat, then you must be an Independent in our data set the way we’ve set things up and having a third variable wouldn’t have any new information. \\[Y_i = \\beta_0 + X_{i1} \\beta_1 + X_{i2} \\beta_2 + \\epsilon_i\\] The mean for the three groups are: * If \\(i\\) is Republican \\(E[Y_i] = \\beta_0 +\\beta_1\\) * If \\(i\\) is Democrat \\(E[Y_i] = \\beta_0 + \\beta_2\\). * If \\(i\\) is Independent \\(E[Y_i] = \\beta_0\\). If we compare the means like \\(\\beta_0\\) the mean for the Independents versus \\(\\beta_0 +\\beta_1\\) the mean for the Republicans, i.e. subtract those two, we get \\(\\beta_1\\). Which means \\(\\beta_1\\) compares Republicans to Independents, and similarly \\(\\beta_2\\) compares Democrats to Independents and \\(\\beta_1 - \\beta_2\\) compares Republicans to Democrats. By omitting the regression variable for the Independents, the intercept became the value for the Independents, and all of the other coefficients have become interpreted relative to Independents and shows choice of reference category changes the interpretation. If we had included the regressor for Independents and excluded the one for Republicans, then the intercept would be for Republicans, and the coefficient in front of the Democratic would be Democrats versus Republicans. The coefficient in front of the Independent would be Independent versus Republican. To illustrate how this works we move to R. In this example we look at a factor variable and see how R is treating it the dataset is InsectSprays and we’re going to fit a linear model to it. require(datasets);data(InsectSprays); require(stats); require(ggplot2) g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill = spray)) g = g + geom_violin(colour = &quot;black&quot;, size = 2) g = g + xlab(&quot;Type of spray&quot;) + ylab(&quot;Insect count&quot;) g Y is the count, the number of insects, X is the spray. We use a violin plot to show the data, which is kind of like a histogram but sort of tilted on its side and repeated on both sides so it looks a little like a violin. It looks like a violin if you’re data cooperates, otherwise, it looks like a blob. As you can see there are eight spray A, B, C, D, E, and F and you can see the insect counts. It’s unfortunate they’re not telling us whether or not the count is the count of the number of alive or the number dead insects. So we don’t know if this is a better spray or a worse spray. However, we still can test the difference between different factor levels in this case using linear models. Wwhen we include insect spray as a linear model and y as an outcome. summary(lm(count ~ spray, data = InsectSprays))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.5000000 1.132156 12.8074279 1.470512e-19 ## sprayB 0.8333333 1.601110 0.5204724 6.044761e-01 ## sprayC -12.4166667 1.601110 -7.7550382 7.266893e-11 ## sprayD -9.5833333 1.601110 -5.9854322 9.816910e-08 ## sprayE -11.0000000 1.601110 -6.8702352 2.753922e-09 ## sprayF 2.1666667 1.601110 1.3532281 1.805998e-01 We get the Intercept, spray B, spray C, spray D, spray E, and spray F, notice that spray A is conspicuously missing. The idea is that everything here is in comparison with spray A. So, 0.833 is the change in the mean between spray B and spray A. In this case, 14.5 is the mean for spray A. You can double check that by looking at the plot. Spray B seems reasonable affected by a little bit from spray A, whereas spray C looks like it’s affected a lot it has a coefficient of \\(-12\\). If we wanted to compare spray B and spray C we would have to look at \\(0.833 - (-12.416)\\). We wouldn’t have a standard error for that comparison immediately. However, that would give us the estimate. If we were to take the average count for the sprays, for those with spray A, we would get 14.5 with spray B we would get \\(14.5 + 0.833\\). What R does is it picks the spray level that’s the lowest alphanumerically, in this case, spray level A, to set as the reference level. Here we like to show you how you can hard code the same model and not rely on R to pick the reference level. summary(lm(count ~ I(1 * (spray == &#39;B&#39;)) + I(1 * (spray == &#39;C&#39;)) + I(1 * (spray == &#39;D&#39;)) + I(1 * (spray == &#39;E&#39;)) + I(1 * (spray == &#39;F&#39;)) , data = InsectSprays))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.5000000 1.132156 12.8074279 1.470512e-19 ## I(1 * (spray == &quot;B&quot;)) 0.8333333 1.601110 0.5204724 6.044761e-01 ## I(1 * (spray == &quot;C&quot;)) -12.4166667 1.601110 -7.7550382 7.266893e-11 ## I(1 * (spray == &quot;D&quot;)) -9.5833333 1.601110 -5.9854322 9.816910e-08 ## I(1 * (spray == &quot;E&quot;)) -11.0000000 1.601110 -6.8702352 2.753922e-09 ## I(1 * (spray == &quot;F&quot;)) 2.1666667 1.601110 1.3532281 1.805998e-01 Here count is the outcome and we create a variable using the I function which performs the operation inside the regression, inside the model statement. We look at the instances where the spray is equal to B then multiply that by 1 to change it from Boolean to numeric and do the same for the other sprays and add them all together. In this example we included all of the sprays except A which means we forced A to be the reference level. The result is identical to R picking the reference level as we expected. What happens if we include spray A? summary(lm(count ~ I(1 * (spray == &#39;B&#39;)) + I(1 * (spray == &#39;C&#39;)) + I(1 * (spray == &#39;D&#39;)) + I(1 * (spray == &#39;E&#39;)) + I(1 * (spray == &#39;F&#39;)) + I(1 * (spray == &#39;A&#39;)), data = InsectSprays))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.5000000 1.132156 12.8074279 1.470512e-19 ## I(1 * (spray == &quot;B&quot;)) 0.8333333 1.601110 0.5204724 6.044761e-01 ## I(1 * (spray == &quot;C&quot;)) -12.4166667 1.601110 -7.7550382 7.266893e-11 ## I(1 * (spray == &quot;D&quot;)) -9.5833333 1.601110 -5.9854322 9.816910e-08 ## I(1 * (spray == &quot;E&quot;)) -11.0000000 1.601110 -6.8702352 2.753922e-09 ## I(1 * (spray == &quot;F&quot;)) 2.1666667 1.601110 1.3532281 1.805998e-01 Notice it gives an NA in front of the spray A coefficient. We have six means, for six sprays and seven parameters in intercept. If we do want the coefficients, instead of being interpreted as levels referenced to a control level be the mean for each of the groups? Well, we can do that by removing the intercept. summary(lm(count ~ spray - 1, data = InsectSprays))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## sprayA 14.500000 1.132156 12.807428 1.470512e-19 ## sprayB 15.333333 1.132156 13.543487 1.001994e-20 ## sprayC 2.083333 1.132156 1.840148 7.024334e-02 ## sprayD 4.916667 1.132156 4.342749 4.953047e-05 ## sprayE 3.500000 1.132156 3.091448 2.916794e-03 ## sprayF 16.666667 1.132156 14.721181 1.573471e-22 library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union summarise(group_by(InsectSprays, spray), mn = mean(count)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 × 2 ## spray mn ## &lt;fct&gt; &lt;dbl&gt; ## 1 A 14.5 ## 2 B 15.3 ## 3 C 2.08 ## 4 D 4.92 ## 5 E 3.5 ## 6 F 16.7 Here count is the outcome and spray is the predictor, but we remove the intercept. Notice we get a different set of coefficients, one for each spray level. It includes A, B, C, D, E and F without dropping any levels. It can do that because it has six parameters, and six means to work with. The coefficients are exactly equal to the means for each spray in the data. If calculated the means for each spray it would work out to be the same numbers. We want to emphasize this model is no different than the previous model that included an intercept, and just the coefficients have a different interpretation. If we add these together, 14.5 and 0.833 from the model with intercept we should get the mean for spray B, which is the case. In the model with the intercept, the intercept is interpreted as the spray A mean and all the coefficients are interpreted as relative to spray A differences from spray A and when we fit the data without the intercept we get the mean for each spray. The p values are testing whether or not, A is different from B, and A is different from C, and A is different from D, and so on, whereas the p values from the model without intercept are testing whether or not those means are different from 0, which is a very different test. We were trying to illustrate how you play around with factor variables in lm is very important in terms of how you interpret it. It’s not just a conceptual or theoretical thing to worry about it is a very practical thing. What your intercept means changes dramatically depending on what your reference level is. One more thing that we want to discuss is the idea of re-leveling. You can re-level to have differenct reference level. For example, if you want to compare spray C to spray A, you can re-level the spray variable to have C as the reference level. spray2 &lt;- relevel(InsectSprays$spray, &quot;C&quot;) summary(lm(count ~ spray2, data = InsectSprays))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.083333 1.132156 1.840148 7.024334e-02 ## spray2A 12.416667 1.601110 7.755038 7.266893e-11 ## spray2B 13.250000 1.601110 8.275511 8.509776e-12 ## spray2D 2.833333 1.601110 1.769606 8.141205e-02 ## spray2E 1.416667 1.601110 0.884803 3.794750e-01 ## spray2F 14.583333 1.601110 9.108266 2.794343e-13 3.2.2 Summary of the InsectSprays example If we treat Spray as a factor, R includes an intercept and omits the alphabetically first level of the factor. All t-tests are for comparisons of Sprays versus Spray A. Empirical mean for A is the intercept. Other group means are the itc plus their coefficient. If we omit an intercept, then it includes terms for all levels of the factor. Group means are the coefficients. Tests are tests of whether the groups are different than zero. (Are the expected counts zero for that spray.) If we want comparisons between, Spray B and C, say we could refit the model with C (or B) as the reference level. Note We want to make a few points about the InsectSprays dataset, which we believe is important for the data scientist. Counts are bounded from below by 0, violates the assumption of normality of the errors. Also there are counts near zero, so both the actual assumption and the intent of the assumption are violated. Variance does not appear to be constant. Perhaps taking logs of the counts would help. There are 0 counts, so maybe log(Count + 1) Also, we’ll cover Poisson GLMs for fitting count data. In this section we go through an example that underlies the topic of so called ANCOVA. In this example we will fit multiple lines with different intercepts and different slopes. We will use the swiss dataset, recall we’re trying to model fertilities as a linear function of agriculture, which is the percent of that province that was working in agriculture. library(datasets); data(swiss) head(swiss) ## Fertility Agriculture Examination Education Catholic ## Courtelary 80.2 17.0 15 12 9.96 ## Delemont 83.1 45.1 6 9 84.84 ## Franches-Mnt 92.5 39.7 5 5 93.40 ## Moutier 85.8 36.5 12 7 33.77 ## Neuveville 76.9 43.5 17 15 5.16 ## Porrentruy 76.1 35.3 9 7 90.57 ## Infant.Mortality ## Courtelary 22.2 ## Delemont 22.2 ## Franches-Mnt 20.2 ## Moutier 20.3 ## Neuveville 20.6 ## Porrentruy 26.6 If we do hist(swiss$Catholic), notice that it’s very bimodal, that’s because most provinces are either majority Catholic or majority Protestant, from this time period. hist(swiss$Catholic) We now create a catholic, binary variable, which is one if the province is majority catholic, and zero if it’s majority protestant. library(dplyr); swiss = mutate(swiss, CatholicBin = 1 * (Catholic &gt; 50)) We can plot the data to see the two groups, the CatholicBin factor variable, that’s zero for majority Protestant and one for majority Catholic. g = ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(CatholicBin))) g = g + geom_point(size = 6, colour = &quot;black&quot;) + geom_point(size = 4) g = g + xlab(&quot;% in Agriculture&quot;) + ylab(&quot;Fertility&quot;) g For the time being, we will ignore the outlaiers and simply work on fitting a line where we want two separate lines. One for the majority Catholic provinces, and one for the majority Protestant provinces. For notations Y is fertility, X1 is the percent of the province working in agriculture, and X2 is a binary variable, where it is one if the province is over 50% catholic, and zero if the province is majority Protestant. Let’s consider model one, where we modeled the expected y, given x1 and x2, is an intercept plus a slope times x1. \\(E[Y | X_1 = x_1, X_2 = 0] = \\beta_0 + \\beta_1 x_1\\). This is the line that would disregard the religion of the province entirely. Let’s consider a second model \\(E[Y | X_1 = x_1, X_2 = 1] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In the event that X two is equal to zero, i.e. if the province is majority protestant, this works out to be \\(\\beta_0 + \\beta_1 x_1\\). In the event that x2 is equal to one, i.e. if the province is majority Catholic, this works out to be \\(\\beta_0 + \\beta_2 + \\beta_1 x_1\\). The model that includes X1 and X2, but no interaction fits two models that have the same slope, but they have different intercepts, \\(\\beta_0\\) and then \\(\\beta_0+\\beta_2\\). If we consider one third model \\(E[Y | X_1 = x_1, X_2 = 1] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\), in this model when X2 is zero, it works out to be \\(\\beta_0 + \\beta_1 x_1\\), and when X2 is 1, which is the case when the province is majority Catholic, we get \\(\\beta_0 + \\beta_2 + \\beta_1 x_1 + \\beta_3 x_1\\), now let’s reorganize terms to get \\(\\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) x_1\\). This model is showing if we omitted that interaction term, we fit two lines the same slope, if we include the interaction term, we get two lines different slopes and different intercepts. And the coefficient in front of the Catholic term is going to change in the intercept going from Protestant to Catholic. 3.2.3 Exploring the models in R We start by fitting the model, where we fit the expected fertility as a linear function of the percent of the province working in agriculture. fit = lm(Fertility ~ Agriculture, data = swiss) g1 = g g1 = g1 + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2) g1 Here we want to save the plot in g1, then we can keep adding different things to it. The coefficients summaries can be obtained by summary(fit)$coef. summary(lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.8322366 4.1058630 14.815944 1.032493e-18 ## Agriculture 0.1241776 0.0810977 1.531210 1.328763e-01 ## factor(CatholicBin)1 7.8843292 3.7483622 2.103406 4.118221e-02 This just disregards the color of the points. Let’s do one that fits two parallel lines. fit = lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss) g1 = g g1 = g1 + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2) g1 = g1 + geom_abline(intercept = coef(fit)[1] + coef(fit)[3], slope = coef(fit)[2], size = 2) g1 Because the variable CatholicBin is 0 or 1, we don’t actually have to have the factor statement. Because coding a variable of 0 versus 1 treats it as a factor. However we like to call factor variables and the reason is sometimes you have a variable like 0,1,2 for a 3 level variable, and if you don’t call that a factor, R is going to treat that as a continuous regressor. It’s going to say 2 is twice 1, even if 2 is just a numeric coding for representing red hair color and 1 is for brown hair color, where 2 really has nothing to do with being twice 1 in that case. summary(lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss))$coef ## Estimate Std. Error t value ## (Intercept) 62.04993019 4.78915566 12.9563402 ## Agriculture 0.09611572 0.09881204 0.9727127 ## factor(CatholicBin)1 2.85770359 10.62644275 0.2689238 ## Agriculture:factor(CatholicBin)1 0.08913512 0.17610660 0.5061430 ## Pr(&gt;|t|) ## (Intercept) 1.919379e-16 ## Agriculture 3.361364e-01 ## factor(CatholicBin)1 7.892745e-01 ## Agriculture:factor(CatholicBin)1 6.153416e-01 We have an intercept and a slope, this intercept is the intercept for mostly Protestant provinces, the slope is the slope for mostly Protestant provinces. This 2.86 plus 62.04 is the intercept for the mostly Catholic provinces and the slope 0.096 + 0.089 is the slope for the mostly Catholic provinces. For lines with different intercepts and different slopes depending on the percent of the province that is Catholic, put an asterisk and do the fit. What happens when you add an asterisks in between two variables in R? It automatically fits the interaction. In general you want to include the main effects, if you include the interaction. fit = lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss) g1 = g g1 = g1 + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2) g1 = g1 + geom_abline(intercept = coef(fit)[1] + coef(fit)[3], slope = coef(fit)[2] + coef(fit)[4], size = 2) g1 based on the coefficients all the Catholic terms were positive the line with the slightly higher intercept is the Catholic line and the line for the slightly lower intercept is the mostly Protestant. Now you can probably see that well for the blue dots it is not clear how the two outlier blue dots impacting the fit, so we might want to investigate that. We will do that in the next section where we talk about residuals and influence diagnostics and that sort of thing. 3.3 Adjustment Adjustment, is the idea of putting regressors into a linear model to investigate the role of a third variable on the relationship between another two. Since it is often the case that a third variable can distort, or confound if you will, the relationship between two others. As an example, consider looking at lung cancer rates and breath mint usage. For the sake of completeness, imagine if you were looking at forced expiratory volume (a measure of lung function) and breath mint usage. If you found a statistically significant regression relationship, it wouldn’t be wise to rush off to the newspapers with the headline “Breath mint usage causes shortness of breath!”, for a variety of reasons. First off, even if the association is sound, you don’t know that it’s causal. But, more importantly in this case, the likely culprit is smoking habits. Smoking rates are likely related to both breath mint usage rates and lung function. How would you defend your finding against the accusation that it’s just variability in smoking habits? If your finding held up among non-smokers and smokers analyzed separately, then you might have something. In other words, people wouldn’t even begin to believe this finding unless it held up while holding smoking status constant. That is the idea of adding a regression variable into a model as adjustment. The coefficient of interest is interpreted as the effect of the predictor on the response, holding the adjustment variable constant. In this lecture, we’ll use simulation to investigate how adding a regressor into a model addresses the idea of adjustment. 3.3.1 Adjustment Examples In this section, we’re gonna go over some examples of how adjusting for one variable can impact the apparent relationship of another variable on an outcome. The easiest way to see this is to look at a two group variable. For example, a treatment versus a control or something you might see in an AB test when we’re adjusting for a continuous variable. Here’s the first simulation: n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), runif(n/2)); beta0 &lt;- 0; beta1 &lt;- 2; tau &lt;- 1; sigma &lt;- .2 y &lt;- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma) plot(x, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x), lwd = 2) abline(h = mean(y[1 : (n/2)]), lwd = 3) abline(h = mean(y[(n/2 + 1) : n]), lwd = 3) fit &lt;- lm(y ~ x + t) abline(coef(fit)[1], coef(fit)[2], lwd = 3) abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3) points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 2) points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 2) In this case, the red group was the group reciving treatment and the blue group was control. The horizontal lines show the marginal effect of group status disregarding the x. The y was a measure of blood pressure, then we would think that if we hadn’t factored in x, the mean for the group that received the treatment was around 2 and the mean for the control was around 0.8. You notice there’s a pretty clear linear relationship between the outcome and the regressor. So what we could do is fit a model that looks like \\(y = \\beta_0 + \\beta_1 T + \\beta_2 x + \\epsilon\\), where \\(T\\) is the treatment indicator, with values of \\(\\{0,1\\}\\). This would fit two parallel lines with \\(\\beta_1\\) representing the change in intercepts between the groups and \\(\\beta_2\\) the common slope that exists across the two groups. Taking a closer look at y-axis shows the marginal effect, the effect that we have if we disregard x, and the effect that we have if we incorporate x in a linear model and look at the change in the intercepts, are about the same. In addition, note in this example there is a lot of direct evidence to compare the groups for any given value of x. If we binned x we would have red and blue circles for a direct comparison of the treatment for kind of a fairly isolated level of x. Let’s try a setting where it’s gonna make a big difference. n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), 1.5 + runif(n/2)); beta0 &lt;- 0; beta1 &lt;- 2; tau &lt;- 0; sigma &lt;- .2 y &lt;- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma) plot(x, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x), lwd = 2) abline(h = mean(y[1 : (n/2)]), lwd = 3) abline(h = mean(y[(n/2 + 1) : n]), lwd = 3) fit &lt;- lm(y ~ x + t) abline(coef(fit)[1], coef(fit)[2], lwd = 3) abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3) points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 2) points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 2) The horizontal lines show marginal difference between the red, treated group, and the blue, control group. However, if we fit that model and look at the change in the intercepts, we’d see a tiny difference. This is a case where we would go from a massive treatment effect to nothing when we accounted for x. Moreover, if we knew that the x value was one or smaller, we know that value is in the blue group, and if it was 1.5 or higher, we know the value was in the treated group. So knowledge of x at some level pretty much gives us perfect knowledge of received treatment. In a randomized setuation it would be very hard to pick what treatment you had based on your x level because the x levels were all jumbled up some of the high x levels went to the treated, some of the high x levels went to the control. However, in this case, we clearly didn’t randomize. The question of which model here is the right one to consider is not the discussion for this section, here we want to show how the inclusion of x can change the estimate. As an example, imagin y, your outcome, was the blood pressure and the x variable was cholesterol or something highly related to whether or not you would’ve gotten prescribed a medication. You could see that adjusting for x is really just adjusting for the same thing that would lead you to have treatment. Again, this is what makes observational data analysis very hard as opposed to instances where you have randomized sample. This is an example where we had a strong marginal effect when we disregarded x, and a very subtle or a non-existent effect when we accounted for x. One more point is there is no value of x we can hold constant and compare red versus blue directly, which is a bad setting, where we’re relying very heavily on the model to compare the group. A summary of the important points about the previous example can be listed as: * The X variable is highly related to group status * The X variable is related to Y, the intercept doesn’t depend on the group variable. * The X variable remains related to Y holding group status constant * The group variable is marginally related to Y disregarding X. * The model would estimate no adjusted effect due to group. * There isn’t any data to inform the relationship between group and Y. * This conclusion is entirely based on the model. Another senario is where there is some overlap between data point. n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), .9 + runif(n/2)); beta0 &lt;- 0; beta1 &lt;- 2; tau &lt;- -1; sigma &lt;- .2 y &lt;- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma) plot(x, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x), lwd = 2) abline(h = mean(y[1 : (n/2)]), lwd = 3) abline(h = mean(y[(n/2 + 1) : n]), lwd = 3) fit &lt;- lm(y ~ x + t) abline(coef(fit)[1], coef(fit)[2], lwd = 3) abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3) points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 2) points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 2) There is some direct evidence for comparing the two groups. The marginal mean for the red and blue groups suggests the red is higher than the blue. However, the linear fit in the intercepts shows the blue is higher than the red. This shows the adjusted estimate is significant and the exact opposite of the unadjusted estimate. This phenomenon is often called Simpson’s Paradox, and the idea is things can change to the exact opposite when you perform adjustment. Next senario is shown in the following plot. n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(.5 + runif(n/2), runif(n/2)); beta0 &lt;- 0; beta1 &lt;- 2; tau &lt;- 1; sigma &lt;- .2 y &lt;- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma) plot(x, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x), lwd = 2) abline(h = mean(y[1 : (n/2)]), lwd = 3) abline(h = mean(y[(n/2 + 1) : n]), lwd = 3) fit &lt;- lm(y ~ x + t) abline(coef(fit)[1], coef(fit)[2], lwd = 3) abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3) points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 2) points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 2) Here there is no marginal effect. However, there’s a huge effect when we adjust for x. There’s no simple rule that says this is always what will happen with adjustment. Pretty much any permutation of going from significant to non-significant, staying both significant, staying non-significant, flipping signs can occur. As our final example we look at the following graph. n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2, -1, 1), runif(n/2, -1, 1)); beta0 &lt;- 0; beta1 &lt;- 2; tau &lt;- 0; tau1 &lt;- -4; sigma &lt;- .2 y &lt;- beta0 + x * beta1 + t * tau + t * x * tau1 + rnorm(n, sd = sigma) plot(x, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x), lwd = 2) abline(h = mean(y[1 : (n/2)]), lwd = 3) abline(h = mean(y[(n/2 + 1) : n]), lwd = 3) fit &lt;- lm(y ~ x + t + I(x * t)) abline(coef(fit)[1], coef(fit)[2], lwd = 3) abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2] + coef(fit)[4], lwd = 3) points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 2) points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 2) This example considers an instance where assuming the slopes were common across the two groups would be wrong. The linear model \\(y = \\beta_0 + \\beta_1 T + \\beta_2 x + \\beta_3 T x + \\epsilon\\) would fit two lines with different intercepts and different slopes. Another important thing to ascertain is there is no treatment effect. If we look at the center there is no evidence of a treatment effect, if we look at the right side of the graph there is a big evidence that blue has a higher outcome than red, and if you look at the left side we see there is a lot of evidence that red has a higher outcome than blue. The interaction is the reason why the main treatment effect doesn’t have a lot of meaning and the coefficient in front of the treated effects is not interpreted as the treatment effect by itself. The treatment effect depends on what level of x you’re at, and you can’t just read the term from the regression output associated with the treatment and act as if it’s a treatment effect because we have an interaction term in the model. Again this shows how adjustment can really change things if you have a setting where you have not just adjustment, but so-called modification. We want to reiterate that nothing we’ve talked about is specific to having a binary treatment and a continuous x. p &lt;- 1 n &lt;- 100; x2 &lt;- runif(n); x1 &lt;- p * runif(n) - (1 - p) * x2 beta0 &lt;- 0; beta1 &lt;- 1; tau &lt;- 4 ; sigma &lt;- .01 y &lt;- beta0 + x1 * beta1 + tau * x2 + rnorm(n, sd = sigma) plot(x1, y, type = &quot;n&quot;, frame = FALSE) abline(lm(y ~ x1), lwd = 2) co.pal &lt;- heat.colors(n) points(x1, y, pch = 21, col = &quot;black&quot;, bg = co.pal[round((n - 1) * x2 + 1)], cex = 2) The above plot shows outcome, y, and the continuous variable \\(x1\\) and continuous variable \\(x2\\) color coded, higher lighter values mean higher, and more red darker values means lower. At first glance you might say there is not much of a relationship between y and x1, however, if we look at it in three dimensions you can see that most of the variation of y is explained by its relationship with x2. Here we show the 2D plot of y versus x2. An easy way to look at the other variables effect without having to resort to three dimensional plots, which don’t work if you move beyond two variables is to look at residuals. plot(resid(lm(x1 ~ x2)), resid(lm(y ~ x2)), frame = FALSE, col = &quot;black&quot;, bg = &quot;lightblue&quot;, pch = 21, cex = 2) abline(lm(I(resid(lm(x1 ~ x2))) ~ I(resid(lm(y ~ x2)))), lwd = 2) Here we show the residual of y after having removed x2, the linear effect of x2, and the residual of x1 having removed linear effect of x2. As you can see there’s a very strong relationship left over between y and x1 after having removed the effect of x2. We want to reiterate we haven’t said what exactly is the right model, the best way to think about that is you have to bring in some of the specific subject matter, or clinical scientific subject matter expertise into your model building exercise. If you’re doing model building with a regular data set where you want interpretable coefficients getting the team of people some with the right scientific expertise, some with the statistical expertise, and some with the computing expertise is essential. 3.4 Residuals again Recall from before that the vertical distances between the observed data points and the fitted regression line are called residuals. We can generalize this idea to the vertical distances between the observed data and the fitted surface in multivariable settings. 3.5 Model selection 3.6 Practical R Exercises in swirl During this week of the course you should complete the following lessons in the Regression Models swirl course: MultiVar Examples2 MultiVar Examples3 Residuals Diagnostics and Variation 3.7 Week 3 Quiz 3.8 (OPTIONAL) Practice exercise in regression modeling "],["week-04.html", "Chapter 4 Week 04 4.1 GLM 4.2 Logistic Regression 4.3 Poisson Regression 4.4 Hodgepodge 4.5 Practical R Exercises in swirl 4.6 Week 4 Quiz 4.7 Course Project", " Chapter 4 Week 04 4.1 GLM 4.2 Logistic Regression 4.3 Poisson Regression 4.4 Hodgepodge 4.5 Practical R Exercises in swirl 4.6 Week 4 Quiz 4.7 Course Project "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) FirstName LastName Lecturer(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved Delivered the course in some way - video or audio Content Author(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved If any other authors besides lead instructor Content Contributor(s) (include section name/link in parentheses) - make new line if more than one section involved Wrote less than a chapter Content Editor(s)/Reviewer(s) Checked your content Content Director(s) Helped guide the content direction Content Consultants (include chapter name/link in parentheses or word “General”) - make new line if more than one chapter involved Gave high level advice on content Acknowledgments Gave small assistance to content but not to the level of consulting Production Content Publisher(s) Helped with publishing platform Content Publishing Reviewer(s) Reviewed overall content and aesthetics on publishing platform Technical Course Publishing Engineer(s) Helped with the code for the technical aspects related to the specific course generation Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Ava Hoffman, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright Art and Design Illustrator(s) Created graphics for the course Figure Artist(s) Created figures/plots for course Videographer(s) Filmed videos Videography Editor(s) Edited film Audiographer(s) Recorded audio Audiography Editor(s) Edited audio recordings Funding Funder(s) Institution/individual who funded course including grant number Funding Staff Staff members who help with funding   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.5 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2024-02-05 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.5) ## bookdown 0.24 2023-03-28 [1] Github (rstudio/bookdown@88bc4ea) ## bslib 0.4.2 2022-12-16 [1] CRAN (R 4.0.2) ## cachem 1.0.7 2023-02-24 [1] CRAN (R 4.0.2) ## callr 3.5.0 2020-10-08 [1] RSPM (R 4.0.2) ## cli 3.6.1 2023-03-23 [1] CRAN (R 4.0.2) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.20 2023-01-17 [1] CRAN (R 4.0.2) ## fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.0.2) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.4.2 2020-08-27 [1] RSPM (R 4.0.5) ## htmltools 0.5.5 2023-03-23 [1] CRAN (R 4.0.2) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## jsonlite 1.7.1 2020-09-07 [1] RSPM (R 4.0.2) ## knitr 1.33 2023-03-28 [1] Github (yihui/knitr@a1052d1) ## magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.0.2) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.4.0 2020-10-07 [1] RSPM (R 4.0.2) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 1.1.0 2023-03-14 [1] CRAN (R 4.0.2) ## rmarkdown 2.10 2023-03-28 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.3 2022-04-02 [1] CRAN (R 4.0.2) ## sass 0.4.5 2023-01-24 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2023-03-28 [1] Github (R-lib/testthat@e99155a) ## usethis 1.6.3 2020-09-17 [1] RSPM (R 4.0.2) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2023-03-28 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["references.html", "Chapter 5 References", " Chapter 5 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
